{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import huggingface_hub\n",
    "from transformers import AutoTokenizer\n",
    "import voyageai\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "voyageai.api_key = os.getenv(\"VOYAGE_API\")\n",
    "vo = voyageai.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('voyageai/voyage-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_tokens=4096, overlap=512):\n",
    "    \"\"\"\n",
    "    Splits text into chunks based on the token limit of voyage-law-2 tokenizer.\n",
    "    Uses a sliding window approach with overlap.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text to be chunked.\n",
    "        max_tokens (int): Maximum tokens per chunk (4096 for voyage-law-2).\n",
    "        overlap (int): Overlapping tokens to maintain context between chunks.\n",
    "\n",
    "    Returns:\n",
    "        list of str: List of text chunks.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        chunk = tokens[start:start + max_tokens]\n",
    "        chunks.append(tokenizer.decode(chunk))\n",
    "        start += max_tokens - overlap\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts, model=\"voyage-law-2\", batch_size=32):\n",
    "    \"\"\"\n",
    "    Compute embeddings using the VoyageAI Python client in batches.\n",
    "\n",
    "    Args:\n",
    "        texts (list of str): List of text data to embed.\n",
    "        model (str): The embedding model to use.\n",
    "        batch_size (int): Number of texts per batch.\n",
    "\n",
    "    Returns:\n",
    "        list: List of embedding vectors.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "\n",
    "    texts = [str(text) for text in texts]  \n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size] \n",
    "        \n",
    "        try:\n",
    "            response = vo.embed(batch, model=model)\n",
    "            batch_embeddings = response.embeddings  \n",
    "            embeddings.extend(batch_embeddings)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {i // batch_size + 1}: {e}\")\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Embedding per SECTIONS through ['Processed_Content']:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = pd.read_csv('/Users/npatel237/LawChatBot/Title18_CSV_Data/Title18_processed_sections.csv', encoding='utf-8')\n",
    "processed_content = doc['Processed_Content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenziation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[\"Processed_Content\"] = doc[\"Processed_Content\"].apply(lambda x: chunk_text(x) if len(x) > 4096 else [x])\n",
    "df_exploded = doc.explode(\"Processed_Content\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded[\"Processed_Content\"] = df_exploded[\"Processed_Content\"].astype(str)\n",
    "df_exploded[\"Embedding\"] = get_embeddings(df_exploded[\"Processed_Content\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded.to_parquet(\"embeddings_voyage.parquet\", engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Embedding per chapters through ['Processed_Content']:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = pd.read_csv('/Users/npatel237/LawChatBot/Title18_CSV_Data/chunked_title_18semchunk_pages.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6001\\nImmunity of Witnesses\\nV.\\n5001\\nCorrect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37\\n756, 3058\\n38\\nT. 22 ยง465\\n39\\n5, 3241\\n51...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79\\n1003\\n80\\n287, 1001\\n81\\n289\\n82\\n641, 136...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>123\\n912\\n124\\n211\\n125\\n543\\n126\\n541\\n127\\n1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>199\\n205\\n200\\n204\\n201\\n1913\\n202\\n216\\n203\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               chunk\n",
       "0  6001\\nImmunity of Witnesses\\nV.\\n5001\\nCorrect...\n",
       "1  37\\n756, 3058\\n38\\nT. 22 ยง465\\n39\\n5, 3241\\n51...\n",
       "2  79\\n1003\\n80\\n287, 1001\\n81\\n289\\n82\\n641, 136...\n",
       "3  123\\n912\\n124\\n211\\n125\\n543\\n126\\n541\\n127\\n1...\n",
       "4  199\\n205\\n200\\n204\\n201\\n1913\\n202\\n216\\n203\\n..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc['chunk'] = doc['chunk'].astype(str).fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[\"Embedding\"] = get_embeddings(doc[\"chunk\"].tolist())\n",
    "\n",
    "doc.to_parquet(\"embeddings_voyage_per_pages_semchunked.parquet\", engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = pd.read_parquet(\"embeddings_voyage_per_pages_semchunked.parquet\", engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk</th>\n",
       "      <th>Embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6001\\nImmunity of Witnesses\\nV.\\n5001\\nCorrect...</td>\n",
       "      <td>[-0.0461999773979187, -0.024502042680978775, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37\\n756, 3058\\n38\\nT. 22 ยง465\\n39\\n5, 3241\\n51...</td>\n",
       "      <td>[-0.00995637383311987, 0.008323794230818748, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79\\n1003\\n80\\n287, 1001\\n81\\n289\\n82\\n641, 136...</td>\n",
       "      <td>[-0.011511960998177528, 0.031173910945653915, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>123\\n912\\n124\\n211\\n125\\n543\\n126\\n541\\n127\\n1...</td>\n",
       "      <td>[0.011239621788263321, 0.025644589215517044, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>199\\n205\\n200\\n204\\n201\\n1913\\n202\\n216\\n203\\n...</td>\n",
       "      <td>[-0.015470130369067192, 0.024292832240462303, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               chunk  \\\n",
       "0  6001\\nImmunity of Witnesses\\nV.\\n5001\\nCorrect...   \n",
       "1  37\\n756, 3058\\n38\\nT. 22 ยง465\\n39\\n5, 3241\\n51...   \n",
       "2  79\\n1003\\n80\\n287, 1001\\n81\\n289\\n82\\n641, 136...   \n",
       "3  123\\n912\\n124\\n211\\n125\\n543\\n126\\n541\\n127\\n1...   \n",
       "4  199\\n205\\n200\\n204\\n201\\n1913\\n202\\n216\\n203\\n...   \n",
       "\n",
       "                                           Embedding  \n",
       "0  [-0.0461999773979187, -0.024502042680978775, 0...  \n",
       "1  [-0.00995637383311987, 0.008323794230818748, 0...  \n",
       "2  [-0.011511960998177528, 0.031173910945653915, ...  \n",
       "3  [0.011239621788263321, 0.025644589215517044, 0...  \n",
       "4  [-0.015470130369067192, 0.024292832240462303, ...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "athlyze",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
