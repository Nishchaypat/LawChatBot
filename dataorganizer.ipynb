{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted_Data\\Chapter10_Biological_Weapons.csv\n",
      "NO METADATA\n",
      "Data fetching completed and Saved.\n",
      "Extracted_Data\\Chapter11A_Child_Support.csv\n",
      "NO METADATA\n",
      "Data fetching completed and Saved.\n",
      "Extracted_Data\\Chapter11B_Chemical_Weapons.csv\n",
      "NO METADATA\n",
      "NO METADATA\n",
      "NO METADATA\n",
      "NO METADATA\n",
      "NO METADATA\n",
      "NO METADATA\n",
      "Data fetching completed and Saved.\n",
      "Extracted_Data\\Chapter11_BRIBERY_GRAFT_AND_CONFLICTS_OF_INTEREST.csv\n",
      "NO METADATA\n",
      "NO METADATA\n",
      "NO METADATA\n",
      "NO METADATA\n",
      "Data fetching completed and Saved.\n",
      "Extracted_Data\\Chapter1_General_Provisions.csv\n",
      "NO METADATA\n",
      "NO METADATA\n",
      "NO METADATA\n",
      "NO METADATA\n",
      "NO METADATA\n",
      "NO METADATA\n",
      "NO METADATA\n",
      "NO METADATA\n",
      "NO METADATA\n",
      "Data fetching completed and Saved.\n",
      "Extracted_Data\\Chapter2_Aircraft_and_MotorVehicles.csv\n",
      "NO METADATA\n",
      "NO METADATA\n",
      "NO METADATA\n",
      "NO METADATA\n",
      "Data fetching completed and Saved.\n",
      "Extracted_Data\\Chapter3_ANIMALS_BIRDS_FISH_AND_PLANTS.csv\n",
      "NO METADATA\n",
      "NO METADATA\n",
      "NO METADATA\n",
      "Data fetching completed and Saved.\n",
      "Extracted_Data\\Chapter5_ARSON.csv\n",
      "NO METADATA\n",
      "Data fetching completed and Saved.\n",
      "Extracted_Data\\Chapter7_Assault.csv\n",
      "NO METADATA\n",
      "NO METADATA\n",
      "Data fetching completed and Saved.\n",
      "Extracted_Data\\Chapter9_Bankruptcy.csv\n",
      "NO METADATA\n",
      "Data fetching completed and Saved.\n",
      "Extracted_Data\\Chpater12_Civil_Disorders.csv\n",
      "NO METADATA\n",
      "Data fetching completed and Saved.\n",
      "Extracted_Data\\Chpater13_Civil_Rights.csv\n",
      "NO METADATA\n",
      "Data fetching completed and Saved.\n",
      "Extracted_Data\\Chpater15_CLAIMS_AND_SERVICES_IN_MATTERS_AFFECTING_GOVERNMENT.csv\n",
      "NO METADATA\n",
      "NO METADATA\n",
      "NO METADATA\n",
      "NO METADATA\n",
      "NO METADATA\n",
      "Data fetching completed and Saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import regex as re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# Specify the folder path where your CSV files are located\n",
    "folder_path = 'Extracted_Data/*.csv'  # Change this to your actual folder path\n",
    "\n",
    "# Get a list of all CSV files in the specified folder\n",
    "csv_files = glob.glob(folder_path)\n",
    "# Iterate through each CSV file\n",
    "for file in csv_files:\n",
    "    print(file)\n",
    "    try:\n",
    "        df=pd.read_csv(rf'{file}')\n",
    "    except:\n",
    "        df=pd.read_csv(rf\"{file}\", encoding='latin-1')\n",
    "\n",
    "    if 'Content' and 'Metadata' in df.columns:\n",
    "        def clean_section(section):\n",
    "            # Use regex to remove everything before the section number\n",
    "            cleaned_section = re.sub(r'^.*?(\\d+[A-Z]?)\\.?\\s*(.*)', r'\\1. \\2', section)\n",
    "            return cleaned_section\n",
    "\n",
    "        # Apply the cleaning function to the Section column\n",
    "        df['Section'] = df['Section'].apply(clean_section)\n",
    "\n",
    "\n",
    "        # Function to fetch the data from a URL\n",
    "        def remove_editorial_notes(text):\n",
    "            # Split at the first occurrence of \"EDITORIAL NOTES\" and take the part before it\n",
    "            return text.split('EDITORIAL NOTES')[0].strip()\n",
    "\n",
    "        def remove_forwarddata(text, section):\n",
    "            # Split at the first occurrence of \"EDITORIAL NOTES\" and take the part before it\n",
    "            return text.split(section)[1].strip()\n",
    "\n",
    "        def get_metadata(text):\n",
    "            if 'EDITORIAL NOTES' 'Editorial Notes' in text:\n",
    "                return text.split('EDITORIAL NOTES')[1].strip()\n",
    "            if 'Editorial Notes' in text:\n",
    "                return text.split('Editorial Notes')[1].strip()\n",
    "            if 'Historical and Revision Notes' in text:\n",
    "                return text.split('Historical and Revision Notes')[1].strip()\n",
    "            print(\"NO METADATA\")\n",
    "            return None \n",
    "\n",
    "        def get_data_from_url(row):\n",
    "            url = row['Url']\n",
    "            section = clean_section(row['Section'])  # Clean the section text\n",
    "            \n",
    "            try:\n",
    "                # Make a request to the URL\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "\n",
    "                # Parse the HTML content using BeautifulSoup\n",
    "                soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "                # Extract the text data (customize this based on your needs)\n",
    "                content = soup.get_text()\n",
    "                content = content.strip()  # Clean the content\n",
    "                try:\n",
    "                    content=remove_forwarddata(content,section )\n",
    "                except:\n",
    "                    pass\n",
    "                # content= remove_editorial_notes(remove_forwarddata(content,section ))\n",
    "                content=content.replace(\"\\n\", \"\")\n",
    "                content= re.sub(r'<!--.*?-->', '', content, flags=re.DOTALL)\n",
    "                content = content.strip()\n",
    "                return content\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching data from {url}: {e}\")\n",
    "                return f\"Error fetching data: {e}\"\n",
    "\n",
    "        # Go through each URL and fetch the data\n",
    "        df['Content'] = df.apply(get_data_from_url, axis=1)\n",
    "        if 'Content' in df.columns:\n",
    "            df['Metadata'] = df['Content'].apply(get_metadata)\n",
    "            df['Content']= df['Content'].apply(remove_editorial_notes)\n",
    "        df.to_csv(rf'{file}')\n",
    "        print(\"Data fetching completed and Saved.\")\n",
    "    else:\n",
    "        print(f\"{file} is not processed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted_Data\\Chapter10_Biological_Weapons.csv: 1\n",
      "Extracted_Data\\Chapter11A_Child_Support.csv: 1\n",
      "Extracted_Data\\Chapter11B_Chemical_Weapons.csv: 6\n",
      "Extracted_Data\\Chapter11_BRIBERY_GRAFT_AND_CONFLICTS_OF_INTEREST.csv: 4\n",
      "Extracted_Data\\Chapter1_General_Provisions.csv: 9\n",
      "Extracted_Data\\Chapter2_Aircraft_and_MotorVehicles.csv: 4\n",
      "Extracted_Data\\Chapter3_ANIMALS_BIRDS_FISH_AND_PLANTS.csv: 3\n",
      "Extracted_Data\\Chapter5_ARSON.csv: 1\n",
      "Extracted_Data\\Chapter7_Assault.csv: 2\n",
      "Extracted_Data\\Chapter9_Bankruptcy.csv: 1\n",
      "Extracted_Data\\Chpater12_Civil_Disorders.csv: 1\n",
      "Extracted_Data\\Chpater13_Civil_Rights.csv: 1\n",
      "Extracted_Data\\Chpater15_CLAIMS_AND_SERVICES_IN_MATTERS_AFFECTING_GOVERNMENT.csv: 5\n"
     ]
    }
   ],
   "source": [
    "folder_path = 'Extracted_Data/*.csv'  # Change this to your actual folder path\n",
    "\n",
    "# Get a list of all CSV files in the specified folder\n",
    "csv_files = glob.glob(folder_path)\n",
    "# Iterate through each CSV file\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        df=pd.read_csv(rf'{file}')\n",
    "    except:\n",
    "        df=pd.read_csv(rf\"{file}\", encoding='latin-1')\n",
    "    null_count = df['Metadata'].isnull().sum()\n",
    "    print(f\"{file}: {null_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted_Data\\Chapter10_Biological_Weapons.csv: 8\n",
      "Extracted_Data\\Chapter11A_Child_Support.csv: 2\n",
      "Extracted_Data\\Chapter11B_Chemical_Weapons.csv: 8\n",
      "Extracted_Data\\Chapter11_BRIBERY_GRAFT_AND_CONFLICTS_OF_INTEREST.csv: 22\n",
      "Extracted_Data\\Chapter1_General_Provisions.csv: 15\n",
      "Extracted_Data\\Chapter2_Aircraft_and_MotorVehicles.csv: 13\n",
      "Extracted_Data\\Chapter3_ANIMALS_BIRDS_FISH_AND_PLANTS.csv: 7\n",
      "Extracted_Data\\Chapter5_ARSON.csv: 1\n",
      "Extracted_Data\\Chapter7_Assault.csv: 6\n",
      "Extracted_Data\\Chapter9_Bankruptcy.csv: 4\n",
      "Extracted_Data\\Chpater12_Civil_Disorders.csv: 4\n",
      "Extracted_Data\\Chpater13_Civil_Rights.csv: 7\n",
      "Extracted_Data\\Chpater15_CLAIMS_AND_SERVICES_IN_MATTERS_AFFECTING_GOVERNMENT.csv: 6\n"
     ]
    }
   ],
   "source": [
    "folder_path = 'Extracted_Data/*.csv'  # Change this to your actual folder path\n",
    "\n",
    "# Get a list of all CSV files in the specified folder\n",
    "csv_files = glob.glob(folder_path)\n",
    "# Iterate through each CSV file\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        df=pd.read_csv(rf'{file}')\n",
    "    except:\n",
    "        df=pd.read_csv(rf\"{file}\", encoding='latin-1')\n",
    "    null_count = df['Metadata'].isnull().sum()\n",
    "    print(f\"{file}: {null_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted_Data\\Chapter11A_Child_Support\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "file_name=\"Extracted_Data\\Chapter11A_Child_Support.csv\"\n",
    "file_name=file_name.split('.csv')[0]\n",
    "print(file_name)\n",
    "df=pd.read_csv(rf'{file_name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "def clean_section(section):\n",
    "    # Use regex to remove everything before the section number\n",
    "    cleaned_section = re.sub(r'^.*?(\\d+[A-Z]?)\\.?\\s*(.*)', r'\\1. \\2', section)\n",
    "    return cleaned_section\n",
    "\n",
    "# Apply the cleaning function to the Section column\n",
    "df['Section'] = df['Section'].apply(clean_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 USC Ch. 11A: Front Matter            Result 1 of 1                                      Current2018 Ed. and Supplement V (1/3/2024)2018 Ed. and Supplement IV (1/5/2023)2018 Ed. and Supplement III (1/3/2022)2018 Ed. and Supplement II (1/13/2021)2018 Ed. and Supplement I (1/24/2020)2018 Main Ed. (1/14/2019)2012 Ed. and Supplement V (1/12/2018)2012 Ed. and Supplement IV (1/6/2017)2012 Ed. and Supplement III (1/3/2016)2012 Ed. and Supplement II (1/5/2015)2012 Ed. and Supplement I (1/16/2014)2012 Main Ed. (1/15/2013)2006 Ed. and Supplement V (1/3/2012)2006 Ed. and Supplement IV (1/7/2011)2006 Ed. and Supplement III (2/1/2010)2006 Ed. and Supplement II (1/5/2009)2006 Ed. and Supplement I (1/8/2008)2006 Main Ed. (1/3/2007)2000 Ed. and Supplement V (1/2/2006)2000 Ed. and Supplement IV (1/3/2005)2000 Ed. and Supplement III (1/19/2004)2000 Ed. and Supplement II (1/6/2003)2000 Ed. and Supplement I (1/22/2002)2000 Main Ed. (1/2/2001)1994 Ed. and Supplement V (1/23/2000)1994 Ed. and Supplement IV (1/5/1999)1994 Ed. and Supplement III (1/26/1998)1994 Ed. and Supplement II (1/6/1997)1994 Ed. and Supplement I (1/16/1996)1994 Main Ed. (1/4/1995)                         Back to Original Document << Previous                                 TITLE 18 / PART I / CHAPTER 11A / Front Matter                                 Next >>[Print]                                  [Print selection][OLRC Home]Help 18 USC Ch. 11A: Front MatterFrom Title 18-CRIMES AND CRIMINAL PROCEDUREPART I-CRIMESCHAPTER 11A-CHILD SUPPORTCHAPTER 11A-CHILD SUPPORTSec.228.Failure to pay legal child support obligations.\n"
     ]
    }
   ],
   "source": [
    "print(df['Content'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                     5. -Front Matter\n",
      "1    81. Arson within special maritime and territor...\n",
      "Name: Section, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['Section'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    18 USC Ch. 5: Front Matter            Result 1...\n",
      "1    Whoever, within the special maritime and terri...\n",
      "Name: Content, dtype: object\n",
      "Data fetching completed.\n"
     ]
    }
   ],
   "source": [
    "# Function to fetch the data from a URL\n",
    "def remove_editorial_notes(text):\n",
    "    # Split at the first occurrence of \"EDITORIAL NOTES\" and take the part before it\n",
    "    return text.split('EDITORIAL NOTES')[0].strip()\n",
    "\n",
    "def remove_forwarddata(text, section):\n",
    "    # Split at the first occurrence of \"EDITORIAL NOTES\" and take the part before it\n",
    "    return text.split(section)[1].strip()\n",
    "\n",
    "def get_metadata(text):\n",
    "    return text.split('EDITORIAL NOTES')[1].strip()\n",
    "\n",
    "def get_data_from_url(row):\n",
    "    url = row['Url']\n",
    "    section = clean_section(row['Section'])  # Clean the section text\n",
    "    \n",
    "    try:\n",
    "        # Make a request to the URL\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Extract the text data (customize this based on your needs)\n",
    "        content = soup.get_text()\n",
    "        content = content.strip()  # Clean the content\n",
    "        try:\n",
    "            content=remove_forwarddata(content,section )\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            content= remove_editorial_notes(content)\n",
    "        except:\n",
    "            pass\n",
    "        # content= remove_editorial_notes(remove_forwarddata(content,section ))\n",
    "        content=content.replace(\"\\n\", \"\")\n",
    "        content= re.sub(r'<!--.*?-->', '', content, flags=re.DOTALL)\n",
    "        content = content.strip()\n",
    "        return content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data from {url}: {e}\")\n",
    "        return f\"Error fetching data: {e}\"\n",
    "\n",
    "# Go through each URL and fetch the data\n",
    "df['Content'] = df.apply(get_data_from_url, axis=1)\n",
    "df['Metadata'] = df.apply(get_metadata, axis=1)\n",
    "\n",
    "print(df['Content'])\n",
    "\n",
    "print(\"Data fetching completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Whoever, within the special maritime and territorial jurisdiction of the United States, willfully and maliciously sets fire to or burns any building, structure or vessel, any machinery or building materials or supplies, military or naval stores, munitions of war, or any structural aids or appliances for navigation or shipping, or attempts or conspires to do such an act, shall be imprisoned for not more than 25 years, fined the greater of the fine under this title or the cost of repairing or replacing any property that is damaged or destroyed, or both.If the building be a dwelling or if the life of any person be placed in jeopardy, he shall be fined under this title or imprisoned for any term of years or for life, or both.(June 25, 1948, ch. 645, 62 Stat. 688; Pub. L. 103–322, title XXXIII, §330016(1)(H), (K), Sept. 13, 1994, 108 Stat. 2147; Pub. L. 104–132, title VII, §708(b), Apr. 24, 1996, 110 Stat. 1296; Pub. L. 107–56, title VIII, §§810(a), 811(a), Oct. 26, 2001, 115 Stat. 380, 381.)Historical and Revision NotesBased on title 18, U.S.C., 1940 ed., §§464, 465 (Mar. 4, 1909, ch. 321, §§285, 286, 35 Stat. 1144).Sections were consolidated and rewritten both as to form and substance and that part of each section relating to destruction of property by means other than burning constitutes section 1363 of this title.The words \"within the maritime and territorial jurisdiction of the United States\" were added to preserve existing limitations of territorial applicability. (See section 7 of this title and note thereunder.)The phrase \"any building, structure, or vessel, any machinery or building materials and supplies, military or naval stores, munitions of war or any structural aids or appliances for navigation or shipping\" was substituted for \"any dwelling house, or any store, barn, stable, or other building, parcel of a dwelling house\", in section 464 of title 18, U.S.C., 1940 ed., and \"any arsenal, armory, magazine, rope walk, ship house, warehouse, blockhouse, or barrack, or any storehouse, barn or stable, not parcel of a dwelling house, or any other building not mentioned in the section last preceding, or any vessel, built, building, or undergoing repair, or any lighthouse, or beacon, or any machinery, timber, cables, rigging, or other materials or appliances for building, repairing or fitting out vessels, or any pile of wood, boards, or other lumber, or any military, naval or victualing stores, arms, or other munitions of war\", in section 465 of title 18, U.S.C., 1940 ed. The substituted phrase is a concise and comprehensive description of the things enumerated in both sections.The punishment provisions are new and are graduated with some regard to the gravity of the offense. It was felt that a possible punishment of 20 years for burning a wood pile or injuring or destroying an outbuilding was disproportionate and not in harmony with recent legislation.Editorial NotesAmendments2001-Pub. L. 107–56, in first par., struck out \",\\xa0or attempts to set fire to or burn\" after \"maliciously sets fire to or burns\" and inserted \"or attempts or conspires to do such an act,\" before \"shall be imprisoned\" and, in second par., substituted \"for any term of years or for life\" for \"not more than twenty years\".1996-Pub. L. 104–132, in first par., substituted \"imprisoned for not more than 25 years, fined the greater of the fine under this title or the cost of repairing or replacing any property that is damaged or destroyed, or both\" for \"fined under this title or imprisoned not more than five years, or both\".1994-Pub. L. 103–322 substituted \"fined under this title\" for \"fined not more than $1,000\" in first par. and for \"fined not more than $5,000\" in second par.'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Content'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'18 USC Ch. 5: Front Matter            Result 1 of 1                        \\xa0             Current2018 Ed. and Supplement V (1/3/2024)2018 Ed. and Supplement IV (1/5/2023)2018 Ed. and Supplement III (1/3/2022)2018 Ed. and Supplement II (1/13/2021)2018 Ed. and Supplement I (1/24/2020)2018 Main Ed. (1/14/2019)2012 Ed. and Supplement V (1/12/2018)2012 Ed. and Supplement IV (1/6/2017)2012 Ed. and Supplement III (1/3/2016)2012 Ed. and Supplement II (1/5/2015)2012 Ed. and Supplement I (1/16/2014)2012 Main Ed. (1/15/2013)2006 Ed. and Supplement V (1/3/2012)2006 Ed. and Supplement IV (1/7/2011)2006 Ed. and Supplement III (2/1/2010)2006 Ed. and Supplement II (1/5/2009)2006 Ed. and Supplement I (1/8/2008)2006 Main Ed. (1/3/2007)2000 Ed. and Supplement V (1/2/2006)2000 Ed. and Supplement IV (1/3/2005)2000 Ed. and Supplement III (1/19/2004)2000 Ed. and Supplement II (1/6/2003)2000 Ed. and Supplement I (1/22/2002)2000 Main Ed. (1/2/2001)1994 Ed. and Supplement V (1/23/2000)1994 Ed. and Supplement IV (1/5/1999)1994 Ed. and Supplement III (1/26/1998)1994 Ed. and Supplement II (1/6/1997)1994 Ed. and Supplement I (1/16/1996)1994 Main Ed. (1/4/1995)            \\xa0            Back to Original Document\\xa0<< Previous                \\xa0                TITLE 18 / PART I / CHAPTER 5 / Front Matter                \\xa0                Next >>[Print]                \\xa0\\xa0                [Print selection][OLRC Home]Help\\xa018 USC Ch. 5: Front MatterFrom Title 18-CRIMES AND CRIMINAL PROCEDUREPART I-CRIMESCHAPTER 5-ARSONCHAPTER 5-ARSONSec.81.Arson within special maritime and territorial jurisdiction.'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Content'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(rf'Extracted_Data/{file_name}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
