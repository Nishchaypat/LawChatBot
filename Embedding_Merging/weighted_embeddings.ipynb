{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "import vertexai\n",
    "import langchain\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import tiktoken\n",
    "import voyageai\n",
    "from tqdm import tqdm\n",
    "import pyarrow.parquet as pq\n",
    "import spacy\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "try:\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = r\"C:\\Users\\mkolla1\\LawChatBot\\gcpservicekey.json\"\n",
    "except:\n",
    "    print(\"Error at Try block\")\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = r\"gcpservicekey.json\"\n",
    "    \n",
    "PROJECT_ID = \"lawrag\"\n",
    "LOCATION = \"us-central1\"\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "\n",
    "voyageai.api_key = os.getenv(\"VOYAGE_API\")\n",
    "\n",
    "# Load NLP pipeline for query analysis\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Legal dictionary terms - expand as needed\n",
    "LEGAL_TERMS = {\n",
    "    \"habeas corpus\", \"mens rea\", \"actus reus\", \"stare decisis\", \n",
    "    \"prima facie\", \"de novo\", \"res judicata\", \"certiorari\",\n",
    "    \"statutory\", \"U.S.C.\", \"CFR\", \"jurisdiction\", \"adjudicate\"\n",
    "}\n",
    "\n",
    "# Regex patterns for legal citations\n",
    "CITATION_PATTERNS = [\n",
    "    r'\\d+\\s+U\\.S\\.C\\.\\s+§*\\s*\\d+',  # US Code\n",
    "    r'\\d+\\s+C\\.F\\.R\\.\\s+§*\\s*\\d+',   # Code of Federal Regulations\n",
    "    r'[A-Za-z]+\\s+v\\.\\s+[A-Za-z]+',  # Case names\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Generator for Gemini and Voyage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingGenerator:\n",
    "    def __init__(self, gemini_model_name=\"text-embedding-005\", voyage_model_name=\"voyage-law-2\"):\n",
    "        \"\"\"\n",
    "        Initializes the embedding generator with Gemini and VoyageAI models.\n",
    "        \"\"\"\n",
    "        self.gemini_model = VertexAIEmbeddings(gemini_model_name)\n",
    "        self.voyage_model_name = voyage_model_name\n",
    "        self.voyage_client = voyageai.Client()\n",
    "\n",
    "    def get_embeddings_gemini(self, texts, batch_size=32):\n",
    "        \"\"\"\n",
    "        Compute embeddings using VertexAIEmbeddings in batches.\n",
    "\n",
    "        Args:\n",
    "            texts (list of str): List of text data to embed.\n",
    "            batch_size (int): Number of texts to process per batch.\n",
    "\n",
    "        Returns:\n",
    "            list: List of embedding vectors.\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating Embeddings\"):\n",
    "            batch = texts[i:i + batch_size]  # Get batch of texts\n",
    "            batch_embeddings = self.gemini_model.embed_documents(batch)  # Generate embeddings\n",
    "            embeddings.extend(batch_embeddings)  # Store results\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def get_embeddings_voyage(self, texts, batch_size=32):\n",
    "        \"\"\"\n",
    "        Compute embeddings using the VoyageAI Python client in batches.\n",
    "\n",
    "        Args:\n",
    "            texts (list of str): List of text data to embed.\n",
    "            batch_size (int): Number of texts per batch.\n",
    "\n",
    "        Returns:\n",
    "            list: List of embedding vectors.\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "        texts = [str(text) for text in texts]  \n",
    "\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i + batch_size] \n",
    "            \n",
    "            try:\n",
    "                response = self.voyage_client.embed(batch, model=self.voyage_model_name)\n",
    "                batch_embeddings = response.embeddings  \n",
    "                embeddings.extend(batch_embeddings)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch {i // batch_size + 1}: {e}\")\n",
    "\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saved Embeddings Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegalEmbeddingLoader:\n",
    "    \"\"\"Loads embeddings from parquet files for both models and all granularity levels.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_path):\n",
    "        self.base_path = base_path\n",
    "        self.gemini_embeddings = {}\n",
    "        self.voyager_embeddings = {}\n",
    "        self.metadata = {}\n",
    "        \n",
    "    def load_embeddings(self):\n",
    "        \"\"\"Load the six specified embedding files.\"\"\"\n",
    "        file_mappings = {\n",
    "            \"gemini_chapters\": \"embeddings_gemini_text-005_chapters_semchunk.parquet\",\n",
    "            \"voyager_chapters\": \"embeddings_voyage_per_chapter_semchunked.parquet\",\n",
    "            \"gemini_pages\": \"embeddings_gemini_text-005_pages_semchunk.parquet\",\n",
    "            \"voyager_pages\": \"embeddings_voyage_per_pages_semchunked.parquet\",\n",
    "            \"gemini_sections\": \"embeddings_gemini_text-005.parquet\",\n",
    "            \"voyager_sections\": \"embeddings_voyage.parquet\",\n",
    "        }\n",
    "\n",
    "        for key, file_name in file_mappings.items():\n",
    "            print(self.base_path)\n",
    "            file_path = os.path.join(self.base_path, key.split(\"_\")[-1], file_name)\n",
    "            print(file_path)\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"File {file_name} not found. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            # Read parquet file\n",
    "            table = pq.read_table(file_path)\n",
    "            df = table.to_pandas()\n",
    "            print(f\"\\nColumns in {file_name}: {df.columns.tolist()}\")\n",
    "            # Extract embeddings and metadata\n",
    "            embeddings = np.stack(df[\"Embedding\"].values)\n",
    "\n",
    "            # Determine model and granularity\n",
    "            model, granularity = key.split(\"_\")\n",
    "\n",
    "            # Store embeddings\n",
    "            if model == \"gemini\":\n",
    "                self.gemini_embeddings[granularity] = torch.tensor(embeddings, dtype=torch.float32)\n",
    "            else:\n",
    "                self.voyager_embeddings[granularity] = torch.tensor(embeddings, dtype=torch.float32)\n",
    "\n",
    "            # Store metadata\n",
    "            self.metadata[key] = df.drop('Embedding', axis=1)\n",
    "\n",
    "            print(f\"Loaded {file_name} ({model} - {granularity})\")\n",
    "        return self.gemini_embeddings, self.voyager_embeddings, self.metadata\n",
    "\n",
    "    def get_embedding_dimensions(self):\n",
    "        \"\"\"Return the dimensions of embeddings for both models.\"\"\"\n",
    "        gemini_dim = {k: v.shape[1] for k, v in self.gemini_embeddings.items()}\n",
    "        voyager_dim = {k: v.shape[1] for k, v in self.voyager_embeddings.items()}\n",
    "        return gemini_dim, voyager_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming LegalEmbeddingLoader class has been defined as in the code you provided\n",
    "# loader = LegalEmbeddingLoader(base_path=\"New_Embeddings_2025\")\n",
    "# gemini_embeddings, voyager_embeddings, metadata = loader.load_embeddings()\n",
    "\n",
    "# # Get the embedding dimensions\n",
    "# gemini_dim, voyager_dim = loader.get_embedding_dimensions()\n",
    "\n",
    "# # Print the dimensions for both models\n",
    "# print(\"Gemini Embedding Dimensions:\")\n",
    "# print(gemini_dim)\n",
    "\n",
    "# print(\"\\nVoyager Embedding Dimensions:\")\n",
    "# print(voyager_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Analyser and Intent Recognization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegalQueryAnalyzer:\n",
    "    \"\"\"Analyzes legal queries to determine intent and model weights.\"\"\"\n",
    "    \n",
    "    def __init__(self, legal_terms=LEGAL_TERMS, citation_patterns=CITATION_PATTERNS):\n",
    "        self.legal_terms = legal_terms\n",
    "        self.citation_patterns = citation_patterns\n",
    "        self.embedder = EmbeddingGenerator()\n",
    "        \n",
    "    def analyze_query(self, query):\n",
    "        \"\"\"\n",
    "        Analyze query characteristics to determine model weights.\n",
    "        Returns a dictionary of features and recommended weights.\n",
    "        \"\"\"\n",
    "        # Process with spaCy\n",
    "        doc = nlp(query)\n",
    "        \n",
    "        # Feature extraction\n",
    "        features = {\n",
    "            'legal_term_density': self._calculate_legal_term_density(query),\n",
    "            'citation_count': self._count_citations(query),\n",
    "            'structural_complexity': self._assess_complexity(doc),\n",
    "            'query_length': len(doc),\n",
    "            'jurisdiction_signals': self._detect_jurisdiction(doc)\n",
    "        }\n",
    "        \n",
    "        # Calculate recommended weights\n",
    "        weights = self._determine_weights(features)\n",
    "        \n",
    "        return {\n",
    "            'features': features,\n",
    "            'weights': weights,\n",
    "            'gemini_embedding': self.embedder.get_embeddings_gemini([query]),\n",
    "            'voyage_embedding': self.embedder.get_embeddings_voyage([query])\n",
    "        }\n",
    "    \n",
    "    def _calculate_legal_term_density(self, query):\n",
    "        \"\"\"Calculate the density of legal terminology in the query.\"\"\"\n",
    "        # Normalize and tokenize query\n",
    "        query_lower = query.lower()\n",
    "        total_tokens = len(query_lower.split())\n",
    "        \n",
    "        # Count legal terms\n",
    "        legal_term_count = sum(1 for term in self.legal_terms if term.lower() in query_lower)\n",
    "        \n",
    "        # Calculate density\n",
    "        if total_tokens > 0:\n",
    "            return (legal_term_count / total_tokens) * 100\n",
    "        return 0\n",
    "    \n",
    "    def _count_citations(self, query):\n",
    "        \"\"\"Count legal citations in the query.\"\"\"\n",
    "        citation_count = 0\n",
    "        for pattern in self.citation_patterns:\n",
    "            citation_count += len(re.findall(pattern, query))\n",
    "        return citation_count\n",
    "    \n",
    "    def _assess_complexity(self, doc):\n",
    "        \"\"\"\n",
    "        Assess the structural complexity of the query.\n",
    "        Returns a score from 0-1 based on:\n",
    "        - Number of clauses\n",
    "        - Presence of legal conditionals\n",
    "        - Sentence structure complexity\n",
    "        \"\"\"\n",
    "        # Count clauses\n",
    "        clause_markers = [\"if\", \"when\", \"whether\", \"notwithstanding\", \"provided that\"]\n",
    "        clause_count = sum(1 for token in doc if token.text.lower() in clause_markers)\n",
    "        \n",
    "        # Check for complex legal conditionals\n",
    "        has_conditionals = any(cm in doc.text.lower() for cm in clause_markers)\n",
    "        \n",
    "        # Assess syntactic complexity (simplified)\n",
    "        depth = max((token.dep_.count('_') for token in doc), default=0)\n",
    "        \n",
    "        # Calculate complexity score (0-1)\n",
    "        complexity = min(1.0, (clause_count * 0.2) + (0.3 if has_conditionals else 0) + (depth * 0.1))\n",
    "        \n",
    "        return complexity\n",
    "    \n",
    "    def _detect_jurisdiction(self, doc):\n",
    "        \"\"\"\n",
    "        Detect jurisdictional signals in the query.\n",
    "        Returns a dictionary of jurisdictional features.\n",
    "        \"\"\"\n",
    "        # Look for jurisdictional entities\n",
    "        jurisdictions = {\n",
    "            'federal': 0,\n",
    "            'state': 0,\n",
    "            'international': 0,\n",
    "            'specific_court': None\n",
    "        }\n",
    "        \n",
    "        # Check for federal signals\n",
    "        federal_terms = [\"federal\", \"U.S.\", \"United States\", \"SCOTUS\", \"Supreme Court\"]\n",
    "        jurisdictions['federal'] = any(term.lower() in doc.text.lower() for term in federal_terms)\n",
    "        \n",
    "        # Check for state signals\n",
    "        state_names = [\n",
    "    \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\", \"Delaware\", \n",
    "    \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \n",
    "    \"Louisiana\", \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \"Mississippi\", \n",
    "    \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \"New Hampshire\", \"New Jersey\", \"New Mexico\", \n",
    "    \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \n",
    "    \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \n",
    "    \"Virginia\", \"Washington\", \"West Virginia\", \"Wisconsin\", \"Wyoming\"\n",
    "]\n",
    "  # Add all states\n",
    "        jurisdictions['state'] = any(state in doc.text for state in state_names)\n",
    "        \n",
    "        # Check for international signals\n",
    "        international_terms = [\"international\", \"foreign\", \"treaty\", \"convention\"]\n",
    "        jurisdictions['international'] = any(term.lower() in doc.text.lower() for term in international_terms)\n",
    "        \n",
    "        # Look for specific courts\n",
    "        court_patterns = [\"Circuit\", \"District Court\", \"Supreme Court\"]\n",
    "        for pattern in court_patterns:\n",
    "            if pattern in doc.text:\n",
    "                jurisdictions['specific_court'] = pattern\n",
    "                break\n",
    "                \n",
    "        return jurisdictions\n",
    "    \n",
    "    def _determine_weights(self, features):\n",
    "        \"\"\"\n",
    "        Determine the optimal weights for each model based on features.\n",
    "        Uses a rule-based approach initially, could be replaced with ML model.\n",
    "        \"\"\"\n",
    "        # Default weights slightly favor specialized model\n",
    "        gemini_weight = 0.4\n",
    "        voyager_weight = 0.6\n",
    "        \n",
    "        # Adjust for legal density and citations\n",
    "        if features['legal_term_density'] > 5 or features['citation_count'] > 0:\n",
    "            # Increase weight for legal model\n",
    "            voyager_weight += 0.15\n",
    "            gemini_weight -= 0.15\n",
    "        \n",
    "        # Adjust for complexity\n",
    "        if features['structural_complexity'] > 0.7:\n",
    "            voyager_weight += 0.1\n",
    "            gemini_weight -= 0.1\n",
    "        \n",
    "        # Adjust for jurisdictional specificity\n",
    "        if features['jurisdiction_signals']['specific_court']:\n",
    "            voyager_weight += 0.1\n",
    "            gemini_weight -= 0.1\n",
    "        \n",
    "        # Ensure weights are valid\n",
    "        voyager_weight = min(max(voyager_weight, 0.1), 0.9)\n",
    "        gemini_weight = 1.0 - voyager_weight\n",
    "        \n",
    "        return {\n",
    "            'gemini': gemini_weight,\n",
    "            'voyager': voyager_weight\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings: 100%|██████████| 1/1 [00:00<00:00,  3.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['features', 'weights', 'gemini_embedding', 'voyage_embedding'])\n",
      " Shape of Gemini Query Embedding: (1, 768)\n",
      " Shape of Gemini Query Embedding after Unsqeeze: torch.Size([1, 1, 768])\n",
      " Shape of Gemini Query Embedding after Sqeeze: torch.Size([768])\n",
      " Shape of Voyage Query Embedding: (1, 1024)\n",
      "{'features': {'legal_term_density': 12.5, 'citation_count': 0, 'structural_complexity': 0.0, 'query_length': 9, 'jurisdiction_signals': {'federal': False, 'state': False, 'international': False, 'specific_court': None}}, 'weights': {'gemini': 0.25, 'voyager': 0.75}, 'gemini_embedding': [[-0.0813518688082695, -0.03374248370528221, 0.025017933920025826, -0.01886814273893833, -0.003584230085834861, 0.04847944900393486, -0.022799337282776833, 0.07440640032291412, 0.022810891270637512, 0.010746952146291733, 0.0031785459723323584, -0.041993673890829086, -0.062119387090206146, -0.08199410885572433, -0.027054833248257637, -0.024165762588381767, 0.010855771601200104, -0.036382801830768585, 0.019957387819886208, -0.007785398978739977, 0.030614733695983887, -0.043836090713739395, -0.014879901893436909, -0.02753479965031147, 0.01959039457142353, 0.031144900247454643, 0.0049483031034469604, 0.018481841310858727, -0.03277428075671196, 0.01965908706188202, -0.02319987118244171, -0.01701771840453148, 0.022129755467176437, 0.07821046561002731, 0.015987740829586983, 0.015959884971380234, 0.036117784678936005, 0.014987438917160034, -0.010865837335586548, 0.04553946852684021, 0.00792524591088295, 0.008034183643758297, -0.03734192997217178, 0.009125795215368271, 0.0025019932072609663, 0.023437416180968285, 0.006709912791848183, 0.07705353945493698, -0.06497254967689514, -0.02964439056813717, 0.013958270661532879, 0.053770601749420166, -0.0007696808897890151, 0.04421932250261307, 0.05598582327365875, 0.061990175396203995, -0.06240735203027725, -0.018609022721648216, -0.021008625626564026, 0.05420242249965668, -0.05258355662226677, 0.020286383107304573, -0.06586983799934387, 0.03074771538376808, -0.022964002564549446, 0.011093608103692532, 0.06989852339029312, 0.03225995972752571, -0.04858999326825142, 0.03494429960846901, -0.056073348969221115, 0.03619248420000076, -0.021278683096170425, 0.006163760554045439, -0.058439578860998154, 0.01724272407591343, -0.040404003113508224, 0.07410089671611786, -0.046666696667671204, 0.03028937615454197, -0.07767429202795029, -0.06769641488790512, 0.015225742943584919, -0.012299764901399612, -0.0002951475325971842, 0.0305035300552845, -0.012777627445757389, -0.06902489811182022, 0.0024277730844914913, 0.009403283707797527, -0.041057031601667404, -0.025628721341490746, 0.046992480754852295, -0.07958769798278809, 0.0483887754380703, 0.05614492669701576, -0.02783145383000374, 0.03755241259932518, 0.010311754420399666, -0.02110048197209835, -0.05956216901540756, 0.014635082334280014, -0.04394403100013733, -0.017728839069604874, -0.0038448062259703875, 0.057311005890369415, 0.013426990248262882, -0.056401416659355164, 0.08069334179162979, -0.017844000831246376, -0.06521739065647125, -0.06820019334554672, -0.010318188928067684, -0.015385321341454983, -0.0054468573071062565, -0.01676015369594097, 0.020229868590831757, -0.0016257757088169456, -0.04673810303211212, -0.01830996200442314, -0.011144515126943588, -0.010665281675755978, 0.06276475638151169, -0.02297252044081688, 0.02172585390508175, -0.02054411731660366, -0.058239586651325226, -0.016454465687274933, 0.025054438039660454, -0.018712615594267845, -0.02136032097041607, 0.011884098872542381, 0.02214549109339714, 0.02378714270889759, -0.03245685622096062, -0.01049251202493906, 0.01701541803777218, -0.008480370044708252, 0.06719120591878891, 0.07979337126016617, 0.017247630283236504, -0.0200536847114563, -0.08269170671701431, 0.012858684174716473, -0.044538505375385284, -0.012744485400617123, 0.012666918337345123, 0.01706826314330101, -0.02964974194765091, -0.04347524046897888, -0.012495352886617184, 0.022993912920355797, 0.011263438500463963, 0.046542368829250336, 0.052032142877578735, 0.05824287608265877, 0.003161895787343383, -0.03155367076396942, -0.04099881649017334, 0.008633795194327831, 0.0035745566710829735, 0.01332335826009512, -0.024384280666708946, 0.07553131133317947, -0.015347035601735115, -0.026546385139226913, 0.045533664524555206, -0.041276950389146805, 0.005972149316221476, -0.04370085895061493, 0.06406985968351364, 0.02298707887530327, -0.057263635098934174, 0.036538537591695786, 0.031160868704319, -0.00978985708206892, -0.06643960624933243, -0.0029420084320008755, 0.046269871294498444, -0.04991862177848816, -0.02937518060207367, -0.019931070506572723, 0.014591910876333714, -0.012568391859531403, -0.11193860322237015, -0.04047458618879318, -0.042687349021434784, -0.02499394491314888, -0.0052267638966441154, -0.02125345543026924, -0.06845111399888992, 0.0037881401367485523, 0.04802265018224716, 0.06870739907026291, 0.02111244760453701, -0.01758735254406929, -0.0259043350815773, -0.01893371343612671, -0.021479278802871704, 0.04572015255689621, -0.008096884936094284, 0.013436022214591503, 0.05250959098339081, 0.08173678070306778, -0.032060399651527405, 0.027594173327088356, -0.024220528081059456, 0.04107702150940895, -0.045648299157619476, -0.04931802675127983, 0.0067747668363153934, 0.03697560355067253, 0.0045030442997813225, -0.039540186524391174, -0.004152899142354727, -0.08765539526939392, 0.022875726222991943, -0.057830967009067535, 0.07213762402534485, -0.007512274663895369, 0.007553779054433107, -0.0454840362071991, -0.07620913535356522, -0.008460935205221176, -0.041787467896938324, -0.03549299016594887, 0.04324415698647499, -0.016732802614569664, 0.09497217833995819, -0.0370607003569603, 0.02512183226644993, 0.024342676624655724, -0.0335862934589386, -0.04123658314347267, -0.014090235345065594, -0.01520779449492693, 0.07218748331069946, -0.03151049464941025, -0.07043220102787018, 0.012365777045488358, 0.011075333692133427, 0.00016866426449269056, 0.01978425309062004, -0.02102585881948471, -0.07069692760705948, -0.055110398679971695, 0.042536817491054535, 0.038684286177158356, -0.04927406087517738, -0.07873710244894028, -0.03402479737997055, 0.017154380679130554, 0.003474962431937456, -0.012856547720730305, 0.04168138653039932, -0.08743894845247269, 0.026706863194704056, 0.03454569727182388, 0.007464038208127022, -0.00286128930747509, 0.061265978962183, 0.05058272182941437, -0.027334988117218018, -0.047384604811668396, 0.024517925456166267, 0.0012803312856703997, -0.01807374879717827, 0.000629748625215143, 0.018026752397418022, 0.026313386857509613, -0.043697357177734375, 0.044817522168159485, -0.008065902628004551, -0.010330302640795708, 0.024728722870349884, -0.022799866273999214, 0.025156449526548386, -0.008021725341677666, 0.00956944189965725, -0.006863883696496487, 0.0010228923056274652, -0.06948491930961609, 0.0228322371840477, 0.058273717761039734, -0.017662761732935905, 0.024060936644673347, -0.0033913555089384317, 0.01906747557222843, 0.01726514659821987, -0.035383470356464386, 0.05794510617852211, 0.09202376753091812, -0.021524986252188683, -0.05831504613161087, 0.01508259680122137, -0.040784481912851334, 0.016864318400621414, 0.047386277467012405, 0.03532956540584564, 0.011674282141029835, -0.03199924901127815, 0.038408439606428146, 0.05734029784798622, 0.03032108023762703, -0.0014587173936888576, -0.047955095767974854, 0.02832995355129242, 0.004027220886200666, 0.010029006749391556, 0.01129008550196886, -0.06709392368793488, 0.013276588171720505, 0.014908550307154655, -0.016594091430306435, -0.04710770025849342, 0.009477024897933006, 0.015896040946245193, 0.0005077891401015222, 0.05301262065768242, 0.022754313424229622, 0.0526069775223732, -0.0007789990049786866, -0.0295779537409544, -0.014613929204642773, -0.053290050476789474, -0.008622219786047935, 0.0010582089889794588, 0.03185094892978668, 0.018324196338653564, -0.014858339913189411, 0.02644003927707672, 0.0356149859726429, 0.02945960871875286, -0.006824804004281759, -0.0044028665870428085, -0.043799951672554016, -0.036449186503887177, 0.01580986939370632, -0.02751070074737072, 0.019022297114133835, -0.042604267597198486, 0.0304226353764534, -0.021411001682281494, -0.019209954887628555, -0.026135317981243134, 0.049142658710479736, -0.009862330742180347, -0.010013004764914513, -0.027629900723695755, 0.047466300427913666, -0.012225585989654064, 0.032472606748342514, 0.016891827806830406, 0.012156368233263493, 0.033353593200445175, -0.023630768060684204, -0.03962987661361694, 0.009533281438052654, -0.05509156733751297, -0.0491180494427681, 0.04005426540970802, -0.011375377885997295, 0.024297643452882767, -0.0012617624597623944, -0.05325494706630707, -0.0020589218474924564, -0.02824491634964943, -0.030788958072662354, 0.006587150972336531, 0.09060100466012955, -0.019829435274004936, -0.022464845329523087, 0.04754279553890228, 0.003076970111578703, -0.002401744481176138, 0.0272013358771801, -0.03174103796482086, -0.049855541437864304, 0.03242460638284683, -0.007296025287359953, 0.027165574952960014, -0.03295564651489258, -0.0060727265663445, -0.018226757645606995, -0.004385650157928467, 0.054290227591991425, 0.07659967243671417, 0.03279188647866249, -0.01816834881901741, -0.005320487543940544, 0.03850315138697624, 0.04618174955248833, -0.0460856594145298, -0.04914506524801254, 0.010488007217645645, -0.02952868677675724, -0.046760376542806625, -0.0031429012306034565, 0.028191538527607918, 0.009085121564567089, 0.06256654113531113, -0.0052855550311505795, 0.003127739764750004, -0.02165187895298004, -0.02978578396141529, -0.030020160600543022, -0.0020078131929039955, 0.03659588098526001, -0.025483017787337303, 0.006582674104720354, 0.031026694923639297, -0.10481749475002289, 0.01163929421454668, 0.010863919742405415, 0.05098685622215271, -0.06678067892789841, 0.01848074421286583, 0.008163371123373508, -0.021162109449505806, -0.03571658581495285, -0.026730431243777275, 0.005996466148644686, -0.012438982725143433, 0.01730574481189251, 0.012969942763447762, -0.041426289826631546, 0.008909907191991806, -0.001646981225349009, 0.026235993951559067, -0.052672553807497025, -0.035188257694244385, 0.0176288653165102, 0.025174856185913086, 0.010518738068640232, -0.05092515051364899, -0.06673331558704376, 0.04514136537909508, 0.013153260573744774, 0.030262811109423637, 0.04862683266401291, 0.03904298320412636, 0.0031107328832149506, 0.029553281143307686, 0.03374365717172623, -0.02684350311756134, -0.002385122003033757, -0.01303882896900177, 0.025351587682962418, 0.04315873980522156, 0.05566876009106636, -0.000111916902824305, 0.03240324929356575, -0.04521980509161949, -0.007852889597415924, 0.01991073600947857, -0.009298168122768402, 0.035253364592790604, 0.05689716339111328, 0.025208216160535812, 0.02698729746043682, -0.017474204301834106, 0.0179305262863636, -0.06243445724248886, 0.06659124791622162, 0.06578175723552704, -0.04578588530421257, 0.006216805893927813, -0.003800349310040474, 0.0038218186236917973, -0.007838458754122257, 0.07822608947753906, -0.067410908639431, 0.0427558533847332, -0.004470630548894405, -0.010419012978672981, 0.02695893868803978, 0.04924113303422928, -0.04624933749437332, -0.023492854088544846, 0.003778826678171754, -0.02981349267065525, 0.05130629986524582, -0.017225103452801704, -0.025531083345413208, 0.018367215991020203, 0.028658682480454445, 0.005129127763211727, -0.038593266159296036, 0.017924776300787926, 0.011838238686323166, -0.03431962430477142, -0.005296553485095501, 0.01233074814081192, -0.07301121205091476, -0.0054048579186201096, -0.026676498353481293, 0.0012264695251360536, -0.042385730892419815, 0.0034684159327298403, -0.0035104649141430855, -0.01057454664260149, 0.03663193807005882, -0.029680006206035614, 0.0423886738717556, -0.021693609654903412, -0.04551604390144348, -0.029024768620729446, 0.006889894604682922, -0.03397665545344353, 0.01159468200057745, -0.01651027612388134, -0.001119555439800024, -0.03390771150588989, -0.07572846859693527, 0.044290509074926376, -0.010021940805017948, 0.0404791533946991, -0.06750217825174332, 0.02929784543812275, 0.02447735331952572, -0.03340509161353111, -0.019912561401724815, -0.06401032209396362, -0.011433327570557594, -0.00686870701611042, -0.00755248824134469, -0.056172724813222885, -0.04158250242471695, 0.018644990399479866, -0.0011501292465254664, 0.017388388514518738, -0.02935379184782505, -0.04816122725605965, 0.03960378095507622, 0.002069560345262289, 0.009844789281487465, 0.05222158133983612, 0.011287184432148933, 0.03211679682135582, 0.0031742998398840427, 0.007265996187925339, -0.06451310962438583, -0.009215762838721275, -0.0006158636533655226, 0.07553102821111679, -0.029350554570555687, 0.020885610952973366, -0.04704710841178894, -0.015052593313157558, 0.05081665888428688, 0.03368604928255081, -0.001969040371477604, -0.0015433677472174168, 0.047407470643520355, -0.05142230913043022, -0.010073509067296982, -0.04680967703461647, -0.008976180106401443, 0.00184156047180295, -0.018073666840791702, -0.02879556082189083, -0.04761920124292374, 0.014756065793335438, -0.06207556277513504, -0.03515385463833809, -0.02472866140305996, -0.0031926243100315332, 0.04059373214840889, -0.034819599241018295, -0.021661071106791496, -0.012083623558282852, -0.004806126467883587, 0.03291110321879387, 0.015886418521404266, 0.016096659004688263, 0.028860751539468765, -0.027683846652507782, 0.04678715765476227, -0.04219198226928711, 0.019997205585241318, 0.02315242402255535, -0.023243263363838196, 0.017705025151371956, -0.024196892976760864, 0.013327416032552719, 0.0013938151532784104, -0.041851405054330826, -0.013662793673574924, 0.030764682218432426, -0.045721184462308884, 0.031617362052202225, -0.016224201768636703, 0.006921425927430391, 0.018859485164284706, -0.008202476426959038, 0.0485767126083374, 0.0038302100729197264, 0.02848505787551403, 0.038511063903570175, -0.0022108610719442368, -0.05452336370944977, -0.0262233205139637, -0.014633608050644398, -0.03758697584271431, -0.04796723648905754, 0.047870080918073654, -0.006209236569702625, -0.03419649228453636, 0.0015619819751009345, -0.04453204572200775, -0.0033322060480713844, -0.07261292636394501, 0.019493674859404564, -0.02875751629471779, -0.014919614419341087, -0.06512387841939926, 0.015487933531403542, -0.01848297379910946, 0.004553913604468107, 0.00126923609059304, -0.029821569100022316, -0.053231313824653625, -0.0235864520072937, -0.0610368549823761, -0.0086507648229599, -0.014370081946253777, 0.005187659990042448, 0.004796886350959539, -0.056757595390081406, -0.020985081791877747, 0.02011542208492756, 0.01586466282606125, -0.03441287949681282, -0.0062087299302220345, 0.027586214244365692, 0.06488741189241409, 0.07590161263942719, -0.03892916440963745, -0.005601351149380207, -0.03629602864384651, -0.010981862433254719, -0.03401047736406326, -0.04652373120188713, 0.0217405017465353, 0.03480784595012665, 0.002204003045335412, 0.011756976135075092, -0.013217633590102196, -0.002669913461431861, -0.060056086629629135, 0.026683958247303963, -0.017094722017645836, -0.012333710677921772, 0.01785251498222351, 0.061374370008707047, 0.018122248351573944, 0.1018824502825737, -0.005621660500764847, 0.050474945455789566, -0.01816714182496071, 0.01746298372745514, 0.02958291955292225, -0.028182532638311386, 0.017668286338448524, 0.005478875245898962, -0.03819281980395317, -0.07582365721464157, 0.0389748215675354, -0.08095727860927582, -0.033707395195961, -0.018117019906640053, -0.03926829993724823, 0.02546166256070137, -0.025754382833838463, -0.024860041216015816, 0.005655424669384956, 0.00687249144539237, -0.021758239716291428, 0.00758502259850502, 0.005289733409881592, -0.014823246747255325, -0.012573662213981152, -0.006369274575263262, -0.011099237017333508, -0.03014962188899517, -0.012244329787790775, -0.019031109288334846, 0.03630978241562843, 0.0034476756118237972, 0.02966698445379734, -0.006126663647592068, 0.02562767080962658, -0.01610211282968521, -0.08546312898397446, 0.05485037341713905, 0.03508121520280838, 0.001174816396087408, -0.04975379258394241, -0.032235484570264816, 0.016103163361549377, -0.027323685586452484, -0.0006416795076802373, 0.05567602440714836, 0.03214104101061821, 0.016734173521399498, 0.037357207387685776, 0.009053449146449566, 0.05827035382390022, -0.04711425304412842, 0.04048263281583786, -0.011812258511781693, 0.006690058391541243, -0.012264743447303772, -0.04604363813996315, -0.0012473573442548513, -0.019410952925682068, -0.023964939638972282, -0.02364213950932026, 0.04329806566238403, 0.04812592267990112, 0.023641929030418396, -0.0347420796751976, -0.03585527464747429, -0.0012536399299278855, -0.010108943097293377, 0.023789670318365097, 0.0028212242759764194, -0.014666718430817127, -0.006225112359970808, 0.0030566523782908916, -0.002657379489392042, -0.06855472177267075, -0.0444503016769886, -0.019834691658616066, -0.016549859195947647, -0.05984863266348839, -0.05775930732488632, -0.05053134635090828, 0.010046384297311306, -0.019571376964449883, -0.017616406083106995, 0.04318445548415184, -0.003960017114877701, 0.03415362536907196, -0.0377858504652977, 0.014822456985712051, 0.014348982833325863, 0.035534247756004333, -0.05279217287898064, -0.012543841265141964, -0.00961617287248373, -0.010382227599620819, 0.012402717024087906, -0.06244984269142151, 0.045453768223524094, 0.10369212180376053, -0.010111814364790916, 0.02331393212080002, 0.031746622174978256, 0.00898746307939291, -0.032712895423173904, -0.0031934676226228476, 0.03608040511608124, 0.014919414184987545, -0.04003028944134712, -0.002354017924517393, -0.0033608218654990196, -0.014205344952642918]], 'voyage_embedding': [[-0.006386333145201206, 0.02545694261789322, 0.040370333939790726, 0.020234614610671997, -0.014058811590075493, 0.0012051006779074669, -0.014439379796385765, 0.007824430242180824, 0.003433135338127613, 0.03511425480246544, 0.02425423450767994, -0.0007406909135170281, 0.07224594801664352, 0.007525951135903597, 0.010799219831824303, 0.048462722450494766, -0.012961872853338718, 0.054283421486616135, -0.03401660919189453, 0.032153982669115067, 0.010686914436519146, 0.042189907282590866, -0.013111734762787819, 0.031881947070360184, 0.0021438002586364746, -0.010260703042149544, 0.013450130820274353, -0.05311928689479828, -0.06950994580984116, -0.030192594975233078, -0.0076181115582585335, 0.006072324700653553, 0.01599453203380108, -0.011358990333974361, 0.010343466885387897, 0.04418828338384628, -0.010299406945705414, -0.003796390490606427, 0.009080293588340282, 0.024034373462200165, -0.005378556903451681, 0.0266981553286314, 0.0033027087338268757, -0.017102284356951714, 0.027324149385094643, -0.03984222561120987, 0.0016545881517231464, 0.039592891931533813, -0.021451978012919426, 0.015094478614628315, 0.00237732520326972, -0.06306944787502289, 0.03668227419257164, -0.08067383617162704, -0.002360617509111762, -0.02676289901137352, 0.014996929094195366, -0.03714604303240776, -0.057420406490564346, 0.0014507288578897715, -0.007307607214897871, 0.07118286192417145, -0.0038591783959418535, 0.05499834567308426, 0.06408484280109406, -0.012486482039093971, 0.02042527124285698, 0.011386409401893616, -0.011524987407028675, 0.03595603257417679, -0.008591461926698685, 0.06558393687009811, 0.008443048223853111, 0.0075968229211866856, 0.022605877369642258, 0.05122378468513489, -0.05939917266368866, -0.016286645084619522, -0.04202384501695633, -0.008763806894421577, -0.02104506641626358, 0.04297536611557007, -0.008611942641437054, 0.004480861593037844, -0.011547556146979332, 0.014983858913183212, -0.03420429676771164, -0.03210170567035675, -0.018832392990589142, 0.025215357542037964, -0.009839139878749847, -0.0236374344676733, 0.04282526671886444, 0.03994698449969292, -0.011759432032704353, 0.02854514867067337, -0.011380278505384922, -0.006370703224092722, 0.03125608339905739, -0.02748148888349533, 0.0031959956977516413, 0.007446185685694218, -0.00425504008308053, 0.02396700531244278, -0.02431328222155571, -0.038172345608472824, -0.026727797463536263, 0.028955021873116493, 0.02305859886109829, -0.0330362506210804, -0.021919788792729378, -0.03884849697351456, -0.032045114785432816, 0.0001550839515402913, -0.04764512926340103, -0.03159993886947632, 0.010608497075736523, 0.032850850373506546, -0.022685911506414413, -0.039191000163555145, 0.022236155346035957, 0.02764293923974037, -0.002297829370945692, -0.01572384126484394, -0.02730986662209034, 0.07672842592000961, -0.008220662362873554, -0.02585253305733204, 0.02469031885266304, 0.06408591568470001, -0.007360356859862804, -0.04470190405845642, 0.08229716867208481, -0.009066129103302956, 0.008595235645771027, 0.008454029448330402, 6.292285252129659e-05, 0.06115831807255745, -0.028316901996731758, -0.013279282487928867, -0.03989443928003311, -0.029172290116548538, 0.0222075916826725, -0.01434890367090702, -0.02218441665172577, 0.091960608959198, 0.011185918003320694, 0.03665640205144882, 0.031576547771692276, -0.06307591497898102, -0.005959481466561556, 0.04105750098824501, 0.017778005450963974, -0.044866353273391724, -0.03535442426800728, -0.018449734896421432, -0.033481962978839874, 0.01782805286347866, 0.06183955818414688, 0.0500173345208168, 0.005153644364327192, 0.013901234604418278, 0.009144091978669167, 0.05504550784826279, -0.006398526486009359, -0.008761771954596043, -0.06186758354306221, -0.007993223145604134, 0.07093116641044617, -0.017053576186299324, 0.026761211454868317, 0.005483315791934729, -5.820700971526094e-05, -0.032056838274002075, -0.027873072773218155, 0.042351339012384415, -0.03596896678209305, 0.012068522162735462, 0.009993281215429306, 0.009335217997431755, 0.009794564917683601, 0.02359108440577984, 0.026778995990753174, 0.014609419740736485, -0.010479804128408432, 0.051232948899269104, 0.004944429267197847, -0.01350941602140665, 0.002039535902440548, 0.022039975970983505, -0.004949482157826424, 0.04248949885368347, -0.01235551480203867, 0.04416833817958832, 0.0079282121732831, -0.008616657927632332, 0.054791659116744995, 0.006911004427820444, -0.033432383090257645, -0.03192304074764252, 0.014072218909859657, 0.04061124473810196, -0.020219389349222183, 0.05560709908604622, 0.020274903625249863, 0.023319721221923828, -0.06964875757694244, -0.041813988238573074, -0.06533036381006241, -0.05365177243947983, 0.027578266337513924, -0.010326758958399296, -0.01627357490360737, -0.0018704559188336134, -0.03348210081458092, -0.047071147710084915, -0.02417423389852047, 0.017101813107728958, -0.0384565070271492, 0.017292331904172897, -0.007885162718594074, -0.009758836589753628, 0.0192689448595047, 0.04954494163393974, -0.026462361216545105, -0.005556478630751371, 0.016142642125487328, 0.015082082711160183, -0.0020407484844326973, -0.0031006010249257088, 0.07336777448654175, -0.00040071216062642634, -0.061708055436611176, 0.03106529638171196, -0.02978997863829136, 0.032728102058172226, -0.01251248549669981, -0.04179034382104874, -0.0205874964594841, -0.03983093425631523, -0.054464515298604965, 0.022568823769688606, 0.07486391812562943, -0.03817092999815941, 0.011245674453675747, 0.017585791647434235, 0.02465740405023098, -0.08388034254312515, -0.019842324778437614, 0.03692399337887764, 0.005411095917224884, 0.002422597259283066, 0.0376189760863781, -0.03975323215126991, 0.04800838604569435, 0.04312802478671074, -0.005816388875246048, -0.019343994557857513, -0.09440530091524124, -0.011590976268053055, -0.045626748353242874, -0.032293569296598434, 0.004838726948946714, -0.021810382604599, 0.047551386058330536, 0.03466847166419029, -0.004702034872025251, 0.011005840264260769, -0.0032399878837168217, -0.005870554130524397, 0.015487375669181347, 0.017594484612345695, 0.08117290586233139, -0.017493361607193947, 0.039284881204366684, -0.02258310653269291, -0.010039429180324078, -0.011241533793509007, -0.049197182059288025, 0.0018944224575534463, -0.014668704941868782, 0.011644366197288036, -0.04582886025309563, -0.048919759690761566, 0.038861799985170364, 0.013583522289991379, -0.0024177466984838247, -0.015638822689652443, 0.0010380250168964267, 0.051299236714839935, 0.033560652285814285, -0.04161100462079048, 0.01563265733420849, -0.034567415714263916, -0.02355012483894825, -0.040994442999362946, -0.017959827557206154, 0.011129934340715408, 0.01764211244881153, 0.03953764960169792, -0.012197130359709263, -0.03811649605631828, 0.00025226405705325305, 0.01985047571361065, 0.021167948842048645, 0.012723620980978012, 0.015682680532336235, -0.0191914364695549, 0.02493860572576523, -0.029358968138694763, 0.005518617574125528, -0.08514823019504547, -0.032539740204811096, 0.0038342855405062437, -0.04212300851941109, -0.0648135095834732, 0.026755014434456825, 0.02447456493973732, 0.004632577300071716, 0.023702647536993027, 0.053006645292043686, 0.015495460480451584, 0.034909654408693314, -0.027460772544145584, -0.009529788978397846, 0.006111533846706152, 0.010253258980810642, 0.03870523348450661, -0.0429554246366024, 0.025390246883034706, -0.06453648209571838, 0.04465555399656296, -0.01824735850095749, -0.012733321636915207, 0.002700697397813201, -0.012123226188123226, 0.05823179706931114, 0.01569056138396263, -0.021499406546354294, -0.03429585322737694, -0.044608667492866516, -0.030721306800842285, 0.009670582599937916, -0.004570597317069769, 0.03843597695231438, -0.051624227315187454, -0.059592120349407196, 0.01754186861217022, -0.0321076326072216, 0.02371964231133461, 0.03958480805158615, -0.013002260588109493, -0.03578437492251396, 0.036356374621391296, 0.020751068368554115, 0.014909078367054462, 0.0008283720235340297, 0.004748721607029438, 0.01126957405358553, -0.008512909524142742, -0.0311152171343565, 0.02420387603342533, -0.03224075585603714, 0.013795297592878342, 0.0003303786797914654, 0.05194975808262825, 0.034834738820791245, 0.0011096721282228827, -0.04663459211587906, 0.010078527964651585, 0.02855377271771431, 0.06009267270565033, -0.03883485496044159, -0.022882631048560143, -0.040296848863363266, -0.019389400258660316, -0.07846412807703018, -0.018566956743597984, 0.005644193850457668, 0.017450785264372826, -0.0008138875709846616, -0.0098139438778162, 0.011002471670508385, -0.04051962122321129, 0.031709346920251846, -0.02486463263630867, -0.011752291582524776, -0.004450680688023567, 0.05049404129385948, -0.00023107642482500523, 0.01076182909309864, 0.0016936286119744182, 0.05437895655632019, -0.014953138306736946, 0.014643104746937752, 0.04755175858736038, -0.029187176376581192, 0.05132193863391876, -0.02968880906701088, -0.026847174391150475, -0.011109993793070316, -0.002538674511015415, 0.027292216196656227, 0.05976836010813713, 0.010002645663917065, -0.02920038066804409, 0.001422635861672461, 0.053879279643297195, 0.012205147184431553, 0.003478407859802246, 0.0014309223042801023, 0.043882694095373154, 0.018733223900198936, -0.01640898734331131, -0.055229827761650085, -0.004943696781992912, -0.003113536164164543, 0.020968131721019745, -0.019001690670847893, -0.04451542720198631, -0.009210719726979733, -0.007907260209321976, 0.04728996008634567, 0.04927869886159897, 0.0039041812997311354, 0.007246974390000105, -0.0186491496860981, 0.05539906397461891, 0.01789250783622265, -0.013455790467560291, 0.027828877791762352, -0.0161121916025877, -0.009576400741934776, 0.016074296087026596, -0.05485040321946144, 0.05890334025025368, 0.045250557363033295, 0.015150395222008228, -0.051115453243255615, 0.013877789489924908, 0.04722636565566063, 0.007669851649552584, 0.04528505355119705, -0.018365120515227318, -0.05089017376303673, -0.016593309119343758, -0.016580913215875626, -0.00388262327760458, 0.04111839830875397, -0.047385357320308685, 0.0427401140332222, -0.0016907317331060767, 0.014504323713481426, -0.013341185636818409, -0.019580325111746788, -0.06981068104505539, -0.02322513610124588, 0.04403872415423393, 0.03273174166679382, -0.012828582897782326, 0.024026021361351013, 0.02783592790365219, 0.014102315530180931, 0.005027360748499632, -0.007867647334933281, -0.004635474178940058, -0.06220981478691101, -0.010755226947367191, 0.00427808053791523, -0.0003093594859819859, -0.029666980728507042, 0.006321388762444258, -0.007381308823823929, -0.049273308366537094, -0.028002219274640083, -0.07809656113386154, 0.03040939010679722, -0.006091794464737177, 0.006155011244118214, 0.03057565726339817, 0.006664938293397427, -0.011852671392261982, 0.03877341374754906, -0.02463827282190323, 0.0028591498266905546, 0.016210921108722687, 0.06498516350984573, 0.0382554791867733, 0.05274740606546402, -0.0312700979411602, -0.034380942583084106, -0.024625876918435097, -0.04269484058022499, 0.00504218228161335, -0.004116529133170843, 0.027980323880910873, 0.03420928493142128, -0.027863774448633194, -0.01129512395709753, 0.017544157803058624, 0.0025282828137278557, 0.00044760110904462636, 0.004846474621444941, -0.042945150285959244, -0.03856039047241211, -0.04051665961742401, 0.025493793189525604, -0.003436638740822673, 0.019289426505565643, 0.02522573247551918, 0.00693903025239706, -0.013108433224260807, 0.03139298036694527, -0.0028230398893356323, -0.04057459533214569, -0.04361214116215706, -0.016279637813568115, 0.04028194397687912, -0.028903014957904816, -0.049982037395238876, 0.012447272427380085, 0.008551444858312607, 0.012676193378865719, -0.016794607043266296, -0.04237254709005356, 0.02600128762423992, 0.0241401270031929, -0.01095975935459137, -0.01041447278112173, -0.01288207434117794, -0.013152224011719227, -0.008291535079479218, 0.056038256734609604, -0.008738327771425247, -0.013457407243549824, -0.02537546120584011, -0.050245583057403564, -0.07876871526241302, -0.004910542629659176, 0.028455683961510658, -0.06131245940923691, -0.01489129289984703, 0.01832631602883339, -0.0015493069076910615, 0.02442673221230507, -0.037560634315013885, -0.004106692969799042, -0.014706971123814583, 0.023029090836644173, 0.007737759035080671, -0.01466358546167612, -0.007847166620194912, -0.03925847262144089, -0.002927057910710573, 0.05875108018517494, -0.03001757152378559, -0.047763701528310776, 0.0026292861439287663, -0.0016987822018563747, -0.03529554605484009, 0.0372384749352932, -0.04173765704035759, 0.05044284462928772, 0.04059372842311859, 0.03761250898241997, -0.010726123116910458, -0.03126760572195053, 0.011500196531414986, 0.015265327878296375, -0.01179864164441824, 0.021821700036525726, 0.012176918797194958, -0.022437185049057007, 0.002597622573375702, 0.0226814653724432, -0.04653003439307213, -0.006420556455850601, 0.03581024706363678, -0.00425504008308053, -0.029184887185692787, -0.017795849591493607, 0.03622039034962654, 0.010619410313665867, 0.03582371771335602, -0.013580961152911186, 0.01708052307367325, 0.002548443153500557, -0.03261424973607063, -0.02208416908979416, 0.031380582600831985, -0.012615291401743889, 0.06211119145154953, 0.005332947708666325, 0.021855654194951057, 0.02230002172291279, -0.036905400454998016, 0.01766151562333107, -0.011837614700198174, -0.024425655603408813, -0.03067627362906933, 0.010430911555886269, -0.008094816468656063, -0.001688003190793097, -0.02551097422838211, -0.00808161124587059, 0.011502890847623348, -0.010076279751956463, -0.06809411942958832, 0.02961062639951706, -0.0029790671542286873, -0.03223543241620064, 0.018473180010914803, 0.013780105859041214, 0.0143314553424716, -0.03070851042866707, 0.018103457987308502, -0.018530579283833504, -0.011326652951538563, 0.03176593407988548, 0.08010578155517578, 0.05157356336712837, -0.05378004163503647, 0.042954884469509125, 0.0012271978193894029, 0.0203553419560194, 0.03245849162340164, -0.014127292670309544, 0.01558923814445734, 0.006967594381421804, 0.034472428262233734, -0.018404599279165268, 0.04111786186695099, -0.0035096670035272837, -0.0014470908790826797, 0.006245126947760582, 0.020114025101065636, -0.03737213462591171, 0.01038962323218584, -0.03302560746669769, 0.0011129395570605993, 0.01291225478053093, 0.012758517637848854, -0.01972193643450737, -0.022008176892995834, -0.012594271451234818, -0.06246258690953255, 0.09995760023593903, -0.0019704150035977364, -0.014732555486261845, -0.004288640804588795, 0.049677524715662, -0.008531234227120876, -0.0433647595345974, 0.005560386460274458, 0.011953254230320454, -0.00862757209688425, -0.005728539545089006, 0.0033006034791469574, 0.05134477838873863, -0.02902751974761486, -0.053197767585515976, -0.046820834279060364, 0.022799834609031677, -0.06668192148208618, 0.06687338650226593, -0.0061683934181928635, -0.039996299892663956, 0.003000086173415184, -0.027441369369626045, -0.023262592032551765, -0.015137594193220139, 0.01987995021045208, -0.027453631162643433, 0.0002480534603819251, -0.012252777814865112, 0.01738557033240795, 0.04837002605199814, -0.03182872384786606, -0.023255856707692146, 0.01241843868046999, 0.02642705850303173, -0.02768632583320141, 0.05469895526766777, -0.015095825307071209, 0.02957805246114731, 0.00869089923799038, 0.033639878034591675, 0.003577886614948511, 0.07028388231992722, -0.041454169899225235, 0.021257145330309868, 0.0022167169954627752, -0.01506982184946537, -0.014993744902312756, -0.03752802684903145, -0.05741259083151817, -0.00037524657091125846, 0.020210767164826393, 0.02100680023431778, 0.0006088830414228141, -0.036657482385635376, 0.008594696410000324, 0.06761633604764938, -0.013023548759520054, 0.01863432675600052, -0.001499975798651576, -0.030743137001991272, -0.057762909680604935, -0.02344987913966179, -0.014451507478952408, -0.011547893285751343, 0.027454307302832603, 0.019489377737045288, 0.005570895969867706, 0.00692097470164299, 0.003951878286898136, 0.004909868817776442, 0.01752920262515545, -0.007278031669557095, -0.0008828062564134598, -0.0018426493043079972, 0.03461538627743721, -0.037009283900260925, -0.016517316922545433, 0.0147508280351758, 0.04117647558450699, 0.00981145165860653, 0.018925899639725685, -0.05903160572052002, 0.014194156974554062, 0.06989152729511261, -0.023128123953938484, 0.0043393527157604694, 0.0367550291121006, -0.03212542086839676, 0.00751409400254488, 0.0061516184359788895, 0.04227122291922569, -0.02029666304588318, -0.012025440111756325, 0.048646509647369385, 0.01453019492328167, 0.006555698812007904, -0.015722360461950302, 0.015082082711160183, 0.001574822934344411, -0.03167646750807762, -0.009857733733952045, -0.0010649727191776037, 0.046164777129888535, -0.04680059105157852, -0.005247793160378933, 0.027244651690125465, -0.04200174659490585, -0.02627291902899742, -0.0673552080988884, 0.025336656719446182, 0.008496202528476715, -0.03563346713781357, 0.006776805035769939, 0.014309829100966454, 0.024491945281624794, -0.002019998850300908, 0.04299665614962578, -0.030232073739171028, -0.01638742908835411, 0.009446647018194199, -0.03543243929743767, -0.002895259764045477, 0.05256200581789017, -0.023386552929878235, -0.0476602204144001, 0.01374682504683733, -0.005542702041566372, 0.0031020159367471933, 0.02313843183219433, -0.012178200297057629, -0.04712773486971855, 0.014698347076773643, 0.030087633058428764, 0.05002784729003906, -0.012017321772873402, 0.018439900130033493, -0.02021898701786995, -0.005785803776234388, 0.016887309029698372, 0.02417665906250477, 0.047179948538541794, 0.024239717051386833, 0.03547663241624832, 0.0007442951900884509, -0.005037061870098114, -0.02438146062195301, 0.05857349559664726, -0.013395426794886589, 0.016591152176260948, 0.008639749139547348, -0.04824849218130112, -0.001620364491827786, 0.0006497087888419628, 0.00673234136775136, -0.013173107989132404, -0.012494430877268314, -0.014118433929979801, -0.00512976199388504, 0.061546362936496735, 0.00716828741133213, 0.026591980829834938, -0.03726434335112572, -0.029626458883285522, -0.03395947813987732, -0.005908483173698187, -0.031111983582377434, 0.06953150033950806, -0.04406917467713356, 0.007039544638246298, 0.04188641160726547, -0.047963544726371765, -0.00763535825535655, 0.002820884110406041, -0.015871649608016014, 0.003810437163338065, -0.008455511182546616, -0.0225174892693758, 0.04186377301812172, 0.017490031197667122, -0.0076242596842348576, -0.009285365231335163, -0.02818526141345501, -0.0137212248519063, 0.012656791135668755, 0.046725139021873474, 0.029937805607914925, -0.006350492127239704, -0.004374148789793253, -0.09984010457992554, -0.03554917126893997, -0.029837559908628464, -0.013146077282726765, 0.026906795799732208, 0.027720008045434952, 0.03475066274404526, 0.018073545768857002, 0.031842365860939026, -0.042138371616601944, -0.011468936689198017, 0.04048836603760719, 0.013881832361221313, -0.02108289673924446, 0.015429236926138401, 0.015269233845174313, -0.01778116263449192, 0.05066920444369316, 0.08491324633359909, -0.021717412397265434, -0.015477405861020088, 0.016508154571056366, 0.06850829720497131, -0.025192318484187126, -0.04169023036956787, -0.035864632576704025, 0.04225882515311241, 0.014971057884395123, 0.016813471913337708, 0.007443086709827185, 0.07808254659175873, -0.04939403757452965, -0.08049005270004272, -0.022730646654963493, -0.05693830922245979, 0.023839542642235756, 0.016542917117476463, 0.0008646166534163058, 0.020795531570911407, 0.031391095370054245, -0.008242758922278881, 0.03761035203933716, 0.022858647629618645, -0.015976207330822945, -0.024276835843920708, 0.02946029230952263, 0.0019380440935492516, 0.02443023771047592, -0.008467502892017365, 0.022215675562620163, -0.0002298637991771102, -0.044735319912433624, -0.013934110291302204, 0.012246713973581791, 0.011546006426215172, -0.04751200973987579, 0.0012397284153848886, -0.001069284277036786, 0.019089743494987488, 0.029012419283390045, 0.026233576238155365, -0.04991143196821213, -0.03998929262161255, 0.022278599441051483, 0.05209527164697647, 0.017417773604393005, -0.017907986417412758, 0.017756102606654167, 0.021920328959822655, -0.0308118537068367, 0.010176373645663261, -0.020178399980068207, -0.03291282802820206, -0.05135798454284668, 0.019584503024816513, -0.05054604634642601, 0.026660291478037834, -0.031107334420084953, -0.007937172427773476, 0.0062149460427463055, 0.03240486606955528, -0.0006812375504523516, -0.002435262780636549, -0.020907094702124596, -0.039815209805965424, -0.018537988886237144, 0.03418651223182678, -0.0013495403109118342, -0.03911295533180237, -1.562966099299956e-05, 0.008374533616006374, 0.023732729256153107, 0.024626415222883224, 0.039912763983011246, -0.02358865924179554, -0.008020171895623207, -0.05519425868988037, -0.01091677788645029, 0.006241085007786751, -0.02334202267229557, 0.01678113453090191, 0.008896678686141968, -0.03950800746679306, 0.0212091114372015, 0.030320193618535995, 0.0010417976882308722, -0.007877213880419731, -0.043940093368291855, -0.02684825100004673, 0.022948922589421272, 0.0051796152256429195, 0.0030520951841026545, 0.013240275904536247, 0.0337708443403244, 0.0022231843322515488, 0.03268350660800934, -0.018816359341144562, 0.03177455812692642, -0.02541962079703808, -0.023423604667186737, -0.025988351553678513, 0.000567383598536253, -0.018198853358626366, 0.04126826673746109, -0.02943226508796215, -0.026035645976662636, -0.006603666115552187, -0.029748346656560898, 0.024688798934221268, 0.03845192864537239, -0.004446907434612513, 0.011686134152114391, 0.024671418592333794, -0.01432593073695898, 0.007978672161698341, 0.0753910094499588, -0.015314641408622265, 0.01991339959204197, 0.02689729817211628, 0.040607742965221405, 0.01892145350575447, 0.0052343192510306835, -0.004043500870466232, 0.019591374322772026, -0.0001250372879439965, 0.03067185916006565, -0.018250051885843277, 0.0205276720225811, 0.022118663415312767, -0.00014753859431948513, -0.046738073229789734, 0.02444869466125965, 0.012365755625069141, 0.007573647890239954, -0.011258071288466454, -0.009538942947983742, 0.06655648350715637, -0.01145249791443348, 0.03514271602034569, -0.04066655412316322, 0.037820544093847275, -0.02670583501458168, -0.027206387370824814, -0.03359299153089523, 0.022051967680454254, 0.03761078789830208, -0.019371481612324715, 0.03337821736931801, 0.008413607254624367, -0.08740968257188797, -0.058098141103982925, 0.011420969851315022, 0.004332919139415026, -0.010317596606910229, 0.020184088498353958, 0.010305945761501789, -0.00755687290802598, 0.008632692508399487, 0.015508933924138546, 0.03024662658572197, -0.022696826606988907, 0.013197900727391243, 0.02579217217862606, -0.08268952369689941, -0.023526005446910858, -0.03018033318221569, -0.021830188110470772, -0.0240993183106184, 0.03991168364882469]]}\n"
     ]
    }
   ],
   "source": [
    "# Testing the LegalQueryAnalyzer class\n",
    "query_output= LegalQueryAnalyzer().analyze_query(\"What is the legal definition of mens rea?\")\n",
    "print(query_output.keys())\n",
    "print(f\" Shape of Gemini Query Embedding: {np.array(query_output['gemini_embedding']).shape}\")\n",
    "print(f\" Shape of Gemini Query Embedding after Unsqeeze: {torch.tensor(query_output['gemini_embedding']).unsqueeze(0).shape}\")\n",
    "print(f\" Shape of Gemini Query Embedding after Sqeeze: {torch.tensor(query_output['gemini_embedding']).squeeze(0).shape}\")\n",
    "print(f\" Shape of Voyage Query Embedding: {np.array(query_output['voyage_embedding']).shape}\")\n",
    "print(query_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarichial Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New_Embeddings_2025\n",
      "New_Embeddings_2025\\chapters\\embeddings_gemini_text-005_chapters_semchunk.parquet\n",
      "\n",
      "Columns in embeddings_gemini_text-005_chapters_semchunk.parquet: ['chunk', 'Embedding']\n",
      "Loaded embeddings_gemini_text-005_chapters_semchunk.parquet (gemini - chapters)\n",
      "New_Embeddings_2025\n",
      "New_Embeddings_2025\\chapters\\embeddings_voyage_per_chapter_semchunked.parquet\n",
      "\n",
      "Columns in embeddings_voyage_per_chapter_semchunked.parquet: ['chunk', 'Embedding']\n",
      "Loaded embeddings_voyage_per_chapter_semchunked.parquet (voyager - chapters)\n",
      "New_Embeddings_2025\n",
      "New_Embeddings_2025\\pages\\embeddings_gemini_text-005_pages_semchunk.parquet\n",
      "\n",
      "Columns in embeddings_gemini_text-005_pages_semchunk.parquet: ['chunk', 'Embedding']\n",
      "Loaded embeddings_gemini_text-005_pages_semchunk.parquet (gemini - pages)\n",
      "New_Embeddings_2025\n",
      "New_Embeddings_2025\\pages\\embeddings_voyage_per_pages_semchunked.parquet\n",
      "\n",
      "Columns in embeddings_voyage_per_pages_semchunked.parquet: ['chunk', 'Embedding']\n",
      "Loaded embeddings_voyage_per_pages_semchunked.parquet (voyager - pages)\n",
      "New_Embeddings_2025\n",
      "New_Embeddings_2025\\sections\\embeddings_gemini_text-005.parquet\n",
      "\n",
      "Columns in embeddings_gemini_text-005.parquet: ['Section', 'Url', 'Content', 'Metadata', 'Processed_Content', 'Processed_Section', 'Processed_Metadata', 'Embedding']\n",
      "Loaded embeddings_gemini_text-005.parquet (gemini - sections)\n",
      "New_Embeddings_2025\n",
      "New_Embeddings_2025\\sections\\embeddings_voyage.parquet\n",
      "\n",
      "Columns in embeddings_voyage.parquet: ['Section', 'Url', 'Content', 'Metadata', 'Processed_Content', 'Processed_Section', 'Processed_Metadata', 'Embedding']\n",
      "Loaded embeddings_voyage.parquet (voyager - sections)\n",
      "dict_keys(['gemini_chapters', 'voyager_chapters', 'gemini_pages', 'voyager_pages', 'gemini_sections', 'voyager_sections'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings: 100%|██████████| 1/1 [00:00<00:00,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Shape of Gemini Query Embedding: torch.Size([1, 768])\n",
      " Shape of Section Projection: torch.Size([1647, 512])\n",
      " Shape of Page Projection: torch.Size([2176, 512])\n",
      " Shape of Chapter Projection: torch.Size([809, 512])\n",
      " Shape of Query Projection: torch.Size([1, 512])\n",
      " Shape of Section Scores: torch.Size([1647, 1])\n",
      " Shape of Page Scores: torch.Size([2176, 1])\n",
      " Shape of Chapter Scores: torch.Size([809, 1])\n",
      " Shape of Section Top: torch.Size([5, 512])\n",
      " Shape of Page Top: torch.Size([5, 512])\n",
      " Shape of Chapter Top: torch.Size([5, 512])\n",
      "Fused embedding shape: torch.Size([1, 768])\n",
      "Top k section indices: tensor([461, 142, 677, 953, 494])\n",
      "Top k page indices: tensor([2033, 1385, 1959, 1027, 1904])\n",
      "Top k chapter indices: tensor([524, 177, 518, 181, 525])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class HierarchicalFusion(nn.Module):\n",
    "    def __init__(self, embedding_dim=768, hidden_dim=512, top_k=5):\n",
    "        super(HierarchicalFusion, self).__init__()\n",
    "        self.top_k = top_k\n",
    "        \n",
    "        # Projection layers for each granularity into a common hidden space.\n",
    "        self.proj_section = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.proj_page    = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.proj_chapter = nn.Linear(embedding_dim, hidden_dim)\n",
    "        \n",
    "        # Projection for the query (optional, here we project to the same hidden space)\n",
    "        self.proj_query   = nn.Linear(embedding_dim, hidden_dim)\n",
    "        \n",
    "        # Fusion layers:\n",
    "        # First, fuse sections and pages (after top-k selection)\n",
    "        self.fusion_sp    = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        # Next, fuse the above result with chapters\n",
    "        self.fusion_all   = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        \n",
    "        # Decoder layer to project the final fused vector back to original embedding space.\n",
    "        self.decoder      = nn.Linear(hidden_dim, embedding_dim)\n",
    "    \n",
    "    def forward(self, sections, pages, chapters, query):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sections: Tensor of shape (num_sections, embedding_dim)\n",
    "            pages:    Tensor of shape (num_pages, embedding_dim)\n",
    "            chapters: Tensor of shape (num_chapters, embedding_dim)\n",
    "            query:    Tensor of shape (1, embedding_dim)\n",
    "        \n",
    "        Returns:\n",
    "            decoded: Fused representation (1, embedding_dim)\n",
    "            info: Dictionary containing scores and top-k indices for each level.\n",
    "        \"\"\"\n",
    "        # Project embeddings into a common hidden dimension\n",
    "        sec_proj   = F.relu(self.proj_section(sections))  # (num_sections, hidden_dim)\n",
    "        page_proj  = F.relu(self.proj_page(pages))          # (num_pages, hidden_dim)\n",
    "        chap_proj  = F.relu(self.proj_chapter(chapters))      # (num_chapters, hidden_dim)\n",
    "        query_proj = F.relu(self.proj_query(query))          # (1, hidden_dim)\n",
    "        \n",
    "        print(f\" Shape of Section Projection: {sec_proj.shape}\")\n",
    "        print(f\" Shape of Page Projection: {page_proj.shape}\")\n",
    "        print(f\" Shape of Chapter Projection: {chap_proj.shape}\")\n",
    "        print(f\" Shape of Query Projection: {query_proj.shape}\")\n",
    "        # Compute similarity scores using dot product.\n",
    "        # (Alternatively, you could use cosine similarity.)\n",
    "        sec_scores   = torch.matmul(sec_proj, query_proj.T)   # (num_sections, 1)\n",
    "        page_scores  = torch.matmul(page_proj, query_proj.T)    # (num_pages, 1)\n",
    "        chap_scores  = torch.matmul(chap_proj, query_proj.T)    # (num_chapters, 1)\n",
    "        \n",
    "        print(f\" Shape of Section Scores: {sec_scores.shape}\")\n",
    "        print(f\" Shape of Page Scores: {page_scores.shape}\")    \n",
    "        print(f\" Shape of Chapter Scores: {chap_scores.shape}\")\n",
    "\n",
    "        # Select top-k rows based on the similarity scores (squeeze to remove last dim)\n",
    "        _, sec_top_idx  = torch.topk(sec_scores.squeeze(), self.top_k)\n",
    "        _, page_top_idx = torch.topk(page_scores.squeeze(), self.top_k)\n",
    "        _, chap_top_idx = torch.topk(chap_scores.squeeze(), self.top_k)\n",
    "        \n",
    "\n",
    "        # Gather the top-k embeddings from each level\n",
    "        sec_top  = sec_proj[sec_top_idx]   # (top_k, hidden_dim)\n",
    "        page_top = page_proj[page_top_idx]  # (top_k, hidden_dim)\n",
    "        chap_top = chap_proj[chap_top_idx]  # (top_k, hidden_dim)\n",
    "        \n",
    "        print(f\" Shape of Section Top: {sec_top.shape}\")\n",
    "        print(f\" Shape of Page Top: {page_top.shape}\")  \n",
    "        print(f\" Shape of Chapter Top: {chap_top.shape}\")\n",
    "        \n",
    "        # Hierarchical fusion:\n",
    "        # 1. Fuse sections and pages by aggregating (here, we use the mean)\n",
    "        sec_fused  = sec_top.mean(dim=0, keepdim=True)    # (1, hidden_dim)\n",
    "        page_fused = page_top.mean(dim=0, keepdim=True)    # (1, hidden_dim)\n",
    "        sp_concat  = torch.cat([sec_fused, page_fused], dim=-1)  # (1, hidden_dim * 2)\n",
    "        sp_fused   = F.relu(self.fusion_sp(sp_concat))    # (1, hidden_dim)\n",
    "        \n",
    "        # 2. Fuse the above with chapters\n",
    "        chap_fused = chap_top.mean(dim=0, keepdim=True)    # (1, hidden_dim)\n",
    "        all_concat = torch.cat([sp_fused, chap_fused], dim=-1)   # (1, hidden_dim * 2)\n",
    "        all_fused  = F.relu(self.fusion_all(all_concat))   # (1, hidden_dim)\n",
    "        \n",
    "        # Decode back to the original embedding dimension.\n",
    "        decoded = self.decoder(all_fused)   # (1, embedding_dim)\n",
    "        \n",
    "        # Pack additional information (like scores and indices) if you need them for debugging or retrieval.\n",
    "        info = {\n",
    "            'sec_scores': sec_scores,\n",
    "            'page_scores': page_scores,\n",
    "            'chap_scores': chap_scores,\n",
    "            'sec_top_idx': sec_top_idx,\n",
    "            'page_top_idx': page_top_idx,\n",
    "            'chap_top_idx': chap_top_idx\n",
    "        }\n",
    "        \n",
    "        return decoded, info\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Dummy embeddings for demonstration\n",
    "    query= \"\"\" Whoever harbors or conceals any person who he knows, or has reasonable grounds to believe or suspect, has committed, or is about to commit, an offense under this section, shall be fined under this title or imprisoned not more than ten years, or both. \"\"\"\n",
    "    loader= LegalEmbeddingLoader(\"New_Embeddings_2025\")\n",
    "    gemini_embeddings, voyager_embeddings, metadata = loader.load_embeddings()\n",
    "    print(metadata.keys())\n",
    "\n",
    "    query_embedding= EmbeddingGenerator().get_embeddings_gemini([query])\n",
    "    print(f\" Shape of Gemini Query Embedding: {torch.tensor(query_embedding).shape}\")\n",
    "\n",
    "    sections = gemini_embeddings['sections']  # e.g., 1663 section embeddings\n",
    "    pages    = gemini_embeddings['pages']  # e.g., 2176 page embeddings\n",
    "    chapters = gemini_embeddings['chapters']  # e.g., 809 chapter embeddings\n",
    "    query    = torch.tensor(query_embedding)     # Query embedding\n",
    "    \n",
    "    # Initialize the model (you can adjust hidden_dim and top_k as needed)\n",
    "    model = HierarchicalFusion(embedding_dim=768, hidden_dim=512, top_k=5)\n",
    "    \n",
    "    # Forward pass through the network\n",
    "    fused_embedding, details = model(sections, pages, chapters, query)\n",
    "    \n",
    "    print(\"Fused embedding shape:\", fused_embedding.shape)  # Expect (1, 768)\n",
    "    print(\"Top k section indices:\", details['sec_top_idx'])\n",
    "    print(\"Top k page indices:\", details['page_top_idx'])\n",
    "    print(\"Top k chapter indices:\", details['chap_top_idx'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In all cases of murder or manslaughter, the offense shall be deemed to have been committed at the place where the injury was inflicted, or the poison administered or other means employed which caused the death, without regard to the place where the death occurs. (June 25, 1948, ch. 645, 62 Stat. 826.)\n",
      "\n",
      "Historical and Revision Notes\n",
      "Based on title 18, U.S.C., 1940 ed., §553 (Mar. 4, 1909, ch. 321, §336, 35 Stat. 1152).\n",
      "\n",
      "-----------------------------------------------\n",
      "1996—Subsec. (a). Pub. L. 104–132, §502(1)(A), substituted \"nuclear material or nuclear byproduct\n",
      "material\" for \"nuclear material\" wherever appearing.\n",
      "Subsec. (a)(1)(A). Pub. L. 104–132, §502(1)(B)(i), inserted \"or to the environment\" after \"damage to\n",
      "property\".\n",
      "Subsec. (a)(1)(B). Pub. L. 104–132, §502(1)(B)(ii), amended subpar. (B) generally. Prior to amendment,\n",
      "subpar. (B) read as follows: \"knows that circumstances exist which are likely to cause the death of or serious\n",
      "bodily injury to any person or substantial damage to property;\".\n",
      "Subsec. (a)(6). Pub. L. 104–132, §502(1)(C), inserted \"or to the environment\" after \"damage to property\".\n",
      "Subsec. (c)(2). Pub. L. 104–132, §502(2)(A), amended par. (2) generally. Prior to amendment, par. (2) read\n",
      "as follows: \"the defendant is a national of the United States, as defined in section 101 of the Immigration and\n",
      "Nationality Act (8 U.S.C. 1101);\".\n",
      "Subsec. (c)(3). Pub. L. 104–132, §502(2)(B), struck out \"at the time of the offense the nuclear material is in\n",
      "use, storage, or transport, for peaceful purposes, and\" before \"after the conduct\" and struck out \"or\" at end.\n",
      "Subsec. (c)(4). Pub. L. 104–132, §502(2)(C), substituted \"nuclear material or nuclear byproduct material\"\n",
      "for \"nuclear material for peaceful purposes\" and \"; or\" for period at end.\n",
      "Subsec. (c)(5). Pub. L. 104–132, §502(2)(D), added par. (5).\n",
      "Subsec. (f)(1)(A). Pub. L. 104–132, §502(3)(A)(i), struck out \"with an isotopic concentration not in excess\n",
      "of 80 percent plutonium 238\" after \"plutonium\".\n",
      "Subsec. (f)(1)(C). Pub. L. 104–132, §502(3)(A)(ii), substituted \"enriched uranium, defined as uranium\" for\n",
      "\"uranium\".\n",
      "[Release Point 118-78]\n",
      "\n",
      "-----------------------------------------------\n",
      "Sec. 643. Accounting generally for public money.\n",
      "\n",
      "Whoever, being an officer, employee or agent of the United States or of any department or agency thereof, having received public money which he is not authorized to retain as salary, pay, or emolument, fails to render his accounts for the same as provided by law is guilty of embezzlement, and shall be fined under this title or in a sum equal to the amount of the money embezzled, whichever is greater, or imprisoned not more than ten years, or both; but if the amount embezzled does not exceed $1,000, he shall be fined under this title or imprisoned not more than one year, or both.\n",
      "(June 25, 1948, ch. 645, 62 Stat. 726; Pub. L. 103–322, title XXXIII, §330016(1)(H), (2)(G), Sept. 13, 1994, 108 Stat. 2147, 2148; Pub. L. 104–294, title VI, §606(a), Oct. 11, 1996, 110 Stat. 3511.)\n",
      "\n",
      "\n",
      "Sec. 644. Banker receiving unauthorized deposit of public money.\n",
      "\n",
      "Whoever, not being an authorized depositary of public moneys, knowingly receives from any disbursing officer, or collector of internal revenue, or other agent of the United States, any public money on deposit, or by way of loan or accommodation, with or without interest, or otherwise than in payment of a debt against the United States, or uses, transfers, converts, appropriates, or applies any portion of the public money for any purpose not prescribed by law is guilty of embezzlement and shall be fined under this title or not more than the amount so embezzled, whichever is greater, or imprisoned not more than ten years, or both; but if the amount embezzled does not exceed $1,000, he shall be fined not more than $1,000 or imprisoned not more than one year, or both.\n",
      "(June 25, 1948, ch. 645, 62 Stat. 726; Pub. L. 103–322, title XXXIII, §330016(2)(G), Sept. 13, 1994, 108 Stat. 2148; Pub. L. 104–294, title VI, §606(a), Oct. 11, 1996, 110 Stat. 3511.)\n",
      "\n",
      "\n",
      "Sec. 645. Court officers generally.\n",
      "\n",
      "Whoever, being a United States marshal, clerk, receiver, referee, trustee, or other officer of a United States court, or any deputy, assistant, or employee of any such officer, retains or converts to his own use or to the use of another or after demand by the party entitled thereto, unlawfully retains any money coming into his hands by virtue of his official relation, position or employment, is guilty of embezzlement and shall, where the offense is not otherwise punishable by enactment of Congress, be fined under this title or not more than double the value of the money so embezzled, whichever is greater, or imprisoned not more than ten years, or both; but if the amount embezzled does not exceed $1,000, he shall be fined under this title or imprisoned not more than one year, or both.\n",
      "\n",
      "It shall not be a defense that the accused person had any interest in such moneys or fund.\n",
      "(June 25, 1948, ch. 645, 62 Stat. 726; Pub. L. 103–322, title XXXIII, §330016(1)(H), (2)(G), Sept. 13, 1994, 108 Stat. 2147, 2148; Pub. L. 104–294, title VI, §606(a), Oct. 11, 1996, 110 Stat. 3511.)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the embeddings files\n",
    "gemini_sections_df = pd.read_parquet(r\"New_Embeddings_2025\\sections\\embeddings_gemini_text-005.parquet\")\n",
    "gemini_pages_df = pd.read_parquet(r\"New_Embeddings_2025\\pages\\embeddings_gemini_text-005_pages_semchunk.parquet\")\n",
    "gemini_chapters_df = pd.read_parquet(r\"New_Embeddings_2025\\chapters\\embeddings_gemini_text-005_chapters_semchunk.parquet\")\n",
    "\n",
    "voyage_sections_df = pd.read_parquet(r\"New_Embeddings_2025\\sections\\embeddings_voyage.parquet\")\n",
    "voyage_pages_df = pd.read_parquet(r\"New_Embeddings_2025\\pages\\embeddings_voyage_per_pages_semchunked.parquet\")\n",
    "voyage_chapters_df = pd.read_parquet(r\"New_Embeddings_2025\\chapters\\embeddings_voyage_per_chapter_semchunked.parquet\")\n",
    "\n",
    "def get_processed_content_by_index(index, source, model):\n",
    "    \"\"\"\n",
    "    Retrieve the Processed_Content or chunk based on the specified index and source.\n",
    "\n",
    "    Args:\n",
    "        index (int): The row index to retrieve.\n",
    "        source (str): The source dataset (\"gemini_text\", \"gemini_pages\", or \"voyage\").\n",
    "\n",
    "    Returns:\n",
    "        str: The corresponding Processed_Content or chunk.\n",
    "    \"\"\"\n",
    "    if model== \"gemini\":\n",
    "        if source == \"sections\":\n",
    "            return gemini_sections_df.loc[index, \"Processed_Content\"] if index in gemini_sections_df.index else None\n",
    "        elif source == \"pages\":\n",
    "            return gemini_pages_df.loc[index, \"chunk\"] if index in gemini_pages_df.index else None\n",
    "        elif source == \"chapters\":\n",
    "            return gemini_chapters_df.loc[index, \"chunk\"] if index in gemini_chapters_df.index else None\n",
    "        else:\n",
    "            return \"Invalid source specified.\"\n",
    "    if model == \"voyage\":\n",
    "        if source == \"sections\":\n",
    "            return voyage_sections_df.loc[index, \"Processed_Content\"] if index in voyage_sections_df.index else None\n",
    "        elif source == \"pages\":\n",
    "            return voyage_pages_df.loc[index, \"chunk\"] if index in voyage_pages_df.index else None\n",
    "        elif source == \"chapters\":\n",
    "            return voyage_chapters_df.loc[index, \"chunk\"] if index in voyage_chapters_df.index else None\n",
    "        else:\n",
    "            return \"Invalid source specified.\"\n",
    "\n",
    "# Example Usage:\n",
    "index_to_fetch = 456  # Change this index based on your needs\n",
    "print(get_processed_content_by_index(index_to_fetch, source=\"sections\", model= \"gemini\"))  # For Gemini Text\n",
    "print(\"-----------------------------------------------\")\n",
    "print(get_processed_content_by_index(index_to_fetch, source=\"pages\", model= \"gemini\"))  # For Gemini Pages\n",
    "print(\"-----------------------------------------------\")\n",
    "print(get_processed_content_by_index(index_to_fetch, source=\"chapters\", model= \"gemini\"))  # For Voyage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Cosine Similiarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LegalEmbeddingMatcher:\n",
    "    def __init__(self, gemini_embeddings, voyager_embeddings):\n",
    "        \"\"\"\n",
    "        Initialize with embeddings for Gemini and Voyager models.\n",
    "        :param gemini_embeddings: Dictionary with keys 'chapters', 'pages', 'sections' -> Tensor embeddings\n",
    "        :param voyager_embeddings: Dictionary with keys 'chapters', 'pages', 'sections' -> Tensor embeddings\n",
    "        \"\"\"\n",
    "        self.gemini_embeddings = {k: torch.tensor(v) for k, v in gemini_embeddings.items()}\n",
    "        self.voyager_embeddings = {k: torch.tensor(v) for k, v in voyager_embeddings.items()}\n",
    "        \n",
    "        # Ensure embedding dimensions match using a linear transformation\n",
    "        gemini_dim = self.gemini_embeddings['chapters'].shape[1]\n",
    "        voyager_dim = self.voyager_embeddings['chapters'].shape[1]\n",
    "        \n",
    "        self.transform = torch.nn.Linear(voyager_dim, gemini_dim)\n",
    "\n",
    "        # Transform all Voyager embeddings at initialization\n",
    "        self.voyager_embeddings = {\n",
    "            k: self.transform(v) for k, v in self.voyager_embeddings.items()\n",
    "        }\n",
    "\n",
    "    def match_query(self, query_embedding, model_weights):\n",
    "        \"\"\"\n",
    "        Match query embedding against stored embeddings separately for each model.\n",
    "        :param query_embedding: Dictionary with query embeddings from Gemini and Voyager.\n",
    "        :param model_weights: Dictionary with weights for each model {'gemini': w1, 'voyager': w2}.\n",
    "        :return: Top 10 matches across all granularities with cosine similarity scores.\n",
    "        \"\"\"\n",
    "        query_gemini = torch.tensor(query_embedding['gemini'])\n",
    "        query_voyager = torch.tensor(query_embedding['voyager'])\n",
    "\n",
    "        # Transform Voyager query embedding to match Gemini's dimension\n",
    "        query_voyager = self.transform(query_voyager)\n",
    "\n",
    "        similarities = []  # Store (granularity, model, index, weighted similarity)\n",
    "\n",
    "        for level in ['chapters', 'pages', 'sections']:\n",
    "            gemini_sim = F.cosine_similarity(query_gemini, self.gemini_embeddings[level]) * model_weights['gemini']\n",
    "            voyager_sim = F.cosine_similarity(query_voyager, self.voyager_embeddings[level]) * model_weights['voyager']\n",
    "\n",
    "            # Collect all similarities with their granularity and model\n",
    "            similarities.extend([\n",
    "                (level, 'gemini', idx.item(), gemini_sim[idx].item()) for idx in range(len(gemini_sim))\n",
    "            ])\n",
    "            similarities.extend([\n",
    "                (level, 'voyager', idx.item(), voyager_sim[idx].item()) for idx in range(len(voyager_sim))\n",
    "            ])\n",
    "\n",
    "        # Sort by similarity score in descending order and return top 10\n",
    "        top_matches = sorted(similarities, key=lambda x: x[3], reverse=True)[:10]\n",
    "\n",
    "        return top_matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New_Embeddings_2025\n",
      "New_Embeddings_2025\\chapters\\embeddings_gemini_text-005_chapters_semchunk.parquet\n",
      "\n",
      "Columns in embeddings_gemini_text-005_chapters_semchunk.parquet: ['chunk', 'Embedding']\n",
      "Loaded embeddings_gemini_text-005_chapters_semchunk.parquet (gemini - chapters)\n",
      "New_Embeddings_2025\n",
      "New_Embeddings_2025\\chapters\\embeddings_voyage_per_chapter_semchunked.parquet\n",
      "\n",
      "Columns in embeddings_voyage_per_chapter_semchunked.parquet: ['chunk', 'Embedding']\n",
      "Loaded embeddings_voyage_per_chapter_semchunked.parquet (voyager - chapters)\n",
      "New_Embeddings_2025\n",
      "New_Embeddings_2025\\pages\\embeddings_gemini_text-005_pages_semchunk.parquet\n",
      "\n",
      "Columns in embeddings_gemini_text-005_pages_semchunk.parquet: ['chunk', 'Embedding']\n",
      "Loaded embeddings_gemini_text-005_pages_semchunk.parquet (gemini - pages)\n",
      "New_Embeddings_2025\n",
      "New_Embeddings_2025\\pages\\embeddings_voyage_per_pages_semchunked.parquet\n",
      "\n",
      "Columns in embeddings_voyage_per_pages_semchunked.parquet: ['chunk', 'Embedding']\n",
      "Loaded embeddings_voyage_per_pages_semchunked.parquet (voyager - pages)\n",
      "New_Embeddings_2025\n",
      "New_Embeddings_2025\\sections\\embeddings_gemini_text-005.parquet\n",
      "\n",
      "Columns in embeddings_gemini_text-005.parquet: ['Section', 'Url', 'Content', 'Metadata', 'Processed_Content', 'Processed_Section', 'Processed_Metadata', 'Embedding']\n",
      "Loaded embeddings_gemini_text-005.parquet (gemini - sections)\n",
      "New_Embeddings_2025\n",
      "New_Embeddings_2025\\sections\\embeddings_voyage.parquet\n",
      "\n",
      "Columns in embeddings_voyage.parquet: ['Section', 'Url', 'Content', 'Metadata', 'Processed_Content', 'Processed_Section', 'Processed_Metadata', 'Embedding']\n",
      "Loaded embeddings_voyage.parquet (voyager - sections)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['chapters', 'pages', 'sections'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mkolla1\\AppData\\Local\\Temp\\ipykernel_15316\\2006922442.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.gemini_embeddings = {k: torch.tensor(v) for k, v in gemini_embeddings.items()}\n",
      "C:\\Users\\mkolla1\\AppData\\Local\\Temp\\ipykernel_15316\\2006922442.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.voyager_embeddings = {k: torch.tensor(v) for k, v in voyager_embeddings.items()}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'item'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(gemini_embeddings\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m     22\u001b[0m matcher \u001b[38;5;241m=\u001b[39m LegalEmbeddingMatcher(gemini_embeddings, voyager_embeddings)\n\u001b[1;32m---> 23\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmatcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(results)\n",
      "Cell \u001b[1;32mIn[74], line 45\u001b[0m, in \u001b[0;36mLegalEmbeddingMatcher.match_query\u001b[1;34m(self, query_embedding, model_weights)\u001b[0m\n\u001b[0;32m     42\u001b[0m     voyager_sim \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcosine_similarity(query_voyager, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvoyager_embeddings[level]) \u001b[38;5;241m*\u001b[39m model_weights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvoyager\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# Collect all similarities with their granularity and model\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     similarities\u001b[38;5;241m.\u001b[39mextend([\n\u001b[0;32m     46\u001b[0m         (level, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgemini\u001b[39m\u001b[38;5;124m'\u001b[39m, idx\u001b[38;5;241m.\u001b[39mitem(), gemini_sim[idx]\u001b[38;5;241m.\u001b[39mitem()) \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(gemini_sim))\n\u001b[0;32m     47\u001b[0m     ])\n\u001b[0;32m     48\u001b[0m     similarities\u001b[38;5;241m.\u001b[39mextend([\n\u001b[0;32m     49\u001b[0m         (level, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvoyager\u001b[39m\u001b[38;5;124m'\u001b[39m, idx\u001b[38;5;241m.\u001b[39mitem(), voyager_sim[idx]\u001b[38;5;241m.\u001b[39mitem()) \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(voyager_sim))\n\u001b[0;32m     50\u001b[0m     ])\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Sort by similarity score in descending order and return top 10\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[74], line 46\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     voyager_sim \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcosine_similarity(query_voyager, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvoyager_embeddings[level]) \u001b[38;5;241m*\u001b[39m model_weights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvoyager\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# Collect all similarities with their granularity and model\u001b[39;00m\n\u001b[0;32m     45\u001b[0m     similarities\u001b[38;5;241m.\u001b[39mextend([\n\u001b[1;32m---> 46\u001b[0m         (level, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgemini\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43midx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m(), gemini_sim[idx]\u001b[38;5;241m.\u001b[39mitem()) \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(gemini_sim))\n\u001b[0;32m     47\u001b[0m     ])\n\u001b[0;32m     48\u001b[0m     similarities\u001b[38;5;241m.\u001b[39mextend([\n\u001b[0;32m     49\u001b[0m         (level, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvoyager\u001b[39m\u001b[38;5;124m'\u001b[39m, idx\u001b[38;5;241m.\u001b[39mitem(), voyager_sim[idx]\u001b[38;5;241m.\u001b[39mitem()) \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(voyager_sim))\n\u001b[0;32m     50\u001b[0m     ])\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Sort by similarity score in descending order and return top 10\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'item'"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "\n",
    "loader = LegalEmbeddingLoader('New_Embeddings_2025')\n",
    "gemini_embeddings, voyager_embeddings, metadata = loader.load_embeddings()\n",
    "\n",
    "# Analyze query and get model weights\n",
    "query_analyzer=LegalQueryAnalyzer()\n",
    "query=\"\"\"\n",
    "willfully uses the mails for the mailing, carriage in the mails, or delivery of any sexually oriented advertisement in violation of section 3010 of title 39, or willfully violates any regulations of the Board of Governors issued under such section; or\n",
    "\n",
    "(2) sells, leases, rents, lends, exchanges, or licenses the use of, or, except for the purpose expressly authorized by section 3010 of title 39, uses a mailing list maintained by the Board of Governors under such section;\n",
    "\n",
    "shall be fined under this title or imprisoned not more than five years, or both, for the first offense, and shall be fined under this title or imprisoned not more than ten years, or both, for any second or subsequent offense.\n",
    "\"\"\"\n",
    "\n",
    "query_output=query_analyzer.analyze_query(query)\n",
    "\n",
    "\n",
    "query_embedding = { 'gemini': query_output['gemini_embedding'], 'voyager': query_output['voyage_embedding'] }\n",
    "model_weights = query_output['weights']\n",
    "print(gemini_embeddings.keys())\n",
    "matcher = LegalEmbeddingMatcher(gemini_embeddings, voyager_embeddings)\n",
    "results = matcher.match_query(query_embedding, model_weights)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Level Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiLevelAttention(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Enhanced attention mechanism with separate query embeddings for Gemini and Voyager.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __init__(self, output_dim=1024):\n",
    "#         super(MultiLevelAttention, self).__init__()\n",
    "        \n",
    "#         # Model dimensions\n",
    "#         self.gemini_dim = 768\n",
    "#         self.voyager_dim = 1024\n",
    "#         self.output_dim = output_dim\n",
    "#         self.granularities = ['sections', 'chapters', 'pages']\n",
    "#         self.cross_attention_weights = {}\n",
    "\n",
    "#         # Query projectors\n",
    "#         self.gemini_query_projector = nn.Linear(768, output_dim)\n",
    "#         self.voyager_query_projector = nn.Linear(1024, output_dim)\n",
    "        \n",
    "#         # Document projectors\n",
    "#         self.gemini_projector = nn.Linear(self.gemini_dim, output_dim)\n",
    "#         self.voyager_projector = nn.Linear(self.voyager_dim, output_dim)\n",
    "        \n",
    "#         # Aggregation\n",
    "#         self.aggregation_layer = nn.Linear(output_dim, output_dim)\n",
    "#         self.layer_norm = nn.LayerNorm(output_dim)\n",
    "\n",
    "#     def forward(self, gemini_embeddings, voyager_embeddings, gemini_query_embedding, voyager_query_embedding, weights):\n",
    "#         # Project queries\n",
    "#         gemini_query_projected = self.gemini_query_projector(gemini_query_embedding)\n",
    "#         voyager_query_projected = self.voyager_query_projector(voyager_query_embedding)\n",
    "        \n",
    "#         granularity_results = {}\n",
    "#         diagnostics = {}\n",
    "        \n",
    "#         # Store granularities\n",
    "#         gemini_granularities = []\n",
    "#         voyager_granularities = []\n",
    "\n",
    "\n",
    "#         for granularity in self.granularities:\n",
    "#             print(f\"Processing granularity: {granularity}\")\n",
    "#             gemini_emb = gemini_embeddings[granularity]\n",
    "#             voyager_emb = voyager_embeddings[granularity]\n",
    "#             print(f\"Shape of Gemini Embedding: {gemini_emb.shape}\")\n",
    "#             print(f\"Shape of Voyager Embedding: {voyager_emb.shape}\")\n",
    "#             print(\"--------------------------------------------------------\")\n",
    "\n",
    "            \n",
    "#             # Project document embeddings\n",
    "#             gemini_proj = self.gemini_projector(gemini_emb)\n",
    "#             voyager_proj = self.voyager_projector(voyager_emb)\n",
    "#             print(f\"Shape of Gemini Projected Embedding: {gemini_proj.shape}\")\n",
    "#             print(f\"Shape of Voyager Projected Embedding: {voyager_proj.shape}\")\n",
    "#             print(\"--------------------------------------------------------\")\n",
    "\n",
    "#             # # Ensure correct size\n",
    "#             # gemini_proj = gemini_proj[:, :self.output_dim]  # Ensure correct size\n",
    "#             # voyager_proj = voyager_proj[:, :self.output_dim]\n",
    "#             # print(f\"Shape of Gemini Projected Embedding After Ensuring: {gemini_proj.shape}\")\n",
    "#             # print(f\"Shape of Voyager Projected Embedding After Ensuring: {voyager_proj.shape}\")\n",
    "#             # print(\"--------------------------------------------------------\")\n",
    "\n",
    "#             print(f\"Shape of Gemini Query Embedding: {gemini_query_projected.shape}\")\n",
    "#             print(f\"Shape of Voyager Query Embedding: {voyager_query_projected.shape}\")\n",
    "#             print(\"--------------------------------------------------------\")\n",
    "#             # Calculate similarities\n",
    "#             gemini_similarity = F.cosine_similarity(\n",
    "#                 gemini_proj,\n",
    "#                 gemini_query_projected.expand_as(gemini_proj),\n",
    "#                 dim=1\n",
    "#             ).unsqueeze(0) # Add batch dimension for softmax\n",
    "            \n",
    "#             voyager_similarity = F.cosine_similarity(\n",
    "#                 voyager_proj,\n",
    "#                 voyager_query_projected.expand_as(voyager_proj),\n",
    "#                 dim=1\n",
    "#             ).unsqueeze(0)\n",
    "#             print(f\"Shape of Gemini Similarity: {gemini_similarity.shape}\")\n",
    "#             print(f\"Shape of Voyager Similarity: {voyager_similarity.shape}\")\n",
    "#             print(\"--------------------------------------------------------\")\n",
    "            \n",
    "#             # Normalize similarities\n",
    "#             gemini_prob_weights = F.softmax(gemini_similarity, dim=0)\n",
    "#             voyager_prob_weights = F.softmax(voyager_similarity, dim=0)\n",
    "#             print(f\"Shape of Gemini Weights After Softmax: {gemini_prob_weights.shape}\")\n",
    "#             print(f\"Shape of Voyager Weights After Softmax: {voyager_prob_weights.shape}\")\n",
    "#             print(\"--------------------------------------------------------\")\n",
    "\n",
    "\n",
    "#             # Weighted sum with proper dimensions\n",
    "#             # gemini_weighted = torch.matmul(gemini_prob_weights, gemini_proj)\n",
    "#             # voyager_weighted = torch.matmul(voyager_prob_weights, voyager_proj)\n",
    "#             gemini_weighted = (gemini_prob_weights.T * gemini_proj).sum(dim=1, keepdim=True).T\n",
    "#             voyager_weighted = (voyager_prob_weights.T * voyager_proj).sum(dim=1, keepdim=True).T\n",
    "\n",
    "#             print(f\"Shape of Gemini Weighted Embedding: {gemini_weighted.shape}\")\n",
    "#             print(f\"Shape of Voyager Weighted Embedding: {voyager_weighted.shape}\")\n",
    "#             print(\"--------------------------------------------------------\")\n",
    "\n",
    "#             # Dimension alignment\n",
    "#             max_dim = max(gemini_weighted.shape[-1], voyager_weighted.shape[-1])\n",
    "#             gemini_weighted = F.pad(gemini_weighted, (0, max_dim - gemini_weighted.shape[-1]))\n",
    "#             voyager_weighted = F.pad(voyager_weighted, (0, max_dim - voyager_weighted.shape[-1]))\n",
    "\n",
    "#             # Store processed embeddings for each granularity\n",
    "#             gemini_granularities.append(gemini_weighted)\n",
    "#             voyager_granularities.append(voyager_weighted)\n",
    "\n",
    "#             # Combine model embeddings\n",
    "#             granularity_combined = (\n",
    "#                 weights['gemini'] * gemini_weighted +\n",
    "#                 weights['voyager'] * voyager_weighted\n",
    "#             )\n",
    "            \n",
    "#             granularity_results[granularity] = granularity_combined\n",
    "#             print(f\"Granularity Completed: {granularity}\")\n",
    "        \n",
    "#         # Aggregate results\n",
    "#         combined_embedding = torch.stack([granularity_results[gran] for gran in self.granularities], dim=0).mean(dim=0)\n",
    "\n",
    "#         self.cross_attention_weights = {\n",
    "#             'gemini': gemini_prob_weights.detach().cpu(),\n",
    "#             'voyager': voyager_prob_weights.detach().cpu(),\n",
    "#             'combined': combined_embedding.detach().cpu()\n",
    "#         }\n",
    "\n",
    "#         # Final processing\n",
    "#         fused_embedding = self.layer_norm(self.aggregation_layer(combined_embedding))\n",
    "        \n",
    "#         return fused_embedding, {\n",
    "#             'processed_embeddings': granularity_results,\n",
    "#             'diagnostics': diagnostics,\n",
    "#             'cross_attention_weights': self.cross_attention_weights\n",
    "#         }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiLevelAttention(nn.Module):\n",
    "    def __init__(self, output_dim=1024, top_n=5):\n",
    "        super(MultiLevelAttention, self).__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.top_n = top_n\n",
    "        self.granularities = ['sections', 'chapters', 'pages']\n",
    "\n",
    "        # Query projectors\n",
    "        self.gemini_query_projector = nn.Linear(768, 768)\n",
    "        self.voyager_query_projector = nn.Linear(1024, output_dim)\n",
    "        \n",
    "        # Document projectors\n",
    "        self.gemini_projector = nn.Linear(768, 768)\n",
    "        self.voyager_projector = nn.Linear(1024, output_dim)\n",
    "\n",
    "        # Aggregation\n",
    "        self.layer_norm = nn.LayerNorm(output_dim)\n",
    "\n",
    "    def forward(self, gemini_embeddings, voyager_embeddings, gemini_query_embedding, voyager_query_embedding):\n",
    "        # Project query embeddings\n",
    "        gemini_query_projected = self.gemini_query_projector(gemini_query_embedding)\n",
    "        voyager_query_projected = self.voyager_query_projector(voyager_query_embedding)\n",
    "\n",
    "        # Normalize query embeddings\n",
    "        gemini_query_norm = F.normalize(gemini_query_projected, p=2, dim=1)\n",
    "        voyager_query_norm = F.normalize(voyager_query_projected, p=2, dim=1)\n",
    "\n",
    "        # Process embeddings per granularity\n",
    "        # gemini_section_proj = self.gemini_projector(gemini_embeddings['sections'])\n",
    "        # gemini_chapter_proj = self.gemini_projector(gemini_embeddings['chapters'])\n",
    "        # gemini_page_proj = self.gemini_projector(gemini_embeddings['pages'])\n",
    "\n",
    "        # voyager_section_proj = self.voyager_projector(voyager_embeddings['sections'])\n",
    "        # voyager_chapter_proj = self.voyager_projector(voyager_embeddings['chapters'])\n",
    "        # voyager_page_proj = self.voyager_projector(voyager_embeddings['pages'])\n",
    "\n",
    "        gemini_section_proj = gemini_embeddings['sections']\n",
    "        gemini_chapter_proj = gemini_embeddings['chapters']\n",
    "        gemini_page_proj = gemini_embeddings['pages']\n",
    "\n",
    "        voyager_section_proj = voyager_embeddings['sections']\n",
    "        voyager_chapter_proj = voyager_embeddings['chapters']\n",
    "        voyager_page_proj = voyager_embeddings['pages']\n",
    "\n",
    "        # Normalize document embeddings\n",
    "        gemini_section_norm = F.normalize(gemini_section_proj, p=2, dim=1)\n",
    "        gemini_chapter_norm = F.normalize(gemini_chapter_proj, p=2, dim=1)\n",
    "        gemini_page_norm = F.normalize(gemini_page_proj, p=2, dim=1)\n",
    "\n",
    "        voyager_section_norm = F.normalize(voyager_section_proj, p=2, dim=1)\n",
    "        voyager_chapter_norm = F.normalize(voyager_chapter_proj, p=2, dim=1)\n",
    "        voyager_page_norm = F.normalize(voyager_page_proj, p=2, dim=1)\n",
    "\n",
    "        # Compute cosine similarity using matrix multiplication\n",
    "        gemini_section_similarities = torch.matmul(gemini_section_norm, gemini_query_norm.T).squeeze(1)\n",
    "        gemini_chapter_similarities = torch.matmul(gemini_chapter_norm, gemini_query_norm.T).squeeze(1)\n",
    "        gemini_page_similarities = torch.matmul(gemini_page_norm, gemini_query_norm.T).squeeze(1)\n",
    "\n",
    "        voyager_section_similarities = torch.matmul(voyager_section_norm, voyager_query_norm.T).squeeze(1)\n",
    "        voyager_chapter_similarities = torch.matmul(voyager_chapter_norm, voyager_query_norm.T).squeeze(1)\n",
    "        voyager_page_similarities = torch.matmul(voyager_page_norm, voyager_query_norm.T).squeeze(1)\n",
    "\n",
    "        # Apply softmax\n",
    "        gemini_section_weights = F.softmax(gemini_section_similarities, dim=0)\n",
    "        gemini_chapter_weights = F.softmax(gemini_chapter_similarities, dim=0)\n",
    "        gemini_page_weights = F.softmax(gemini_page_similarities, dim=0)\n",
    "\n",
    "        voyager_section_weights = F.softmax(voyager_section_similarities, dim=0)\n",
    "        voyager_chapter_weights = F.softmax(voyager_chapter_similarities, dim=0)\n",
    "        voyager_page_weights = F.softmax(voyager_page_similarities, dim=0)\n",
    "\n",
    "        # Get top N indices\n",
    "        gemini_section_top_values, gemini_section_top_indices = torch.topk(gemini_section_weights, self.top_n)\n",
    "        gemini_chapter_top_values, gemini_chapter_top_indices = torch.topk(gemini_chapter_weights, self.top_n)\n",
    "        gemini_page_top_values, gemini_page_top_indices = torch.topk(gemini_page_weights, self.top_n)\n",
    "\n",
    "        voyager_section_top_values, voyager_section_top_indices = torch.topk(voyager_section_weights, self.top_n)\n",
    "        voyager_chapter_top_values, voyager_chapter_top_indices = torch.topk(voyager_chapter_weights, self.top_n)\n",
    "        voyager_page_top_values, voyager_page_top_indices = torch.topk(voyager_page_weights, self.top_n)\n",
    "\n",
    "        # Store results in dictionary\n",
    "        top_indices_dict = {\n",
    "            'sections': {\n",
    "                \"gemini_top_values\": gemini_section_top_values,\n",
    "                \"gemini_top_indices\": gemini_section_top_indices,\n",
    "                \"voyager_top_values\": voyager_section_top_values,\n",
    "                \"voyager_top_indices\": voyager_section_top_indices\n",
    "            },\n",
    "            'chapters': {\n",
    "                \"gemini_top_values\": gemini_chapter_top_values,\n",
    "                \"gemini_top_indices\": gemini_chapter_top_indices,\n",
    "                \"voyager_top_values\": voyager_chapter_top_values,\n",
    "                \"voyager_top_indices\": voyager_chapter_top_indices\n",
    "            },\n",
    "            'pages': {\n",
    "                \"gemini_top_values\": gemini_page_top_values,\n",
    "                \"gemini_top_indices\": gemini_page_top_indices,\n",
    "                \"voyager_top_values\": voyager_page_top_values,\n",
    "                \"voyager_top_indices\": voyager_page_top_indices\n",
    "            }\n",
    "        }\n",
    "        print(\"FINAL\")\n",
    "        return {\n",
    "            'top_indices': top_indices_dict\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New_Embeddings_2025\n",
      "New_Embeddings_2025\\chapters\\embeddings_gemini_text-005_chapters_semchunk.parquet\n",
      "\n",
      "Columns in embeddings_gemini_text-005_chapters_semchunk.parquet: ['chunk', 'Embedding']\n",
      "Loaded embeddings_gemini_text-005_chapters_semchunk.parquet (gemini - chapters)\n",
      "New_Embeddings_2025\n",
      "New_Embeddings_2025\\chapters\\embeddings_voyage_per_chapter_semchunked.parquet\n",
      "\n",
      "Columns in embeddings_voyage_per_chapter_semchunked.parquet: ['chunk', 'Embedding']\n",
      "Loaded embeddings_voyage_per_chapter_semchunked.parquet (voyager - chapters)\n",
      "New_Embeddings_2025\n",
      "New_Embeddings_2025\\pages\\embeddings_gemini_text-005_pages_semchunk.parquet\n",
      "\n",
      "Columns in embeddings_gemini_text-005_pages_semchunk.parquet: ['chunk', 'Embedding']\n",
      "Loaded embeddings_gemini_text-005_pages_semchunk.parquet (gemini - pages)\n",
      "New_Embeddings_2025\n",
      "New_Embeddings_2025\\pages\\embeddings_voyage_per_pages_semchunked.parquet\n",
      "\n",
      "Columns in embeddings_voyage_per_pages_semchunked.parquet: ['chunk', 'Embedding']\n",
      "Loaded embeddings_voyage_per_pages_semchunked.parquet (voyager - pages)\n",
      "New_Embeddings_2025\n",
      "New_Embeddings_2025\\sections\\embeddings_gemini_text-005.parquet\n",
      "\n",
      "Columns in embeddings_gemini_text-005.parquet: ['Section', 'Url', 'Content', 'Metadata', 'Processed_Content', 'Processed_Section', 'Processed_Metadata', 'Embedding']\n",
      "Loaded embeddings_gemini_text-005.parquet (gemini - sections)\n",
      "New_Embeddings_2025\n",
      "New_Embeddings_2025\\sections\\embeddings_voyage.parquet\n",
      "\n",
      "Columns in embeddings_voyage.parquet: ['Section', 'Url', 'Content', 'Metadata', 'Processed_Content', 'Processed_Section', 'Processed_Metadata', 'Embedding']\n",
      "Loaded embeddings_voyage.parquet (voyager - sections)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'LegalQueryAnalyzer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m base_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew_Embeddings_2025\u001b[39m\u001b[38;5;124m\"\u001b[39m \n\u001b[0;32m     45\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mWhoever, being an officer, employee or agent of the United States or of any department or agency thereof, having received public money which he is not authorized to retain as salary, pay, or emolument, fails to render his accounts for the same as provided by law is guilty of embezzlement, and shall be fined under this title or in a sum equal to the amount of the money embezzled, whichever is greater, or imprisoned not more than ten years, or both; but if the amount embezzled does not exceed $1,000, he shall be fined under this title or imprisoned not more than one year, or both.\u001b[39m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m---> 47\u001b[0m \u001b[43mtest_multi_level_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 28\u001b[0m, in \u001b[0;36mtest_multi_level_attention\u001b[1;34m(base_path, query)\u001b[0m\n\u001b[0;32m     25\u001b[0m model \u001b[38;5;241m=\u001b[39m MultiLevelAttention(output_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Analyze query\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m query_analyzer \u001b[38;5;241m=\u001b[39m \u001b[43mLegalQueryAnalyzer\u001b[49m()\n\u001b[0;32m     29\u001b[0m query_output \u001b[38;5;241m=\u001b[39m query_analyzer\u001b[38;5;241m.\u001b[39manalyze_query(query)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Run the model\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LegalQueryAnalyzer' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "\n",
    "def test_multi_level_attention(base_path, query):\n",
    "    \"\"\"\n",
    "    Test function for MultiLevelAttention model using real embeddings from saved files.\n",
    "    \n",
    "    Args:\n",
    "        base_path (str): Path to the directory containing embedding parquet files.\n",
    "        query_embedding (torch.Tensor): Query embedding tensor with shape [768].\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    global output\n",
    "    # Load embeddings\n",
    "    loader = LegalEmbeddingLoader(base_path)\n",
    "    gemini_embeddings, voyager_embeddings, metadata = loader.load_embeddings()\n",
    "\n",
    "    # Initialize the model\n",
    "    model = MultiLevelAttention(output_dim=1024)\n",
    "\n",
    "    # Analyze query\n",
    "    query_analyzer = LegalQueryAnalyzer()\n",
    "    query_output = query_analyzer.analyze_query(query)\n",
    "\n",
    "    # Run the model\n",
    "    output = model.forward(\n",
    "        gemini_embeddings, \n",
    "        voyager_embeddings, \n",
    "        torch.tensor(query_output['gemini_embedding']),\n",
    "        torch.tensor(query_output['voyage_embedding'])\n",
    "    )\n",
    "    \n",
    "    for granularity, data in output['top_indices'].items():\n",
    "        print(f\"\\n--- {granularity.capitalize()} ---\")\n",
    "        print(f\"Gemini Top Indices: {data['gemini_top_indices']}\")\n",
    "        print(f\"Voyager Top Indices: {data['voyager_top_indices']}\")\n",
    "\n",
    "base_path = \"New_Embeddings_2025\" \n",
    "query = \"\"\"Whoever, being an officer, employee or agent of the United States or of any department or agency thereof, having received public money which he is not authorized to retain as salary, pay, or emolument, fails to render his accounts for the same as provided by law is guilty of embezzlement, and shall be fined under this title or in a sum equal to the amount of the money embezzled, whichever is greater, or imprisoned not more than ten years, or both; but if the amount embezzled does not exceed $1,000, he shall be fined under this title or imprisoned not more than one year, or both.\n",
    "\"\"\"\n",
    "test_multi_level_attention(base_path, query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_text_in_indices(indices, source, model, input_text):\n",
    "    \"\"\"\n",
    "    Check if the input_text is present in any of the retrieved texts from given indices.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for index in indices:\n",
    "        content = get_processed_content_by_index(index, source, model)\n",
    "        if content:\n",
    "            results[index] = str(input_text).lower() in str(content).lower()\n",
    "        else:\n",
    "            results[index] = None  # Indicates that no content was found for the index\n",
    "    return results\n",
    "\n",
    "\n",
    "def check_text_in_indices_automated(top_indices_dict, input_text, model):\n",
    "    \"\"\"\n",
    "    Automated function to check if the input_text is present in any of the retrieved texts\n",
    "    from given indices specified in the top_indices_dict for the specified model.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for source, indices_dict in top_indices_dict.items():\n",
    "        for key, indices in indices_dict.items():\n",
    "            # Check for the right model and source combination\n",
    "            if model == \"gemini\" and \"gemini\" in key:\n",
    "                matches = check_text_in_indices(indices, source, \"gemini\", input_text)\n",
    "                results[f\"{model}_{source}_{key}\"] = matches\n",
    "            elif model == \"voyage\" and \"voyager\" in key:\n",
    "                matches = check_text_in_indices(indices, source, \"voyage\", input_text)\n",
    "                results[f\"{model}_{source}_{key}\"] = matches\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Call the automated function for \"gemini\" model\n",
    "automated_matches = check_text_in_indices_automated(top_indices_dict, input_text, \"gemini\")\n",
    "print(automated_matches)\n",
    "\n",
    "# Call the automated function for \"voyage\" model\n",
    "automated_matches_voyage = check_text_in_indices_automated(top_indices_dict, input_text, \"voyage\")\n",
    "print(automated_matches_voyage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In all cases of murder or manslaughter, the offense shall be deemed to have been committed at the place where the injury was inflicted, or the poison administered or other means employed which caused the death, without regard to the place where the death occurs. (June 25, 1948, ch. 645, 62 Stat. 826.)\n",
      "\n",
      "Historical and Revision Notes\n",
      "Based on title 18, U.S.C., 1940 ed., §553 (Mar. 4, 1909, ch. 321, §336, 35 Stat. 1152).\n",
      "\n",
      "-----------------------------------------------\n",
      "1996—Subsec. (a). Pub. L. 104–132, §502(1)(A), substituted \"nuclear material or nuclear byproduct\n",
      "material\" for \"nuclear material\" wherever appearing.\n",
      "Subsec. (a)(1)(A). Pub. L. 104–132, §502(1)(B)(i), inserted \"or to the environment\" after \"damage to\n",
      "property\".\n",
      "Subsec. (a)(1)(B). Pub. L. 104–132, §502(1)(B)(ii), amended subpar. (B) generally. Prior to amendment,\n",
      "subpar. (B) read as follows: \"knows that circumstances exist which are likely to cause the death of or serious\n",
      "bodily injury to any person or substantial damage to property;\".\n",
      "Subsec. (a)(6). Pub. L. 104–132, §502(1)(C), inserted \"or to the environment\" after \"damage to property\".\n",
      "Subsec. (c)(2). Pub. L. 104–132, §502(2)(A), amended par. (2) generally. Prior to amendment, par. (2) read\n",
      "as follows: \"the defendant is a national of the United States, as defined in section 101 of the Immigration and\n",
      "Nationality Act (8 U.S.C. 1101);\".\n",
      "Subsec. (c)(3). Pub. L. 104–132, §502(2)(B), struck out \"at the time of the offense the nuclear material is in\n",
      "use, storage, or transport, for peaceful purposes, and\" before \"after the conduct\" and struck out \"or\" at end.\n",
      "Subsec. (c)(4). Pub. L. 104–132, §502(2)(C), substituted \"nuclear material or nuclear byproduct material\"\n",
      "for \"nuclear material for peaceful purposes\" and \"; or\" for period at end.\n",
      "Subsec. (c)(5). Pub. L. 104–132, §502(2)(D), added par. (5).\n",
      "Subsec. (f)(1)(A). Pub. L. 104–132, §502(3)(A)(i), struck out \"with an isotopic concentration not in excess\n",
      "of 80 percent plutonium 238\" after \"plutonium\".\n",
      "Subsec. (f)(1)(C). Pub. L. 104–132, §502(3)(A)(ii), substituted \"enriched uranium, defined as uranium\" for\n",
      "\"uranium\".\n",
      "[Release Point 118-78]\n",
      "\n",
      "-----------------------------------------------\n",
      "Sec. 643. Accounting generally for public money.\n",
      "\n",
      "Whoever, being an officer, employee or agent of the United States or of any department or agency thereof, having received public money which he is not authorized to retain as salary, pay, or emolument, fails to render his accounts for the same as provided by law is guilty of embezzlement, and shall be fined under this title or in a sum equal to the amount of the money embezzled, whichever is greater, or imprisoned not more than ten years, or both; but if the amount embezzled does not exceed $1,000, he shall be fined under this title or imprisoned not more than one year, or both.\n",
      "(June 25, 1948, ch. 645, 62 Stat. 726; Pub. L. 103–322, title XXXIII, §330016(1)(H), (2)(G), Sept. 13, 1994, 108 Stat. 2147, 2148; Pub. L. 104–294, title VI, §606(a), Oct. 11, 1996, 110 Stat. 3511.)\n",
      "\n",
      "\n",
      "Sec. 644. Banker receiving unauthorized deposit of public money.\n",
      "\n",
      "Whoever, not being an authorized depositary of public moneys, knowingly receives from any disbursing officer, or collector of internal revenue, or other agent of the United States, any public money on deposit, or by way of loan or accommodation, with or without interest, or otherwise than in payment of a debt against the United States, or uses, transfers, converts, appropriates, or applies any portion of the public money for any purpose not prescribed by law is guilty of embezzlement and shall be fined under this title or not more than the amount so embezzled, whichever is greater, or imprisoned not more than ten years, or both; but if the amount embezzled does not exceed $1,000, he shall be fined not more than $1,000 or imprisoned not more than one year, or both.\n",
      "(June 25, 1948, ch. 645, 62 Stat. 726; Pub. L. 103–322, title XXXIII, §330016(2)(G), Sept. 13, 1994, 108 Stat. 2148; Pub. L. 104–294, title VI, §606(a), Oct. 11, 1996, 110 Stat. 3511.)\n",
      "\n",
      "\n",
      "Sec. 645. Court officers generally.\n",
      "\n",
      "Whoever, being a United States marshal, clerk, receiver, referee, trustee, or other officer of a United States court, or any deputy, assistant, or employee of any such officer, retains or converts to his own use or to the use of another or after demand by the party entitled thereto, unlawfully retains any money coming into his hands by virtue of his official relation, position or employment, is guilty of embezzlement and shall, where the offense is not otherwise punishable by enactment of Congress, be fined under this title or not more than double the value of the money so embezzled, whichever is greater, or imprisoned not more than ten years, or both; but if the amount embezzled does not exceed $1,000, he shall be fined under this title or imprisoned not more than one year, or both.\n",
      "\n",
      "It shall not be a defense that the accused person had any interest in such moneys or fund.\n",
      "(June 25, 1948, ch. 645, 62 Stat. 726; Pub. L. 103–322, title XXXIII, §330016(1)(H), (2)(G), Sept. 13, 1994, 108 Stat. 2147, 2148; Pub. L. 104–294, title VI, §606(a), Oct. 11, 1996, 110 Stat. 3511.)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the embeddings files\n",
    "gemini_sections_df = pd.read_parquet(r\"New_Embeddings_2025\\sections\\embeddings_gemini_text-005.parquet\")\n",
    "gemini_pages_df = pd.read_parquet(r\"New_Embeddings_2025\\pages\\embeddings_gemini_text-005_pages_semchunk.parquet\")\n",
    "gemini_chapters_df = pd.read_parquet(r\"New_Embeddings_2025\\chapters\\embeddings_gemini_text-005_chapters_semchunk.parquet\")\n",
    "\n",
    "voyage_sections_df = pd.read_parquet(r\"New_Embeddings_2025\\sections\\embeddings_voyage.parquet\")\n",
    "voyage_pages_df = pd.read_parquet(r\"New_Embeddings_2025\\pages\\embeddings_voyage_per_pages_semchunked.parquet\")\n",
    "voyage_chapters_df = pd.read_parquet(r\"New_Embeddings_2025\\chapters\\embeddings_voyage_per_chapter_semchunked.parquet\")\n",
    "\n",
    "def get_processed_content_by_index(index, source, model):\n",
    "    \"\"\"\n",
    "    Retrieve the Processed_Content or chunk based on the specified index and source.\n",
    "\n",
    "    Args:\n",
    "        index (int): The row index to retrieve.\n",
    "        source (str): The source dataset (\"gemini_text\", \"gemini_pages\", or \"voyage\").\n",
    "\n",
    "    Returns:\n",
    "        str: The corresponding Processed_Content or chunk.\n",
    "    \"\"\"\n",
    "    if model== \"gemini\":\n",
    "        if source == \"sections\":\n",
    "            return gemini_sections_df.loc[index, \"Processed_Content\"] if index in gemini_sections_df.index else None\n",
    "        elif source == \"pages\":\n",
    "            return gemini_pages_df.loc[index, \"chunk\"] if index in gemini_pages_df.index else None\n",
    "        elif source == \"chapters\":\n",
    "            return gemini_chapters_df.loc[index, \"chunk\"] if index in gemini_chapters_df.index else None\n",
    "        else:\n",
    "            return \"Invalid source specified.\"\n",
    "    if model == \"voyage\":\n",
    "        if source == \"sections\":\n",
    "            return voyage_sections_df.loc[index, \"Processed_Content\"] if index in voyage_sections_df.index else None\n",
    "        elif source == \"pages\":\n",
    "            return voyage_pages_df.loc[index, \"chunk\"] if index in voyage_pages_df.index else None\n",
    "        elif source == \"chapters\":\n",
    "            return voyage_chapters_df.loc[index, \"chunk\"] if index in voyage_chapters_df.index else None\n",
    "        else:\n",
    "            return \"Invalid source specified.\"\n",
    "\n",
    "# Example Usage:\n",
    "index_to_fetch = 456  # Change this index based on your needs\n",
    "print(get_processed_content_by_index(index_to_fetch, source=\"sections\", model= \"gemini\"))  # For Gemini Text\n",
    "print(\"-----------------------------------------------\")\n",
    "print(get_processed_content_by_index(index_to_fetch, source=\"pages\", model= \"gemini\"))  # For Gemini Pages\n",
    "print(\"-----------------------------------------------\")\n",
    "print(get_processed_content_by_index(index_to_fetch, source=\"chapters\", model= \"gemini\"))  # For Voyage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_text_in_indices_automated(top_indices_dict, input_text, model):\n",
    "    \"\"\"\n",
    "    Automated function to check if the input_text is present in any of the retrieved texts\n",
    "    from given indices specified in the top_indices_dict for the specified model.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for source, indices_dict in top_indices_dict.items():\n",
    "        for key, indices in indices_dict.items():\n",
    "            # Check for the right model and source combination\n",
    "            if model == \"gemini\" and \"gemini\" in key:\n",
    "                matches = check_text_in_indices(indices, source, \"gemini\", input_text)\n",
    "                results[f\"{model}_{source}_{key}\"] = matches\n",
    "            elif model == \"voyage\" and \"voyager\" in key:\n",
    "                matches = check_text_in_indices(indices, source, \"voyage\", input_text)\n",
    "                results[f\"{model}_{source}_{key}\"] = matches\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "# Define the text to check\n",
    "input_text = \"\"\"Whoever, being an officer, employee or agent of the United States or of any department or agency thereof, having received public money which he is not authorized to retain as salary, pay, or emolument, fails to render his accounts for the same as provided by law is guilty of embezzlement, and shall be fined under this title or in a sum equal to the amount of the money embezzled, whichever is greater, or imprisoned not more than ten years, or both; but if the amount embezzled does not exceed $1,000, he shall be fined under this title or imprisoned not more than one year, or both.\n",
    "\"\"\"\n",
    "\n",
    "# Call the automated function for \"gemini\" model\n",
    "automated_matches = check_text_in_indices_automated(top_indices_dict, input_text, \"gemini\")\n",
    "print(automated_matches)\n",
    "\n",
    "# Call the automated function for \"voyage\" model\n",
    "automated_matches_voyage = check_text_in_indices_automated(top_indices_dict, input_text, \"voyage\")\n",
    "print(automated_matches_voyage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{456: True, 1605: None, 145: False, 1604: None, 1741: None}\n"
     ]
    }
   ],
   "source": [
    "def check_text_in_indices(indices, source, model, input_text):\n",
    "    \"\"\"\n",
    "    Check if the input_text is present in any of the retrieved texts from given indices.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for index in indices:\n",
    "        content = get_processed_content_by_index(index, source, model)\n",
    "        if content:\n",
    "            results[index] = str(input_text).lower() in str(content).lower()\n",
    "        else:\n",
    "            results[index] = None  # Indicates that no content was found for the index\n",
    "    return results\n",
    "\n",
    "indices_to_check =[ 456, 1605,  145, 1604, 1741] \n",
    "input_text=  \"\"\"Whoever, being an officer, employee or agent of the United States or of any department or agency thereof, having received public money which he is not authorized to retain as salary, pay, or emolument, fails to render his accounts for the same as provided by law is guilty of embezzlement, and shall be fined under this title or in a sum equal to the amount of the money embezzled, whichever is greater, or imprisoned not more than ten years, or both; but if the amount embezzled does not exceed $1,000, he shall be fined under this title or imprisoned not more than one year, or both.\n",
    "\"\"\"\n",
    "source = \"chapters\"  # Change to \"pages\" or \"chapters\" as needed\n",
    "model = \"gemini\"  # Change to \"voyage\" if needed\n",
    "\n",
    "matches = check_text_in_indices(indices_to_check, source, model, input_text)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New_Embeddings_2025\n",
      "New_Embeddings_2025\\chapters\\embeddings_gemini_text-005_chapters_semchunk.parquet\n",
      "\n",
      "Columns in embeddings_gemini_text-005_chapters_semchunk.parquet: ['chunk', 'Embedding']\n",
      "Loaded embeddings_gemini_text-005_chapters_semchunk.parquet (gemini - chapters)\n",
      "New_Embeddings_2025\n",
      "New_Embeddings_2025\\chapters\\embeddings_voyage_per_chapter_semchunked.parquet\n",
      "\n",
      "Columns in embeddings_voyage_per_chapter_semchunked.parquet: ['chunk', 'Embedding']\n",
      "Loaded embeddings_voyage_per_chapter_semchunked.parquet (voyager - chapters)\n",
      "New_Embeddings_2025\n",
      "New_Embeddings_2025\\pages\\embeddings_gemini_text-005_pages_semchunk.parquet\n",
      "\n",
      "Columns in embeddings_gemini_text-005_pages_semchunk.parquet: ['chunk', 'Embedding']\n",
      "Loaded embeddings_gemini_text-005_pages_semchunk.parquet (gemini - pages)\n",
      "New_Embeddings_2025\n",
      "New_Embeddings_2025\\pages\\embeddings_voyage_per_pages_semchunked.parquet\n",
      "\n",
      "Columns in embeddings_voyage_per_pages_semchunked.parquet: ['chunk', 'Embedding']\n",
      "Loaded embeddings_voyage_per_pages_semchunked.parquet (voyager - pages)\n",
      "New_Embeddings_2025\n",
      "New_Embeddings_2025\\sections\\embeddings_gemini_text-005.parquet\n",
      "\n",
      "Columns in embeddings_gemini_text-005.parquet: ['Section', 'Url', 'Content', 'Metadata', 'Processed_Content', 'Processed_Section', 'Processed_Metadata', 'Embedding']\n",
      "Loaded embeddings_gemini_text-005.parquet (gemini - sections)\n",
      "New_Embeddings_2025\n",
      "New_Embeddings_2025\\sections\\embeddings_voyage.parquet\n",
      "\n",
      "Columns in embeddings_voyage.parquet: ['Section', 'Url', 'Content', 'Metadata', 'Processed_Content', 'Processed_Section', 'Processed_Metadata', 'Embedding']\n",
      "Loaded embeddings_voyage.parquet (voyager - sections)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing granularity: sections\n",
      "Shape of Gemini Embedding: torch.Size([1647, 768])\n",
      "Shape of Voyager Embedding: torch.Size([1647, 1024])\n",
      "--------------------------------------------------------\n",
      "Shape of Gemini Projected Embedding: torch.Size([1647, 1024])\n",
      "Shape of Voyager Projected Embedding: torch.Size([1647, 1024])\n",
      "--------------------------------------------------------\n",
      "Shape of Gemini Query Embedding: torch.Size([1, 1024])\n",
      "Shape of Voyager Query Embedding: torch.Size([1, 1024])\n",
      "--------------------------------------------------------\n",
      "Shape of Gemini Similarity: torch.Size([1, 1647])\n",
      "Shape of Voyager Similarity: torch.Size([1, 1647])\n",
      "--------------------------------------------------------\n",
      "Shape of Gemini Weights After Softmax: torch.Size([1, 1647])\n",
      "Shape of Voyager Weights After Softmax: torch.Size([1, 1647])\n",
      "--------------------------------------------------------\n",
      "Shape of Gemini Weighted Embedding: torch.Size([1, 1647])\n",
      "Shape of Voyager Weighted Embedding: torch.Size([1, 1647])\n",
      "--------------------------------------------------------\n",
      "Granularity Completed: sections\n",
      "Processing granularity: chapters\n",
      "Shape of Gemini Embedding: torch.Size([809, 768])\n",
      "Shape of Voyager Embedding: torch.Size([809, 1024])\n",
      "--------------------------------------------------------\n",
      "Shape of Gemini Projected Embedding: torch.Size([809, 1024])\n",
      "Shape of Voyager Projected Embedding: torch.Size([809, 1024])\n",
      "--------------------------------------------------------\n",
      "Shape of Gemini Query Embedding: torch.Size([1, 1024])\n",
      "Shape of Voyager Query Embedding: torch.Size([1, 1024])\n",
      "--------------------------------------------------------\n",
      "Shape of Gemini Similarity: torch.Size([1, 809])\n",
      "Shape of Voyager Similarity: torch.Size([1, 809])\n",
      "--------------------------------------------------------\n",
      "Shape of Gemini Weights After Softmax: torch.Size([1, 809])\n",
      "Shape of Voyager Weights After Softmax: torch.Size([1, 809])\n",
      "--------------------------------------------------------\n",
      "Shape of Gemini Weighted Embedding: torch.Size([1, 809])\n",
      "Shape of Voyager Weighted Embedding: torch.Size([1, 809])\n",
      "--------------------------------------------------------\n",
      "Granularity Completed: chapters\n",
      "Processing granularity: pages\n",
      "Shape of Gemini Embedding: torch.Size([2176, 768])\n",
      "Shape of Voyager Embedding: torch.Size([2176, 1024])\n",
      "--------------------------------------------------------\n",
      "Shape of Gemini Projected Embedding: torch.Size([2176, 1024])\n",
      "Shape of Voyager Projected Embedding: torch.Size([2176, 1024])\n",
      "--------------------------------------------------------\n",
      "Shape of Gemini Query Embedding: torch.Size([1, 1024])\n",
      "Shape of Voyager Query Embedding: torch.Size([1, 1024])\n",
      "--------------------------------------------------------\n",
      "Shape of Gemini Similarity: torch.Size([1, 2176])\n",
      "Shape of Voyager Similarity: torch.Size([1, 2176])\n",
      "--------------------------------------------------------\n",
      "Shape of Gemini Weights After Softmax: torch.Size([1, 2176])\n",
      "Shape of Voyager Weights After Softmax: torch.Size([1, 2176])\n",
      "--------------------------------------------------------\n",
      "Shape of Gemini Weighted Embedding: torch.Size([1, 2176])\n",
      "Shape of Voyager Weighted Embedding: torch.Size([1, 2176])\n",
      "--------------------------------------------------------\n",
      "Granularity Completed: pages\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [1, 1647] at entry 0 and [1, 809] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 47\u001b[0m\n\u001b[0;32m     45\u001b[0m base_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew_Embeddings_2025\u001b[39m\u001b[38;5;124m\"\u001b[39m \n\u001b[0;32m     46\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the legal definition of mens rea?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 47\u001b[0m \u001b[43mtest_multi_level_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[50], line 32\u001b[0m, in \u001b[0;36mtest_multi_level_attention\u001b[1;34m(base_path, query)\u001b[0m\n\u001b[0;32m     29\u001b[0m weights \u001b[38;5;241m=\u001b[39m query_output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Run the model\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m fused_embedding, details \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgemini_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvoyager_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_output\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgemini_embedding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_output\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvoyage_embedding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Print output details\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFused Embedding Shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, fused_embedding\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[1;32mIn[49], line 120\u001b[0m, in \u001b[0;36mMultiLevelAttention.forward\u001b[1;34m(self, gemini_embeddings, voyager_embeddings, gemini_query_embedding, voyager_query_embedding, weights)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGranularity Completed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgranularity\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# Aggregate results\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m combined_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgranularity_results\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgran\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgran\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgranularities\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attention_weights \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgemini\u001b[39m\u001b[38;5;124m'\u001b[39m: gemini_prob_weights\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu(),\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvoyager\u001b[39m\u001b[38;5;124m'\u001b[39m: voyager_prob_weights\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu(),\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined\u001b[39m\u001b[38;5;124m'\u001b[39m: combined_embedding\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m    126\u001b[0m }\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# Final processing\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1, 1647] at entry 0 and [1, 809] at entry 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "\n",
    "def test_multi_level_attention(base_path, query):\n",
    "    \"\"\"\n",
    "    Test function for MultiLevelAttention model using real embeddings from saved files.\n",
    "    \n",
    "    Args:\n",
    "        base_path (str): Path to the directory containing embedding parquet files.\n",
    "        query_embedding (torch.Tensor): Query embedding tensor with shape [768].\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Load embeddings\n",
    "    loader = LegalEmbeddingLoader(base_path)\n",
    "    gemini_embeddings, voyager_embeddings, metadata = loader.load_embeddings()\n",
    "\n",
    "    # Initialize the model\n",
    "    model = MultiLevelAttention(output_dim=1024)\n",
    "\n",
    "    # Define weights for combining \n",
    "    query_analyzer=LegalQueryAnalyzer()\n",
    "    query_output=query_analyzer.analyze_query(\"What is the legal definition of mens rea?\")\n",
    "    weights = query_output['weights']\n",
    "\n",
    "    # Run the model\n",
    "    fused_embedding, details = model.forward(gemini_embeddings, voyager_embeddings, torch.tensor(query_output['gemini_embedding']),torch.tensor(query_output['voyage_embedding']))\n",
    "\n",
    "    # Print output details\n",
    "    print(\"\\nFused Embedding Shape:\", fused_embedding.shape)\n",
    "    print(\"\\nProcessed Embeddings per Granularity:\")\n",
    "    for granularity, embedding in details['processed_embeddings'].items():\n",
    "        print(f\"  {granularity}: {embedding.shape}\")\n",
    "\n",
    "    print(\"\\nCross-Attention Weights:\")\n",
    "    for model_name, weight_tensor in details['cross_attention_weights'].items():\n",
    "        print(f\"  {model_name}: {weight_tensor.shape}\")\n",
    "\n",
    "\n",
    "base_path = \"New_Embeddings_2025\" \n",
    "query = \"What is the legal definition of mens rea?\"\n",
    "test_multi_level_attention(base_path, query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P shape: torch.Size([3, 2])\n",
      "W_reshaped shape: torch.Size([3, 1])\n",
      "After element-wise multiplication:\n",
      "tensor([[0.1000, 0.2000],\n",
      "        [0.9000, 1.2000],\n",
      "        [3.0000, 3.6000]])\n",
      "\n",
      "Final result:\n",
      "tensor([[0.3000],\n",
      "        [2.1000],\n",
      "        [6.6000]])\n",
      "\n",
      "Transposed result:\n",
      "tensor([[0.3000, 2.1000, 6.6000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define the projection matrix P and weights W\n",
    "P = torch.tensor([\n",
    "    [1.0, 2.0],\n",
    "    [3.0, 4.0],\n",
    "    [5.0, 6.0]\n",
    "])  # Shape: [3, 2]\n",
    "\n",
    "W = torch.tensor([0.1, 0.3, 0.6])  # Shape: [3]\n",
    "\n",
    "# Reshape W to [3, 1] for broadcasting\n",
    "W_reshaped = W.reshape(-1, 1)  # Shape: [3, 1]\n",
    "\n",
    "print(\"P shape:\", P.shape)\n",
    "print(\"W_reshaped shape:\", W_reshaped.shape)\n",
    "\n",
    "# Perform element-wise multiplication\n",
    "W_T_P = W_reshaped * P\n",
    "print(\"After element-wise multiplication:\")\n",
    "print(W_T_P)\n",
    "\n",
    "# Sum along dimension 0 (rows) and keep dimensions\n",
    "result = W_T_P.sum(dim=1, keepdim=True)\n",
    "print(\"\\nFinal result:\")\n",
    "print(result)\n",
    "\n",
    "# If you need to transpose the result (as in the original code)\n",
    "result_transposed = result.T\n",
    "print(\"\\nTransposed result:\")\n",
    "print(result_transposed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
