{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libraries import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"1-15Data-cleaned.csv\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/legal-bert-base-uncased\") # nlpaueb/legal-bert-base-uncased   |  \n",
    "model = AutoModel.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame with long text in 'Content' column\n",
    "chunk_size = 512  # or the max length of your model\n",
    "final_embeddings = []\n",
    "\n",
    "for content in df['Content']:\n",
    "    # Step 1: Split the content into chunks\n",
    "    try:\n",
    "        chunks = [content[i:i + chunk_size] for i in range(0, len(content), chunk_size)]\n",
    "        \n",
    "        # Step 2: Get embeddings for each chunk\n",
    "        chunk_embeddings = []\n",
    "        for chunk in chunks:\n",
    "            inputs = tokenizer(chunk, return_tensors=\"pt\", truncation=True, padding=True, max_length=chunk_size).to(device)\n",
    "            with torch.no_grad():\n",
    "                chunk_embedding = model(**inputs).last_hidden_state.mean(dim=1)  # Mean pooling\n",
    "                chunk_embeddings.append(chunk_embedding.cpu().numpy())\n",
    "        \n",
    "        # Step 3: Combine chunk embeddings (mean pooling)\n",
    "        final_embedding = np.mean(chunk_embeddings, axis=0)  # Mean pooling\n",
    "        final_embeddings.append(final_embedding)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "# Step 4: Add final embeddings to DataFrame\n",
    "df['Final_Embedding'] = final_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(47)\n",
    "df = df.reset_index(drop=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Final_Embedding'] = final_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Final_Embedding'].to_csv('test-embedding-withpooling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Final_Embedding'].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
