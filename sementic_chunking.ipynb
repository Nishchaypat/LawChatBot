{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain openai sentence-transformers\n",
    "!pip install langchain sentence-transformers pandas nltk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/isaacus-dev/semchunk.git\n",
      "  Cloning https://github.com/isaacus-dev/semchunk.git to c:\\users\\mkolla1\\appdata\\local\\temp\\pip-req-build-cg4vavib\n",
      "  Resolved https://github.com/isaacus-dev/semchunk.git to commit 9945642bca366925faf5bd969c2cf1bab1661725\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: mpire[dill] in c:\\users\\mkolla1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from semchunk==3.0.1) (2.10.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mkolla1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from semchunk==3.0.1) (4.66.1)\n",
      "Requirement already satisfied: pygments>=2.0 in c:\\users\\mkolla1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from mpire[dill]->semchunk==3.0.1) (2.14.0)\n",
      "Requirement already satisfied: pywin32>=301 in c:\\users\\mkolla1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from mpire[dill]->semchunk==3.0.1) (305)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\mkolla1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from mpire[dill]->semchunk==3.0.1) (0.70.17)\n",
      "Requirement already satisfied: colorama in c:\\users\\mkolla1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tqdm->semchunk==3.0.1) (0.4.6)\n",
      "Requirement already satisfied: dill>=0.3.9 in c:\\users\\mkolla1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from multiprocess->mpire[dill]->semchunk==3.0.1) (0.3.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/isaacus-dev/semchunk.git 'C:\\Users\\mkolla1\\AppData\\Local\\Temp\\pip-req-build-cg4vavib'\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0\n",
      "[notice] To update, run: C:\\Users\\mkolla1\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#!pip install semchunk\n",
    "!pip install git+https://github.com/isaacus-dev/semchunk.git\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SemChunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import semchunk\n",
    "import tiktoken\n",
    "from semchunk import chunkerify\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# Load a pre-trained embedding model (no API key required)\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize Semantic Chunker\n",
    "chunker = semchunk.chunkerify('gpt-4',chunk_size =50)  # Adjust threshold as needed\n",
    "#chunker = semchunk.chunkerify(tiktoken.encoding_for_model('gpt-4'), max_token_chars = None,chunk_size =50)  # Adjust threshold as needed\n",
    "# Process and chunk the law text\n",
    "text= \"\"\" Author 1\n",
    "Name: Manish Kolla\n",
    "Major: Computer Science\n",
    "\n",
    "Abstract:\n",
    "CareerWide is an AI-powered recruitment platform designed to revolutionize the hiring process for both students and recruiters. By leveraging advanced AI, CareerWide streamlines recruitment through personalized, merit-based candidate matching and removes biases related to ethnicity, gender, and other personal characteristics. For students, the platform offers features like automated resume enhancement, personalized skill development calendars, and AI-driven insights to improve career prospects. Students can also express their individuality by selecting avatars, gaining visibility in a unique, inclusive way. For recruiters, CareerWide provides advanced filtering tools that focus on skills, GPA, and other relevant criteria, simplifying the search for the best candidates.      Author 2\n",
    "Name: Nishchay Patel\n",
    "Major: Computer Science. The platform also supports virtual career fairs, minimizing travel and promoting sustainability. With its focus on equity, efficiency, and innovation, CareerWide is transforming recruitment into a fairer, more sustainable, and inclusive process for all.\n",
    "Faculty Sponsor: \n",
    "Name: Dr. Parag Tamhankar\n",
    "Department: Computer Science\n",
    "Email: ptamhankar@gsu.edu\n",
    "\n",
    "     \"\"\"\n",
    "chunks = chunker(text, overlap= None)\n",
    "for x in chunks:\n",
    "  print(x)\n",
    "  print('-------------------')\n",
    "  print('-------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3647.8250311332504\n"
     ]
    }
   ],
   "source": [
    "file_path = \"Title18_Pages_converted.csv\"  # Change based on your file format\n",
    "df = pd.read_csv(file_path)  \n",
    "length=[]\n",
    "for x in df['Text']:\n",
    "    length.append(len(x))\n",
    "print(sum(length)/len(length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import semchunk\n",
    "from semchunk import chunkerify\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the law text from a CSV file\n",
    "file_path = \"Title18_Pages_converted.csv\"  # Change based on your file format\n",
    "df = pd.read_csv(file_path)  \n",
    "\n",
    "# Assume each page of the law text is in a column named 'content'\n",
    "documents = df[\"Text\"].tolist()\n",
    "\n",
    "# Load a pre-trained embedding model (no API key required)\n",
    "#embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize Semantic Chunker\n",
    "chunker = semchunk.chunkerify('gpt-4', max_token_chars = None,chunk_size =1000)  # Adjust threshold as needed\n",
    "\n",
    "# Process and chunk the law text\n",
    "chunks = chunker(documents)\n",
    "\n",
    "# Convert chunked text into a DataFrame\n",
    "chunked_data = []\n",
    "for x in chunks:\n",
    "    for y in x: \n",
    "        chunked_data.append([y])\n",
    "\n",
    "chunked_df = pd.DataFrame(chunked_data, columns=[\"chunk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2176, 1)\n"
     ]
    }
   ],
   "source": [
    "print(chunked_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3344\n",
      "<class 'str'>\n",
      "Pub. L. 112–154, title VI, §601(a), Aug. 6, 2012, 126 Stat. 1195, provided that:\n",
      "\"(1) \n",
      ".—The purpose of this section [amending this section and section 2413 of Title 38, Veterans'\n",
      "PURPOSE\n",
      "Benefits] is to provide necessary and proper support for the recruitment and retention of the Armed Forces and\n",
      "militia employed in the service of the United States by protecting the dignity of the service of the members of\n",
      "such Forces and militia, and by protecting the privacy of their immediate family members and other attendees\n",
      "during funeral services for such members.\n",
      "\"(2) \n",
      ".—Congress finds that this section is a necessary and proper\n",
      "CONSTITUTIONAL AUTHORITY\n",
      "exercise of its powers under the Constitution, article I, section 8, paragraphs 1, 12, 13, 14, 16, and 18, to\n",
      "provide for the common defense, raise and support armies, provide and maintain a navy, make rules for the\n",
      "government and regulation of the land and naval forces, and provide for organizing and governing such part of\n",
      "the militia as may be employed in the service of the United States.\"\n",
      "§1389. Prohibition on attacks on United States servicemen on account of service\n",
      "(a) \n",
      ".—Whoever knowingly assaults or batters a United States serviceman or an\n",
      "IN GENERAL\n",
      "immediate family member of a United States serviceman, or who knowingly destroys or injures the\n",
      "property of such serviceman or immediate family member, on account of the military service of that\n",
      "serviceman or status of that individual as a United States serviceman, or who attempts or conspires to\n",
      "do so, shall—\n",
      "(1) in the case of a simple assault, or destruction or injury to property in which the damage or\n",
      "attempted damage to such property is not more than $500, be fined under this title in an amount\n",
      "not less than $500 nor more than $10,000 and imprisoned not more than 2 years;\n",
      "(2) in the case of destruction or injury to property in which the damage or attempted damage to\n",
      "such property is more than $500, be fined under this title in an amount not less than $1000 nor\n",
      "more than $100,000 and imprisoned not more than 5 years; and\n",
      "(3) in the case of a battery, or an assault resulting in bodily injury, be fined under this title in an\n",
      "amount not less than $2500 and imprisoned not less than 6 months nor more than 10 years.\n",
      "(b) \n",
      ".—This section shall not apply to conduct by a person who is subject to the\n",
      "EXCEPTION\n",
      "Uniform Code of Military Justice.\n",
      "(c) \n",
      ".—In this section—\n",
      "DEFINITIONS\n",
      "(1) the term \"Armed Forces\" has the meaning given that term in section 1388;\n",
      "(2) the term \"immediate family member\" has the meaning given that term in section 115; and\n",
      "(3) the term \"United States serviceman\"—\n",
      "(A) means a member of the Armed Forces; and\n",
      "(B) includes a former member of the Armed Forces during the 5-year period beginning on the\n",
      "date of the discharge from the Armed Forces of that member of the Armed Forces.\n",
      "(Added Pub. L. 111–84, div. E, §4712(a), Oct. 28, 2009, 123 Stat. 2842.)\n",
      "EDITORIAL NOTES\n",
      "REFERENCES IN TEXT\n",
      "The Uniform Code of Military Justice, referred to in subsec. (b), is classified generally to chapter 47 (§801\n",
      "et seq.) of Title 10, Armed Forces.\n",
      "[CHAPTER 68—REPEALED]\n",
      "[§§1401 to 1407. Repealed. Pub. L. 91–513, title III, §1101(b)(1)(A), Oct. 27, 1970,\n",
      "84 Stat. 1292]\n",
      "Section 1401, acts July 18, 1956, ch. 629, title II, §201, 70 Stat. 572; July 12, 1960, Pub. L. 86–624, §13(a),\n",
      "[Release Point 118-78]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "num=random.randint(0, chunked_df.shape[0])\n",
    "print(len(chunked_df['chunk'][num]))\n",
    "print(type(chunked_df['chunk'][num]))\n",
    "print(chunked_df['chunk'][num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Semantic chunking completed! Saved to 'chunked_title_18semchunk.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Save the results into a new CSV file\n",
    "chunked_df.to_csv(\"chunked_title_18semchunk.csv\", index=False)\n",
    "print(\"✅ Semantic chunking completed! Saved to 'chunked_title_18semchunk.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mkolla1\\AppData\\Local\\Temp\\ipykernel_33704\\1983244083.py:13: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Semantic chunking completed! Saved to 'chunked_title_18.csv'.\n",
      "(7706, 1)\n",
      "                                               chunk\n",
      "0  6001\\nImmunity of Witnesses\\nV.\\n5001\\nCorrect...\n",
      "1  37\\n756, 3058\\n38\\nT. 22 §465\\n39\\n5, 3241\\n51...\n",
      "2  79\\n1003\\n80\\n287, 1001\\n81\\n289\\n82\\n641, 136...\n",
      "3  123\\n912\\n124\\n211\\n125\\n543\\n126\\n541\\n127\\n1...\n",
      "4  199\\n205\\n200\\n204\\n201\\n1913\\n202\\n216\\n203\\n...\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Title 18 law text from CSV\n",
    "file_path = \"Title18_Pages_converted.csv\"  # Adjust for JSON/Excel if needed\n",
    "df = pd.read_csv(file_path)  \n",
    "\n",
    "# Assume each page of the law text is in a column named 'content'\n",
    "documents = df[\"Text\"].tolist()\n",
    "\n",
    "# Use Hugging Face embeddings (local model, no API key needed)\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize RecursiveCharacterTextSplitter for semantic-aware chunking\n",
    "chunker = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "# Process and chunk the law text\n",
    "chunks = []\n",
    "for doc in documents:\n",
    "    chunks.extend(chunker.split_text(doc))\n",
    "\n",
    "# Convert chunked text into a DataFrame\n",
    "chunked_df = pd.DataFrame({\"chunk\": chunks})\n",
    "\n",
    "# Save the results into a new CSV file\n",
    "chunked_df.to_csv(\"chunked_title_18RCTS.csv\", index=False)\n",
    "\n",
    "print(\"✅ Semantic chunking completed! Saved to 'chunked_title_18.csv'.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(chunked_df.shape)\n",
    "print(chunked_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(p) Nothing in this section or sections 2248, 2259, 2264, 2327, 3663, and 3663A and arising out of\n",
      "the application of such sections, shall be construed to create a cause of action not otherwise\n",
      "authorized in favor of any person against the United States or any officer or employee of the United\n",
      "States.\n",
      "(Added Pub. L. 97–291, §5(a), Oct. 12, 1982, 96 Stat. 1255, §3580; renumbered §3664, Pub. L.\n",
      "98–473, title II, §212(a)(1), Oct. 12, 1984, 98 Stat. 1987; amended Pub. L. 101–647, title XXXV,\n",
      "§3596, Nov. 29, 1990, 104 Stat. 4931; Pub. L. 104–132, title II, §206(a), Apr. 24, 1996, 110 Stat.\n",
      "1232; Pub. L. 107–273, div. B, title IV, §4002(e)(1), Nov. 2, 2002, 116 Stat. 1810.)\n",
      "EDITORIAL NOTES\n",
      "REFERENCES IN TEXT\n",
      "The Federal Rules of Criminal Procedure, referred to in subsecs. (c) and (o)(1)(A), are set out in the\n",
      "Appendix to this title.\n",
      "AMENDMENTS\n",
      "2002—Subsec. (o)(1)(C). Pub. L. 107–273 substituted \"subsection (d)(5)\" for \"section 3664(d)(3)\".\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "print(chunked_df['chunk'][random.randint(0, chunked_df.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Export chunked text to a new CSV file\n",
    "chunked_df.to_csv(\"chunked_title_18.csv\", index=False)\n",
    "\n",
    "print(\"Chunking completed! Saved to 'chunked_title_18.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Prototype (https://hasanaboulhasan.medium.com/the-best-text-chunking-method-f5faeb243d80#:~:text=The%20main%20idea%20behind%20semantic,cosine%20similarity%20between%20these%20chunks.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index -1 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 94\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Main Section\u001b[39;00m\n\u001b[0;32m     80\u001b[0m text\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m Author 1\u001b[39m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;124mName: Manish Kolla\u001b[39m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124mMajor: Computer Science\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     92\u001b[0m \n\u001b[0;32m     93\u001b[0m \u001b[38;5;124m     \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m---> 94\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[43mchunk_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChunks:\u001b[39m\u001b[38;5;124m\"\u001b[39m, chunks)\n",
      "Cell \u001b[1;32mIn[1], line 20\u001b[0m, in \u001b[0;36mchunk_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Determine the threshold distance for identifying breakpoints based on the 80th percentile of all distances.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m breakpoint_percentile_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m80\u001b[39m\n\u001b[1;32m---> 20\u001b[0m breakpoint_distance_threshold \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpercentile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistances\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbreakpoint_percentile_threshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Find all indices where the distance exceeds the calculated threshold, indicating a potential chunk breakpoint.\u001b[39;00m\n\u001b[0;32m     22\u001b[0m indices_above_thresh \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i, distance \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(distances) \u001b[38;5;28;01mif\u001b[39;00m distance \u001b[38;5;241m>\u001b[39m breakpoint_distance_threshold]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\numpy\\lib\\function_base.py:4283\u001b[0m, in \u001b[0;36mpercentile\u001b[1;34m(a, q, axis, out, overwrite_input, method, keepdims, interpolation)\u001b[0m\n\u001b[0;32m   4281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _quantile_is_valid(q):\n\u001b[0;32m   4282\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPercentiles must be in the range [0, 100]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 4283\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_quantile_unchecked\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4284\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\numpy\\lib\\function_base.py:4555\u001b[0m, in \u001b[0;36m_quantile_unchecked\u001b[1;34m(a, q, axis, out, overwrite_input, method, keepdims)\u001b[0m\n\u001b[0;32m   4547\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_quantile_unchecked\u001b[39m(a,\n\u001b[0;32m   4548\u001b[0m                         q,\n\u001b[0;32m   4549\u001b[0m                         axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4552\u001b[0m                         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   4553\u001b[0m                         keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   4554\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Assumes that q is in [0, 1], and is an ndarray\"\"\"\u001b[39;00m\n\u001b[1;32m-> 4555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ureduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4556\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_quantile_ureduce_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4557\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4558\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4559\u001b[0m \u001b[43m                    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4560\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4561\u001b[0m \u001b[43m                    \u001b[49m\u001b[43moverwrite_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4562\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\numpy\\lib\\function_base.py:3823\u001b[0m, in \u001b[0;36m_ureduce\u001b[1;34m(a, func, keepdims, **kwargs)\u001b[0m\n\u001b[0;32m   3820\u001b[0m             index_out \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, ) \u001b[38;5;241m*\u001b[39m nd\n\u001b[0;32m   3821\u001b[0m             kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m out[(\u001b[38;5;28mEllipsis\u001b[39m, ) \u001b[38;5;241m+\u001b[39m index_out]\n\u001b[1;32m-> 3823\u001b[0m r \u001b[38;5;241m=\u001b[39m func(a, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3825\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3826\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\numpy\\lib\\function_base.py:4722\u001b[0m, in \u001b[0;36m_quantile_ureduce_func\u001b[1;34m(a, q, axis, out, overwrite_input, method)\u001b[0m\n\u001b[0;32m   4720\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4721\u001b[0m         arr \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m-> 4722\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43m_quantile\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4723\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mquantiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4724\u001b[0m \u001b[43m                   \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4725\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4726\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4727\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\numpy\\lib\\function_base.py:4831\u001b[0m, in \u001b[0;36m_quantile\u001b[1;34m(arr, quantiles, axis, method, out)\u001b[0m\n\u001b[0;32m   4824\u001b[0m arr\u001b[38;5;241m.\u001b[39mpartition(\n\u001b[0;32m   4825\u001b[0m     np\u001b[38;5;241m.\u001b[39munique(np\u001b[38;5;241m.\u001b[39mconcatenate(([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m   4826\u001b[0m                               previous_indexes\u001b[38;5;241m.\u001b[39mravel(),\n\u001b[0;32m   4827\u001b[0m                               next_indexes\u001b[38;5;241m.\u001b[39mravel(),\n\u001b[0;32m   4828\u001b[0m                               ))),\n\u001b[0;32m   4829\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   4830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m supports_nans:\n\u001b[1;32m-> 4831\u001b[0m     slices_having_nans \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39misnan(\u001b[43marr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m   4832\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4833\u001b[0m     slices_having_nans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: index -1 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import openai\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def chunk_text(text):\n",
    "    # Split the input text into individual sentences.\n",
    "    single_sentences_list = _split_sentences(text)\n",
    "    # Combine adjacent sentences to form a context window around each sentence.\n",
    "    combined_sentences = _combine_sentences(single_sentences_list)\n",
    "    \n",
    "    # Convert the combined sentences into vector representations using a neural network model.\n",
    "    embeddings = convert_to_vector(combined_sentences)\n",
    "    \n",
    "    # Calculate the cosine distances between consecutive combined sentence embeddings to measure similarity.\n",
    "    distances = _calculate_cosine_distances(embeddings)\n",
    "    \n",
    "    # Determine the threshold distance for identifying breakpoints based on the 80th percentile of all distances.\n",
    "    breakpoint_percentile_threshold = 80\n",
    "    breakpoint_distance_threshold = np.percentile(distances, breakpoint_percentile_threshold)\n",
    "    # Find all indices where the distance exceeds the calculated threshold, indicating a potential chunk breakpoint.\n",
    "    indices_above_thresh = [i for i, distance in enumerate(distances) if distance > breakpoint_distance_threshold]\n",
    "    # Initialize the list of chunks and a variable to track the start of the next chunk.\n",
    "    chunks = []\n",
    "    start_index = 0\n",
    "    # Loop through the identified breakpoints and create chunks accordingly.\n",
    "    for index in indices_above_thresh:\n",
    "        chunk = ' '.join(single_sentences_list[start_index:index+1])\n",
    "        chunks.append(chunk)\n",
    "        start_index = index + 1\n",
    "    \n",
    "    # If there are any sentences left after the last breakpoint, add them as the final chunk.\n",
    "    if start_index < len(single_sentences_list):\n",
    "        chunk = ' '.join(single_sentences_list[start_index:])\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    # Return the list of text chunks.\n",
    "    return chunks\n",
    "\n",
    "def _split_sentences(text):\n",
    "    # Use regular expressions to split the text into sentences based on punctuation followed by whitespace.\n",
    "    sentences = re.split(r'(?<=[.?!])\\s+', text)\n",
    "    return sentences\n",
    "\n",
    "def _combine_sentences(sentences):\n",
    "    # Create a buffer by combining each sentence with its previous and next sentence to provide a wider context.\n",
    "    combined_sentences = []\n",
    "    for i in range(len(sentences)):\n",
    "        combined_sentence = sentences[i]\n",
    "        if i > 0:\n",
    "            combined_sentence = sentences[i-1] + ' ' + combined_sentence\n",
    "        if i < len(sentences) - 1:\n",
    "            combined_sentence += ' ' + sentences[i+1]\n",
    "        combined_sentences.append(combined_sentence)\n",
    "    return combined_sentences\n",
    "def convert_to_vector(texts):\n",
    "\n",
    "    # Try to generate embeddings for a list of texts using a pre-trained model and handle any exceptions.\n",
    "    try:\n",
    "        response = openai.embeddings.create(\n",
    "            input=texts,\n",
    "            model=\"text-embedding-3-small\"\n",
    "        )\n",
    "        embeddings = np.array([item.embedding for item in response.data])\n",
    "        return embeddings\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "        return np.array([])  # Return an empty array in case of an error\n",
    "\n",
    "def _calculate_cosine_distances(embeddings):\n",
    "    # Calculate the cosine distance (1 - cosine similarity) between consecutive embeddings.\n",
    "    distances = []\n",
    "    for i in range(len(embeddings) - 1):\n",
    "        similarity = cosine_similarity([embeddings[i]], [embeddings[i + 1]])[0][0]\n",
    "        distance = 1 - similarity\n",
    "        distances.append(distance)\n",
    "    return distances\n",
    "\n",
    "# Main Section\n",
    "text= \"\"\" Author 1\n",
    "Name: Manish Kolla\n",
    "Major: Computer Science\n",
    "\n",
    "Abstract:\n",
    "CareerWide is an AI-powered recruitment platform designed to revolutionize the hiring process for both students and recruiters. By leveraging advanced AI, CareerWide streamlines recruitment through personalized, merit-based candidate matching and removes biases related to ethnicity, gender, and other personal characteristics. For students, the platform offers features like automated resume enhancement, personalized skill development calendars, and AI-driven insights to improve career prospects. Students can also express their individuality by selecting avatars, gaining visibility in a unique, inclusive way. For recruiters, CareerWide provides advanced filtering tools that focus on skills, GPA, and other relevant criteria, simplifying the search for the best candidates.      Author 2\n",
    "Name: Nishchay Patel\n",
    "Major: Computer Science. The platform also supports virtual career fairs, minimizing travel and promoting sustainability. With its focus on equity, efficiency, and innovation, CareerWide is transforming recruitment into a fairer, more sustainable, and inclusive process for all.\n",
    "Faculty Sponsor: \n",
    "Name: Dr. Parag Tamhankar\n",
    "Department: Computer Science\n",
    "Email: ptamhankar@gsu.edu\n",
    "\n",
    "     \"\"\"\n",
    "chunks = chunk_text(text)\n",
    "print(\"Chunks:\", chunks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
