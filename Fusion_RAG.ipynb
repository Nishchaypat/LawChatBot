{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "import vertexai\n",
    "import langchain\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import tiktoken\n",
    "import voyageai\n",
    "from tqdm import tqdm\n",
    "import pyarrow.parquet as pq\n",
    "import spacy\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "\n",
    "\n",
    "try:\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = r\"C:\\Users\\mkolla1\\LawChatBot\\gcpservicekey.json\"\n",
    "except:\n",
    "    print(\"Error at Try block\")\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = r\"gcpservicekey.json\"\n",
    "    \n",
    "PROJECT_ID = \"lawrag\"\n",
    "LOCATION = \"us-central1\"\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "\n",
    "voyageai.api_key = os.getenv(\"VOYAGE_API\")\n",
    "\n",
    "# Load NLP pipeline for query analysis\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Legal dictionary terms - expand as needed\n",
    "LEGAL_TERMS = {\n",
    "    \"habeas corpus\", \"mens rea\", \"actus reus\", \"stare decisis\", \n",
    "    \"prima facie\", \"de novo\", \"res judicata\", \"certiorari\",\n",
    "    \"statutory\", \"U.S.C.\", \"CFR\", \"jurisdiction\", \"adjudicate\"\n",
    "}\n",
    "\n",
    "# Regex patterns for legal citations\n",
    "CITATION_PATTERNS = [\n",
    "    r'\\d+\\s+U\\.S\\.C\\.\\s+§*\\s*\\d+',  # US Code\n",
    "    r'\\d+\\s+C\\.F\\.R\\.\\s+§*\\s*\\d+',   # Code of Federal Regulations\n",
    "    r'[A-Za-z]+\\s+v\\.\\s+[A-Za-z]+',  # Case names\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Generator for Gemini and Voyage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingGenerator:\n",
    "    def __init__(self, gemini_model_name=\"text-embedding-005\", voyage_model_name=\"voyage-law-2\"):\n",
    "        \"\"\"\n",
    "        Initializes the embedding generator with Gemini and VoyageAI models.\n",
    "        \"\"\"\n",
    "        self.gemini_model = VertexAIEmbeddings(gemini_model_name)\n",
    "        self.voyage_model_name = voyage_model_name\n",
    "        self.voyage_client = voyageai.Client()\n",
    "        self.voyage_tockenizer=AutoTokenizer.from_pretrained('voyageai/voyage-2')\n",
    "        self.gemini_tockenizer=tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    def chunk_text_gemini(self, text, max_tokens=4096, overlap=512):\n",
    "        tokens = self.gemini_tockenizer.encode(text)\n",
    "\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        while start < len(tokens):\n",
    "            chunk = tokens[start:start + max_tokens]\n",
    "            chunks.append(self.gemini_tockenizer.decode(chunk))\n",
    "            start += max_tokens - overlap  # Sliding window\n",
    "        return chunks\n",
    "    \n",
    "    def chunk_text_voyage(self, text, max_tokens=4096, overlap=512):\n",
    "        \"\"\"\n",
    "        Splits text into chunks based on the token limit of voyage-law-2 tokenizer.\n",
    "        Uses a sliding window approach with overlap.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The input text to be chunked.\n",
    "            max_tokens (int): Maximum tokens per chunk (4096 for voyage-law-2).\n",
    "            overlap (int): Overlapping tokens to maintain context between chunks.\n",
    "\n",
    "        Returns:\n",
    "            list of str: List of text chunks.\n",
    "        \"\"\"\n",
    "        tokens = self.voyage_tockenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        while start < len(tokens):\n",
    "            chunk = tokens[start:start + max_tokens]\n",
    "            chunks.append(self.voyage_tockenizer.decode(chunk))\n",
    "            start += max_tokens - overlap\n",
    "\n",
    "        return chunks\n",
    "    \n",
    "    def get_embeddings_gemini(self, texts, batch_size=32):\n",
    "        \"\"\"\n",
    "        Compute embeddings using VertexAIEmbeddings in batches.\n",
    "\n",
    "        Args:\n",
    "            texts (list of str): List of text data to embed.\n",
    "            batch_size (int): Number of texts to process per batch.\n",
    "\n",
    "        Returns:\n",
    "            list: List of embedding vectors.\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "        texts= self.chunk_text_gemini(texts)\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating Embeddings\"):\n",
    "            batch = texts[i:i + batch_size]  # Get batch of texts\n",
    "            batch_embeddings = self.gemini_model.embed_documents(batch)  # Generate embeddings\n",
    "            embeddings.extend(batch_embeddings)  # Store results\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def get_embeddings_voyage(self, texts, batch_size=32):\n",
    "        \"\"\"\n",
    "        Compute embeddings using the VoyageAI Python client in batches.\n",
    "\n",
    "        Args:\n",
    "            texts (list of str): List of text data to embed.\n",
    "            batch_size (int): Number of texts per batch.\n",
    "\n",
    "        Returns:\n",
    "            list: List of embedding vectors.\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "        texts= self.chunk_text_voyage(texts)\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i + batch_size] \n",
    "            \n",
    "            try:\n",
    "                response = self.voyage_client.embed(batch, model=self.voyage_model_name)\n",
    "                batch_embeddings = response.embeddings  \n",
    "                embeddings.extend(batch_embeddings)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch {i // batch_size + 1}: {e}\")\n",
    "\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saved Embeddings Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegalEmbeddingLoader:\n",
    "    \"\"\"Loads embeddings from parquet files for both models and all granularity levels.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_path):\n",
    "        self.base_path = base_path\n",
    "        self.gemini_embeddings = {}\n",
    "        self.voyager_embeddings = {}\n",
    "        self.metadata = {}\n",
    "        \n",
    "    def load_embeddings(self):\n",
    "        \"\"\"Load the six specified embedding files.\"\"\"\n",
    "        file_mappings = {\n",
    "            \"gemini_chapters\": \"embeddings_gemini_text-005_chapters_semchunk.parquet\",\n",
    "            \"voyager_chapters\": \"embeddings_voyage_per_chapter_semchunked.parquet\",\n",
    "            \"gemini_pages\": \"embeddings_gemini_text-005_pages_semchunk.parquet\",\n",
    "            \"voyager_pages\": \"embeddings_voyage_per_pages_semchunked.parquet\",\n",
    "            \"gemini_sections\": \"embeddings_gemini_text-005.parquet\",\n",
    "            \"voyager_sections\": \"embeddings_voyage.parquet\",\n",
    "        }\n",
    "\n",
    "        for key, file_name in file_mappings.items():\n",
    "            print(self.base_path)\n",
    "            file_path = os.path.join(self.base_path, key.split(\"_\")[-1], file_name)\n",
    "            print(file_path)\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"File {file_name} not found. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            # Read parquet file\n",
    "            table = pq.read_table(file_path)\n",
    "            df = table.to_pandas()\n",
    "            print(f\"\\nColumns in {file_name}: {df.columns.tolist()}\")\n",
    "            # Extract embeddings and metadata\n",
    "            embeddings = np.stack(df[\"Embedding\"].values)\n",
    "\n",
    "            # Determine model and granularity\n",
    "            model, granularity = key.split(\"_\")\n",
    "\n",
    "            # Store embeddings\n",
    "            if model == \"gemini\":\n",
    "                self.gemini_embeddings[granularity] = torch.tensor(embeddings, dtype=torch.float32)\n",
    "            else:\n",
    "                self.voyager_embeddings[granularity] = torch.tensor(embeddings, dtype=torch.float32)\n",
    "\n",
    "            # Store metadata\n",
    "            self.metadata[key] = df.drop('Embedding', axis=1)\n",
    "\n",
    "            print(f\"Loaded {file_name} ({model} - {granularity})\")\n",
    "        return self.gemini_embeddings, self.voyager_embeddings, self.metadata\n",
    "\n",
    "    def get_embedding_dimensions(self):\n",
    "        \"\"\"Return the dimensions of embeddings for both models.\"\"\"\n",
    "        gemini_dim = {k: v.shape[1] for k, v in self.gemini_embeddings.items()}\n",
    "        voyager_dim = {k: v.shape[1] for k, v in self.voyager_embeddings.items()}\n",
    "        return gemini_dim, voyager_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Analyser and Intent Recognization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegalQueryAnalyzer:\n",
    "    \"\"\"Analyzes legal queries to determine intent and model weights.\"\"\"\n",
    "    \n",
    "    def __init__(self, legal_terms=LEGAL_TERMS, citation_patterns=CITATION_PATTERNS):\n",
    "        self.legal_terms = legal_terms\n",
    "        self.citation_patterns = citation_patterns\n",
    "        self.embedder = EmbeddingGenerator()\n",
    "        \n",
    "    def analyze_query(self, query):\n",
    "        \"\"\"\n",
    "        Analyze query characteristics to determine model weights.\n",
    "        Returns a dictionary of features and recommended weights.\n",
    "        \"\"\"\n",
    "        # Process with spaCy\n",
    "        print(f\"QUERY IS: {query}\")\n",
    "        doc = nlp(query)\n",
    "        print(type(doc))\n",
    "        print(f\"DOC AFTER PROCESSING IS QUERY IS: {doc}\")\n",
    "        # Feature extraction\n",
    "        features = {\n",
    "            'legal_term_density': self._calculate_legal_term_density(query),\n",
    "            'citation_count': self._count_citations(query),\n",
    "            'structural_complexity': self._assess_complexity(doc),\n",
    "            'query_length': len(doc),\n",
    "            'jurisdiction_signals': self._detect_jurisdiction(doc)\n",
    "        }\n",
    "        \n",
    "        # Calculate recommended weights\n",
    "        weights = self._determine_weights(features)\n",
    "        \n",
    "        return {\n",
    "            'features': features,\n",
    "            'weights': weights,\n",
    "            'gemini_embedding': self.embedder.get_embeddings_gemini([query]),\n",
    "            'voyage_embedding': self.embedder.get_embeddings_voyage([query])\n",
    "        }\n",
    "    \n",
    "    def _calculate_legal_term_density(self, query):\n",
    "        \"\"\"Calculate the density of legal terminology in the query.\"\"\"\n",
    "        # Normalize and tokenize query\n",
    "        query_lower = query.lower()\n",
    "        total_tokens = len(query_lower.split())\n",
    "        \n",
    "        # Count legal terms\n",
    "        legal_term_count = sum(1 for term in self.legal_terms if term.lower() in query_lower)\n",
    "        \n",
    "        # Calculate density\n",
    "        if total_tokens > 0:\n",
    "            return (legal_term_count / total_tokens) * 100\n",
    "        return 0\n",
    "    \n",
    "    def _count_citations(self, query):\n",
    "        \"\"\"Count legal citations in the query.\"\"\"\n",
    "        citation_count = 0\n",
    "        for pattern in self.citation_patterns:\n",
    "            citation_count += len(re.findall(pattern, query))\n",
    "        return citation_count\n",
    "    \n",
    "    def _assess_complexity(self, doc):\n",
    "        \"\"\"\n",
    "        Assess the structural complexity of the query.\n",
    "        Returns a score from 0-1 based on:\n",
    "        - Number of clauses\n",
    "        - Presence of legal conditionals\n",
    "        - Sentence structure complexity\n",
    "        \"\"\"\n",
    "        # Count clauses\n",
    "        clause_markers = [\"if\", \"when\", \"whether\", \"notwithstanding\", \"provided that\"]\n",
    "        clause_count = sum(1 for token in doc if token.text.lower() in clause_markers)\n",
    "        \n",
    "        # Check for complex legal conditionals\n",
    "        has_conditionals = any(cm in doc.text.lower() for cm in clause_markers)\n",
    "        \n",
    "        # Assess syntactic complexity (simplified)\n",
    "        depth = max((token.dep_.count('_') for token in doc), default=0)\n",
    "        \n",
    "        # Calculate complexity score (0-1)\n",
    "        complexity = min(1.0, (clause_count * 0.2) + (0.3 if has_conditionals else 0) + (depth * 0.1))\n",
    "        \n",
    "        return complexity\n",
    "    \n",
    "    def _detect_jurisdiction(self, doc):\n",
    "        \"\"\"\n",
    "        Detect jurisdictional signals in the query.\n",
    "        Returns a dictionary of jurisdictional features.\n",
    "        \"\"\"\n",
    "        # Look for jurisdictional entities\n",
    "        jurisdictions = {\n",
    "            'federal': 0,\n",
    "            'state': 0,\n",
    "            'international': 0,\n",
    "            'specific_court': None\n",
    "        }\n",
    "        \n",
    "        # Check for federal signals\n",
    "        federal_terms = [\"federal\", \"U.S.\", \"United States\", \"SCOTUS\", \"Supreme Court\"]\n",
    "        jurisdictions['federal'] = any(term.lower() in doc.text.lower() for term in federal_terms)\n",
    "        \n",
    "        # Check for state signals\n",
    "        state_names = [\n",
    "    \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\", \"Delaware\", \n",
    "    \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \n",
    "    \"Louisiana\", \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \"Mississippi\", \n",
    "    \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \"New Hampshire\", \"New Jersey\", \"New Mexico\", \n",
    "    \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \n",
    "    \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \n",
    "    \"Virginia\", \"Washington\", \"West Virginia\", \"Wisconsin\", \"Wyoming\"\n",
    "]\n",
    "  # Add all states\n",
    "        jurisdictions['state'] = any(state in doc.text for state in state_names)\n",
    "        \n",
    "        # Check for international signals\n",
    "        international_terms = [\"international\", \"foreign\", \"treaty\", \"convention\"]\n",
    "        jurisdictions['international'] = any(term.lower() in doc.text.lower() for term in international_terms)\n",
    "        \n",
    "        # Look for specific courts\n",
    "        court_patterns = [\"Circuit\", \"District Court\", \"Supreme Court\"]\n",
    "        for pattern in court_patterns:\n",
    "            if pattern in doc.text:\n",
    "                jurisdictions['specific_court'] = pattern\n",
    "                break\n",
    "                \n",
    "        return jurisdictions\n",
    "    \n",
    "    def _determine_weights(self, features):\n",
    "        \"\"\"\n",
    "        Determine the optimal weights for each model based on features.\n",
    "        Uses a rule-based approach initially, could be replaced with ML model.\n",
    "        \"\"\"\n",
    "        # Default weights slightly favor specialized model\n",
    "        gemini_weight = 0.4\n",
    "        voyager_weight = 0.6\n",
    "        \n",
    "        # Adjust for legal density and citations\n",
    "        if features['legal_term_density'] > 5 or features['citation_count'] > 0:\n",
    "            # Increase weight for legal model\n",
    "            voyager_weight += 0.15\n",
    "            gemini_weight -= 0.15\n",
    "        \n",
    "        # Adjust for complexity\n",
    "        if features['structural_complexity'] > 0.7:\n",
    "            voyager_weight += 0.1\n",
    "            gemini_weight -= 0.1\n",
    "        \n",
    "        # Adjust for jurisdictional specificity\n",
    "        if features['jurisdiction_signals']['specific_court']:\n",
    "            voyager_weight += 0.1\n",
    "            gemini_weight -= 0.1\n",
    "        \n",
    "        # Ensure weights are valid\n",
    "        voyager_weight = min(max(voyager_weight, 0.1), 0.9)\n",
    "        gemini_weight = 1.0 - voyager_weight\n",
    "        \n",
    "        return {\n",
    "            'gemini': gemini_weight,\n",
    "            'voyager': voyager_weight\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fusion Retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#         gemini_section_norm = gemini_section_proj\n",
    "#         gemini_chapter_norm = gemini_chapter_proj\n",
    "#         gemini_page_norm = gemini_page_proj\n",
    "\n",
    "#         voyager_section_norm = voyager_section_proj\n",
    "#         voyager_chapter_norm = voyager_chapter_proj\n",
    "#         voyager_page_norm = voyager_page_proj\n",
    "\n",
    "# gemini_query_norm = gemini_query_projected\n",
    "#         voyager_query_norm = voyager_query_projected\n",
    "\n",
    "\n",
    "        # Cosine similarity calculation using PyTorch\n",
    "        # gemini_section_similarities = F.cosine_similarity(gemini_section_proj, gemini_query_projected, dim=1)\n",
    "        # gemini_chapter_similarities = F.cosine_similarity(gemini_chapter_proj, gemini_query_projected, dim=1)\n",
    "        # gemini_page_similarities = F.cosine_similarity(gemini_page_proj, gemini_query_projected, dim=1)\n",
    "\n",
    "        # voyager_section_similarities = F.cosine_similarity(voyager_section_proj, voyager_query_projected, dim=1)\n",
    "        # voyager_chapter_similarities = F.cosine_similarity(voyager_chapter_proj, voyager_query_projected, dim=1)\n",
    "        # voyager_page_similarities = F.cosine_similarity(voyager_page_proj, voyager_query_projected, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionRetrival(nn.Module):\n",
    "    def __init__(self, output_dim=1024, top_n=5):\n",
    "        super(FusionRetrival, self).__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.top_n = top_n\n",
    "        self.granularities = ['sections', 'chapters', 'pages']\n",
    "\n",
    "        # Query projectors\n",
    "        self.gemini_query_projector = nn.Linear(768, 768)\n",
    "        self.voyager_query_projector = nn.Linear(1024, output_dim)\n",
    "        \n",
    "        # Document projectors\n",
    "        self.gemini_projector = nn.Linear(768, 768)\n",
    "        self.voyager_projector = nn.Linear(1024, output_dim)\n",
    "\n",
    "        # Aggregation\n",
    "        self.layer_norm = nn.LayerNorm(output_dim)\n",
    "\n",
    "\n",
    "    def forward(self, gemini_embeddings, voyager_embeddings, gemini_query_embedding, voyager_query_embedding):\n",
    "        # Compute cosine similarities directly without projections and normalizations\n",
    "        gemini_section_similarities = F.cosine_similarity(gemini_embeddings['sections'], gemini_query_embedding)\n",
    "        gemini_chapter_similarities = F.cosine_similarity(gemini_embeddings['chapters'], gemini_query_embedding)\n",
    "        gemini_page_similarities = F.cosine_similarity(gemini_embeddings['pages'], gemini_query_embedding)\n",
    "\n",
    "        voyager_section_similarities = F.cosine_similarity(voyager_embeddings['sections'], voyager_query_embedding)\n",
    "        voyager_chapter_similarities = F.cosine_similarity(voyager_embeddings['chapters'], voyager_query_embedding)\n",
    "        voyager_page_similarities = F.cosine_similarity(voyager_embeddings['pages'], voyager_query_embedding)\n",
    "\n",
    "        # Apply softmax to the similarity scores\n",
    "        gemini_section_weights = F.softmax(gemini_section_similarities, dim=0)\n",
    "        gemini_chapter_weights = F.softmax(gemini_chapter_similarities, dim=0)\n",
    "        gemini_page_weights = F.softmax(gemini_page_similarities, dim=0)\n",
    "\n",
    "        voyager_section_weights = F.softmax(voyager_section_similarities, dim=0)\n",
    "        voyager_chapter_weights = F.softmax(voyager_chapter_similarities, dim=0)\n",
    "        voyager_page_weights = F.softmax(voyager_page_similarities, dim=0)\n",
    "\n",
    "        print(f\"Shape of gemini_section_weights: {gemini_section_weights.shape}\")\n",
    "        print(f\"Shape of voyager_section_weights: {voyager_section_weights.shape}\")\n",
    "        print(f\"Shape of gemini_chapter_weights: {gemini_chapter_weights.shape}\")\n",
    "        print(f\"Shape of voyager_chapter_weights: {voyager_chapter_weights.shape}\")\n",
    "        print(f\"Shape of gemini_page_weights: {gemini_page_weights.shape}\")\n",
    "        print(f\"Shape of voyager_page_weights: {voyager_page_weights.shape}\")\n",
    "        # Get top N indices\n",
    "        gemini_section_top_values, gemini_section_top_indices = torch.topk(gemini_section_weights, self.top_n)\n",
    "        gemini_chapter_top_values, gemini_chapter_top_indices = torch.topk(gemini_chapter_weights, self.top_n)\n",
    "        gemini_page_top_values, gemini_page_top_indices = torch.topk(gemini_page_weights, self.top_n)\n",
    "\n",
    "        voyager_section_top_values, voyager_section_top_indices = torch.topk(voyager_section_weights, self.top_n)\n",
    "        voyager_chapter_top_values, voyager_chapter_top_indices = torch.topk(voyager_chapter_weights, self.top_n)\n",
    "        voyager_page_top_values, voyager_page_top_indices = torch.topk(voyager_page_weights, self.top_n)\n",
    "\n",
    "        # Store results in dictionary\n",
    "        top_indices_dict = {\n",
    "            'sections': {\n",
    "                \"gemini_top_values\": gemini_section_top_values,\n",
    "                \"gemini_top_indices\": gemini_section_top_indices,\n",
    "                \"voyager_top_values\": voyager_section_top_values,\n",
    "                \"voyager_top_indices\": voyager_section_top_indices\n",
    "            },\n",
    "            'chapters': {\n",
    "                \"gemini_top_values\": gemini_chapter_top_values,\n",
    "                \"gemini_top_indices\": gemini_chapter_top_indices,\n",
    "                \"voyager_top_values\": voyager_chapter_top_values,\n",
    "                \"voyager_top_indices\": voyager_chapter_top_indices\n",
    "            },\n",
    "            'pages': {\n",
    "                \"gemini_top_values\": gemini_page_top_values,\n",
    "                \"gemini_top_indices\": gemini_page_top_indices,\n",
    "                \"voyager_top_values\": voyager_page_top_values,\n",
    "                \"voyager_top_indices\": voyager_page_top_indices\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            'top_indices': top_indices_dict\n",
    "        }\n",
    "\n",
    "    def forward1(self, gemini_embeddings, voyager_embeddings, gemini_query_embedding, voyager_query_embedding):\n",
    "        # Project query embeddings\n",
    "        gemini_query_projected = self.gemini_query_projector(gemini_query_embedding)\n",
    "        voyager_query_projected = self.voyager_query_projector(voyager_query_embedding)\n",
    "\n",
    "        # Normalize query embeddings\n",
    "        gemini_query_norm = F.normalize(gemini_query_projected, p=2, dim=1)\n",
    "        voyager_query_norm = F.normalize(voyager_query_projected, p=2, dim=1)\n",
    "        \n",
    "        # Process embeddings per granularity\n",
    "        gemini_section_proj = self.gemini_projector(gemini_embeddings['sections'])\n",
    "        gemini_chapter_proj = self.gemini_projector(gemini_embeddings['chapters'])\n",
    "        gemini_page_proj = self.gemini_projector(gemini_embeddings['pages'])\n",
    "\n",
    "        voyager_section_proj = self.voyager_projector(voyager_embeddings['sections'])\n",
    "        voyager_chapter_proj = self.voyager_projector(voyager_embeddings['chapters'])\n",
    "        voyager_page_proj = self.voyager_projector(voyager_embeddings['pages'])\n",
    "\n",
    "        gemini_section_proj = gemini_embeddings['sections']\n",
    "        gemini_chapter_proj = gemini_embeddings['chapters']\n",
    "        gemini_page_proj = gemini_embeddings['pages']\n",
    "\n",
    "        voyager_section_proj = voyager_embeddings['sections']\n",
    "        voyager_chapter_proj = voyager_embeddings['chapters']\n",
    "        voyager_page_proj = voyager_embeddings['pages']\n",
    "\n",
    "        # Normalize document embeddings\n",
    "        gemini_section_norm = F.normalize(gemini_section_proj, p=2, dim=1)\n",
    "        gemini_chapter_norm = F.normalize(gemini_chapter_proj, p=2, dim=1)\n",
    "        gemini_page_norm = F.normalize(gemini_page_proj, p=2, dim=1)\n",
    "\n",
    "        voyager_section_norm = F.normalize(voyager_section_proj, p=2, dim=1)\n",
    "        voyager_chapter_norm = F.normalize(voyager_chapter_proj, p=2, dim=1)\n",
    "        voyager_page_norm = F.normalize(voyager_page_proj, p=2, dim=1)\n",
    "        \n",
    "\n",
    "        # Compute cosine similarity using matrix multiplication\n",
    "        gemini_section_similarities = torch.matmul(gemini_section_norm, gemini_query_norm.T).squeeze(1)\n",
    "        gemini_chapter_similarities = torch.matmul(gemini_chapter_norm, gemini_query_norm.T).squeeze(1)\n",
    "        gemini_page_similarities = torch.matmul(gemini_page_norm, gemini_query_norm.T).squeeze(1)\n",
    "\n",
    "        voyager_section_similarities = torch.matmul(voyager_section_norm, voyager_query_norm.T).squeeze(1)\n",
    "        voyager_chapter_similarities = torch.matmul(voyager_chapter_norm, voyager_query_norm.T).squeeze(1)\n",
    "        voyager_page_similarities = torch.matmul(voyager_page_norm, voyager_query_norm.T).squeeze(1)\n",
    "\n",
    "\n",
    "        # Apply softmax\n",
    "        gemini_section_weights = F.softmax(gemini_section_similarities, dim=0)\n",
    "        gemini_chapter_weights = F.softmax(gemini_chapter_similarities, dim=0)\n",
    "        gemini_page_weights = F.softmax(gemini_page_similarities, dim=0)\n",
    "\n",
    "        voyager_section_weights = F.softmax(voyager_section_similarities, dim=0)\n",
    "        voyager_chapter_weights = F.softmax(voyager_chapter_similarities, dim=0)\n",
    "        voyager_page_weights = F.softmax(voyager_page_similarities, dim=0)\n",
    "\n",
    "        print(f\"Shape of gemini_section_weights: {gemini_section_weights.shape}\")\n",
    "        print(f\"Shape of voyager_section_weights: {voyager_section_weights.shape}\")\n",
    "        print(f\"Shape of gemini_chapter_weights: {gemini_chapter_weights.shape}\")\n",
    "        print(f\"Shape of voyager_chapter_weights: {voyager_chapter_weights.shape}\")\n",
    "        print(f\"Shape of gemini_page_weights: {gemini_page_weights.shape}\")\n",
    "        print(f\"Shape of voyager_page_weights: {voyager_page_weights.shape}\")\n",
    "        # Get top N indices\n",
    "        gemini_section_top_values, gemini_section_top_indices = torch.topk(gemini_section_weights, self.top_n)\n",
    "        gemini_chapter_top_values, gemini_chapter_top_indices = torch.topk(gemini_chapter_weights, self.top_n)\n",
    "        gemini_page_top_values, gemini_page_top_indices = torch.topk(gemini_page_weights, self.top_n)\n",
    "\n",
    "        voyager_section_top_values, voyager_section_top_indices = torch.topk(voyager_section_weights, self.top_n)\n",
    "        voyager_chapter_top_values, voyager_chapter_top_indices = torch.topk(voyager_chapter_weights, self.top_n)\n",
    "        voyager_page_top_values, voyager_page_top_indices = torch.topk(voyager_page_weights, self.top_n)\n",
    "\n",
    "        # Store results in dictionary\n",
    "        top_indices_dict = {\n",
    "            'sections': {\n",
    "                \"gemini_top_values\": gemini_section_top_values,\n",
    "                \"gemini_top_indices\": gemini_section_top_indices,\n",
    "                \"voyager_top_values\": voyager_section_top_values,\n",
    "                \"voyager_top_indices\": voyager_section_top_indices\n",
    "            },\n",
    "            'chapters': {\n",
    "                \"gemini_top_values\": gemini_chapter_top_values,\n",
    "                \"gemini_top_indices\": gemini_chapter_top_indices,\n",
    "                \"voyager_top_values\": voyager_chapter_top_values,\n",
    "                \"voyager_top_indices\": voyager_chapter_top_indices\n",
    "            },\n",
    "            'pages': {\n",
    "                \"gemini_top_values\": gemini_page_top_values,\n",
    "                \"gemini_top_indices\": gemini_page_top_indices,\n",
    "                \"voyager_top_values\": voyager_page_top_values,\n",
    "                \"voyager_top_indices\": voyager_page_top_indices\n",
    "            }\n",
    "        }\n",
    "        print(\"FINAL\")\n",
    "        return {\n",
    "            'top_indices': top_indices_dict\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def remove_duplicates(top_indices_dict):\n",
    "    \"\"\"\n",
    "    Removes duplicate indices within each granularity (sections, chapters, pages)\n",
    "    while keeping the ones with the highest values.\n",
    "    \"\"\"\n",
    "    for granularity in top_indices_dict:\n",
    "        gemini_indices = [int(idx.item()) if hasattr(idx, 'item') else int(idx) for idx in top_indices_dict[granularity][\"gemini_top_indices\"]]\n",
    "        gemini_values = [float(val.item()) if hasattr(val, 'item') else float(val) for val in top_indices_dict[granularity][\"gemini_top_values\"]]\n",
    "        voyager_indices = [int(idx.item()) if hasattr(idx, 'item') else int(idx) for idx in top_indices_dict[granularity][\"voyager_top_indices\"]]\n",
    "        voyager_values = [float(val.item()) if hasattr(val, 'item') else float(val) for val in top_indices_dict[granularity][\"voyager_top_values\"]]\n",
    "        \n",
    "        index_value_map = {}\n",
    "        index_source_map = {}\n",
    "        \n",
    "        # Store highest value for each index and track source\n",
    "        for idx, val in zip(gemini_indices, gemini_values):\n",
    "            if idx not in index_value_map or val > index_value_map[idx]:\n",
    "                index_value_map[idx] = val\n",
    "                index_source_map[idx] = \"gemini\"\n",
    "        \n",
    "        for idx, val in zip(voyager_indices, voyager_values):\n",
    "            if idx not in index_value_map or val > index_value_map[idx]:\n",
    "                index_value_map[idx] = val\n",
    "                index_source_map[idx] = \"voyager\"\n",
    "        \n",
    "        # Separate indices back to gemini and voyager based on where the highest value came from\n",
    "        new_gemini_indices = []\n",
    "        new_gemini_values = []\n",
    "        new_voyager_indices = []\n",
    "        new_voyager_values = []\n",
    "        \n",
    "        for idx, val in index_value_map.items():\n",
    "            if index_source_map[idx] == \"gemini\":\n",
    "                new_gemini_indices.append(idx)\n",
    "                new_gemini_values.append(round(val,4))\n",
    "            else:\n",
    "                new_voyager_indices.append(idx)\n",
    "                new_voyager_values.append(round(val,4))\n",
    "        \n",
    "        # Update dictionary\n",
    "        top_indices_dict[granularity][\"gemini_top_indices\"] = new_gemini_indices\n",
    "        top_indices_dict[granularity][\"gemini_top_values\"] = new_gemini_values\n",
    "        top_indices_dict[granularity][\"voyager_top_indices\"] = new_voyager_indices\n",
    "        top_indices_dict[granularity][\"voyager_top_values\"] = new_voyager_values\n",
    "    \n",
    "    return top_indices_dict\n",
    "\n",
    "def get_weighted_indices(top_indices_dict, gemini_weight, voyager_weight):\n",
    "    \"\"\"\n",
    "    Selects a weighted number of indices based on provided weights.\n",
    "    \"\"\"\n",
    "    for granularity in top_indices_dict:\n",
    "        gemini_indices = top_indices_dict[granularity][\"gemini_top_indices\"]\n",
    "        gemini_values = top_indices_dict[granularity][\"gemini_top_values\"]\n",
    "        voyager_indices = top_indices_dict[granularity][\"voyager_top_indices\"]\n",
    "        voyager_values = top_indices_dict[granularity][\"voyager_top_values\"]\n",
    "        \n",
    "        num_gemini = math.ceil(len(gemini_indices) * gemini_weight)\n",
    "        num_voyager = math.ceil(len(voyager_indices) * voyager_weight)\n",
    "        \n",
    "        top_indices_dict[granularity][\"gemini_top_indices\"] = gemini_indices[:num_gemini]\n",
    "        top_indices_dict[granularity][\"gemini_top_values\"] = gemini_values[:num_gemini]\n",
    "        top_indices_dict[granularity][\"voyager_top_indices\"] = voyager_indices[:num_voyager]\n",
    "        top_indices_dict[granularity][\"voyager_top_values\"] = voyager_values[:num_voyager]\n",
    "    \n",
    "    print(\"FINAL\")\n",
    "    return {\n",
    "        'top_indices': top_indices_dict\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New_Embeddings_2025\n",
      "New_Embeddings_2025\\chapters\\embeddings_gemini_text-005_chapters_semchunk.parquet\n",
      "\n",
      "Columns in embeddings_gemini_text-005_chapters_semchunk.parquet: ['chunk', 'Embedding']\n",
      "Loaded embeddings_gemini_text-005_chapters_semchunk.parquet (gemini - chapters)\n",
      "New_Embeddings_2025\n",
      "New_Embeddings_2025\\chapters\\embeddings_voyage_per_chapter_semchunked.parquet\n",
      "\n",
      "Columns in embeddings_voyage_per_chapter_semchunked.parquet: ['chunk', 'Embedding']\n",
      "Loaded embeddings_voyage_per_chapter_semchunked.parquet (voyager - chapters)\n",
      "New_Embeddings_2025\n",
      "New_Embeddings_2025\\pages\\embeddings_gemini_text-005_pages_semchunk.parquet\n",
      "\n",
      "Columns in embeddings_gemini_text-005_pages_semchunk.parquet: ['chunk', 'Embedding']\n",
      "Loaded embeddings_gemini_text-005_pages_semchunk.parquet (gemini - pages)\n",
      "New_Embeddings_2025\n",
      "New_Embeddings_2025\\pages\\embeddings_voyage_per_pages_semchunked.parquet\n",
      "\n",
      "Columns in embeddings_voyage_per_pages_semchunked.parquet: ['chunk', 'Embedding']\n",
      "Loaded embeddings_voyage_per_pages_semchunked.parquet (voyager - pages)\n",
      "New_Embeddings_2025\n",
      "New_Embeddings_2025\\sections\\embeddings_gemini_text-005.parquet\n",
      "\n",
      "Columns in embeddings_gemini_text-005.parquet: ['Section', 'Url', 'Content', 'Metadata', 'Processed_Content', 'Processed_Section', 'Processed_Metadata', 'Embedding']\n",
      "Loaded embeddings_gemini_text-005.parquet (gemini - sections)\n",
      "New_Embeddings_2025\n",
      "New_Embeddings_2025\\sections\\embeddings_voyage.parquet\n",
      "\n",
      "Columns in embeddings_voyage.parquet: ['Section', 'Url', 'Content', 'Metadata', 'Processed_Content', 'Processed_Section', 'Processed_Metadata', 'Embedding']\n",
      "Loaded embeddings_voyage.parquet (voyager - sections)\n",
      "QUERY IS: such vessel, out of port or from the United States, shall be fined under this title or imprisoned not\n",
      "more than ten years, or both.\n",
      "In addition, such vessel, her tackle, apparel, furniture, equipment, and her cargo shall be forfeited\n",
      "to the United States.\n",
      "The Secretary of the Treasury is authorized to promulgate regulations upon compliance with\n",
      "which vessels engaged in the coastwise trade or fisheries or used solely for pleasure may be relieved\n",
      "\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "DOC AFTER PROCESSING IS QUERY IS: such vessel, out of port or from the United States, shall be fined under this title or imprisoned not\n",
      "more than ten years, or both.\n",
      "In addition, such vessel, her tackle, apparel, furniture, equipment, and her cargo shall be forfeited\n",
      "to the United States.\n",
      "The Secretary of the Treasury is authorized to promulgate regulations upon compliance with\n",
      "which vessels engaged in the coastwise trade or fisheries or used solely for pleasure may be relieved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings: 100%|██████████| 1/1 [00:00<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of gemini_section_weights: torch.Size([1647])\n",
      "Shape of voyager_section_weights: torch.Size([1647])\n",
      "Shape of gemini_chapter_weights: torch.Size([809])\n",
      "Shape of voyager_chapter_weights: torch.Size([809])\n",
      "Shape of gemini_page_weights: torch.Size([2176])\n",
      "Shape of voyager_page_weights: torch.Size([2176])\n",
      "\n",
      "--- Sections ---\n",
      "Gemini Top Indices: tensor([ 108, 1037,  720, 1033, 1035]) GEMINI TOP VALUES: tensor([0.0008, 0.0007, 0.0007, 0.0007, 0.0007])\n",
      "Voyager Top Indices: tensor([1036, 1037,  108,  720, 1034]) VOYAGER TOP VALUES: tensor([0.0009, 0.0009, 0.0008, 0.0008, 0.0008])\n",
      "\n",
      "--- Chapters ---\n",
      "Gemini Top Indices: tensor([ 54, 565, 370,  59, 566]) GEMINI TOP VALUES: tensor([0.0015, 0.0015, 0.0015, 0.0015, 0.0015])\n",
      "Voyager Top Indices: tensor([566,  54, 595, 565,  55]) VOYAGER TOP VALUES: tensor([0.0017, 0.0016, 0.0016, 0.0015, 0.0015])\n",
      "\n",
      "--- Pages ---\n",
      "Gemini Top Indices: tensor([ 637,  640, 1296,  639, 1287]) GEMINI TOP VALUES: tensor([0.0006, 0.0006, 0.0006, 0.0005, 0.0005])\n",
      "Voyager Top Indices: tensor([637, 639, 635, 640, 636]) VOYAGER TOP VALUES: tensor([0.0008, 0.0006, 0.0006, 0.0006, 0.0006])\n",
      "AFTER Processing and WEIGHTED\n",
      "FINAL\n",
      "{'top_indices': {'sections': {'gemini_top_values': [0.0007], 'gemini_top_indices': [1033], 'voyager_top_values': [0.0008, 0.0009, 0.0008], 'voyager_top_indices': [108, 1037, 720]}, 'chapters': {'gemini_top_values': [0.0015], 'gemini_top_indices': [370], 'voyager_top_values': [0.0016, 0.0015, 0.0017], 'voyager_top_indices': [54, 565, 566]}, 'pages': {'gemini_top_values': [0.0006], 'gemini_top_indices': [1296], 'voyager_top_values': [0.0008, 0.0006, 0.0006], 'voyager_top_indices': [637, 640, 639]}}}\n",
      "\n",
      "--- Sections ---\n",
      "Gemini Top Indices: [1033] GEMINI TOP VALUES: [0.0007]\n",
      "Voyager Top Indices: [108, 1037, 720] VOYAGER TOP VALUES: [0.0008, 0.0009, 0.0008]\n",
      "\n",
      "--- Chapters ---\n",
      "Gemini Top Indices: [370] GEMINI TOP VALUES: [0.0015]\n",
      "Voyager Top Indices: [54, 565, 566] VOYAGER TOP VALUES: [0.0016, 0.0015, 0.0017]\n",
      "\n",
      "--- Pages ---\n",
      "Gemini Top Indices: [1296] GEMINI TOP VALUES: [0.0006]\n",
      "Voyager Top Indices: [637, 640, 639] VOYAGER TOP VALUES: [0.0008, 0.0006, 0.0006]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "\n",
    "def test_multi_level_attention(base_path, query, forward_fn):\n",
    "    \"\"\"\n",
    "    Test function for MultiLevelAttention model using real embeddings from saved files.\n",
    "    \n",
    "    Args:\n",
    "        base_path (str): Path to the directory containing embedding parquet files.\n",
    "        query_embedding (torch.Tensor): Query embedding tensor with shape [768].\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    global output\n",
    "    # Load embeddings\n",
    "    loader = LegalEmbeddingLoader(base_path)\n",
    "    gemini_embeddings, voyager_embeddings, metadata = loader.load_embeddings()\n",
    "\n",
    "    # Initialize the model\n",
    "    model = FusionRetrival(output_dim=1024)\n",
    "\n",
    "    # Analyze query\n",
    "    query_analyzer = LegalQueryAnalyzer()\n",
    "    query_output = query_analyzer.analyze_query(query)\n",
    "\n",
    "    # Run the model\n",
    "    if forward_fn == \"simple\":\n",
    "        output = model.forward(\n",
    "            gemini_embeddings, \n",
    "            voyager_embeddings, \n",
    "            torch.tensor(query_output['gemini_embedding']),\n",
    "            torch.tensor(query_output['voyage_embedding'])\n",
    "        )\n",
    "    else:\n",
    "        output = model.forward1(\n",
    "            gemini_embeddings, \n",
    "            voyager_embeddings, \n",
    "            torch.tensor(query_output['gemini_embedding']),\n",
    "            torch.tensor(query_output['voyage_embedding'])\n",
    "        )\n",
    "    \n",
    "    for granularity, data in output['top_indices'].items():\n",
    "        print(f\"\\n--- {granularity.capitalize()} ---\")\n",
    "        print(f\"Gemini Top Indices: {data['gemini_top_indices']} GEMINI TOP VALUES: {data['gemini_top_values']}\")\n",
    "        print(f\"Voyager Top Indices: {data['voyager_top_indices']} VOYAGER TOP VALUES: {data['voyager_top_values']}\")\n",
    "    print(\"AFTER Processing and WEIGHTED\")\n",
    "\n",
    "    cleaned_top_indices = remove_duplicates(output['top_indices'])\n",
    "    final_indices = get_weighted_indices(cleaned_top_indices, gemini_weight=0.5, voyager_weight=0.5)\n",
    "    print(final_indices)\n",
    "    for granularity, data in final_indices['top_indices'].items():\n",
    "        print(f\"\\n--- {granularity.capitalize()} ---\")\n",
    "        print(f\"Gemini Top Indices: {data['gemini_top_indices']} GEMINI TOP VALUES: {data['gemini_top_values']}\")\n",
    "        print(f\"Voyager Top Indices: {data['voyager_top_indices']} VOYAGER TOP VALUES: {data['voyager_top_values']}\")\n",
    "base_path = \"New_Embeddings_2025\" \n",
    "query = \"\"\"such vessel, out of port or from the United States, shall be fined under this title or imprisoned not\n",
    "more than ten years, or both.\n",
    "In addition, such vessel, her tackle, apparel, furniture, equipment, and her cargo shall be forfeited\n",
    "to the United States.\n",
    "The Secretary of the Treasury is authorized to promulgate regulations upon compliance with\n",
    "which vessels engaged in the coastwise trade or fisheries or used solely for pleasure may be relieved\n",
    "\"\"\"\n",
    "test_multi_level_attention(base_path, query, forward_fn=\"simple\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrive Content by Index and Testing the Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Director of the Administrative Office of the United States Courts, in consultation with the Attorney General and the Secretary of Health and Human Services, shall, subject to the availability of appropriations, establish a program of drug testing of Federal offenders on post-conviction release. The program shall include such standards and guidelines as the Director may determine necessary to ensure the reliability and accuracy of the drug testing programs. In each judicial district the chief probation officer shall arrange for the drug testing of defendants on post-conviction release pursuant to a conviction for a felony or other offense described in section 3563(a)(4).\n",
      "\n",
      "(Added Pub. L. 103–322, title II, §20414(a)(1), Sept. 13, 1994, 108 Stat. 1830.)\n",
      "\n",
      "Editorial Notes\n",
      "\n",
      "References in Text\n",
      "\n",
      "Section 3563(a)(4), referred to in text, probably means the par. (4) of section 3563(a) added by section 20414(b)(3) of Pub. L. 103–322, which was renumbered par. (5) by Pub. L. 104–132, title II, §203(1)(C), Apr. 24, 1996, 110 Stat. 1227.\n",
      "\n",
      "-----------------------------------------------\n",
      "such vessel, out of port or from the United States, shall be fined under this title or imprisoned not\n",
      "more than ten years, or both.\n",
      "In addition, such vessel, her tackle, apparel, furniture, equipment, and her cargo shall be forfeited\n",
      "to the United States.\n",
      "The Secretary of the Treasury is authorized to promulgate regulations upon compliance with\n",
      "which vessels engaged in the coastwise trade or fisheries or used solely for pleasure may be relieved\n",
      "from complying with this section.\n",
      "(June 25, 1948, ch. 645, 62 Stat. 747; Pub. L. 103–182, title VI, §687, Dec. 8, 1993, 107 Stat. 2221;\n",
      "Pub. L. 103–322, title XXXIII, §330016(1)(L), Sept. 13, 1994, 108 Stat. 2147; Pub. L. 109–304,\n",
      "§17(d)(5), Oct. 6, 2006, 120 Stat. 1707.)\n",
      "HISTORICAL AND REVISION NOTES\n",
      "Based on title 18, U.S.C., 1940 ed., §§34, 36 (June 15, 1917, ch. 30, title V, §§4, 6, 40 Stat. 222; Mar. 28,\n",
      "1940, ch. 72, §5, 54 Stat. 79).\n",
      "Section consolidates said sections of title 18, U.S.C., 1940 ed.\n",
      "Words \"within the United States\" were substituted for \"within the jurisdiction\" etc., in view of the definition\n",
      "of United States in section 5 of this title.\n",
      "Mandatory punishment provision was rephrased in the alternative.\n",
      "Words in subsection (a), referring to title 46, sections 91, 92, and 94, \"each of which sections is hereby\n",
      "declared to be and is continued in full force and effect,\" were omitted as surplusage.\n",
      "The conspiracy provision of said section 36 was omitted as covered by section 371 of this title. See reviser's\n",
      "note under that section.\n",
      "The final paragraph of the revised section was added on advice of the Treasury Department, to conform\n",
      "with administrative practice and because of the unnecessary burden upon domestic commerce had the\n",
      "provisions of this section been enforced against coastwise, fishing, and pleasure vessels.\n",
      "Minor changes of phraseology were made.\n",
      "EDITORIAL NOTES\n",
      "AMENDMENTS\n",
      "2006—Subsec. (a). Pub. L. 109–304 substituted \"section 60105 of title 46\" for \"section 4197 of the Revised\n",
      "Statutes of the United States (46 U.S.C. App. 91)\".\n",
      "1994—Subsec. (b). Pub. L. 103–322 substituted \"fined under this title\" for \"fined not more than $10,000\".\n",
      "1993—Subsec. (a). Pub. L. 103–182 substituted \"section 431 of the Tariff Act of 1930 (19 U.S.C. 1431)\n",
      "and section 4197 of the Revised Statutes of the United States (46 U.S.C. App. 91),\" for \"sections 91, 92, and\n",
      "94 of Title 46\", \"deliver to the Customs Service\" for \"deliver to the collector of customs for the district\n",
      "wherein such vessel is then located\", and \"the Customs Service like\" for \"the collector like\".\n",
      "STATUTORY NOTES AND RELATED SUBSIDIARIES\n",
      "TRANSFER OF FUNCTIONS\n",
      "For transfer of functions, personnel, assets, and liabilities of the United States Customs Service of the\n",
      "Department of the Treasury, including functions of the Secretary of the Treasury relating thereto, to the\n",
      "Secretary of Homeland Security, and for treatment of related references, see sections 203(1), 551(d), 552(d),\n",
      "and 557 of Title 6, Domestic Security, and the Department of Homeland Security Reorganization Plan of\n",
      "November 25, 2002, as modified, set out as a note under section 542 of Title 6. For establishment of U.S.\n",
      "Customs and Border Protection in the Department of Homeland Security, treated as if included in Pub. L.\n",
      "107–296 as of Nov. 25, 2002, see section 211 of Title 6, as amended generally by Pub. L. 114–125, and\n",
      "section 802(b) of Pub. L. 114–125, set out as a note under section 211 of Title 6.\n",
      "EXECUTIVE DOCUMENTS\n",
      "TRANSFER OF FUNCTIONS\n",
      "All offices of collector of customs, comptroller of customs, surveyor of customs, and appraiser of\n",
      "-----------------------------------------------\n",
      "§ 1362. Communication lines, stations or systems.\n",
      "\n",
      "Whoever willfully or maliciously injures or destroys any of the works, property, or material of any radio, telegraph, telephone or cable, line, station, or system, or other means of communication, operated or controlled by the United States, or used or intended to be used for military or civil defense functions of the United States, whether constructed or in process of construction, or willfully or maliciously interferes in any way with the working or use of any such line, or system, or willfully or maliciously obstructs, hinders, or delays the transmission of any communication over any such line, or system, or attempts or conspires to do such an act, shall be fined under this title or imprisoned not more than ten years, or both.\n",
      "\n",
      "In the case of any works, property, or material, not operated or controlled by the United States, this section shall not apply to any lawful strike activity, or other lawful concerted activities for the purposes of collective bargaining or other mutual aid and protection which do not injure or destroy any line or system used or intended to be used for the military or civil defense functions of the United States.\n",
      "\n",
      "(June 25, 1948, ch. 645, 62 Stat. 764; Pub. L. 87–306, Sept. 26, 1961, 75 Stat. 669; Pub. L. 103–322, title XXXII, §320903(d)(2), title XXXIII, §330016(1)(L), Sept. 13, 1994, 108 Stat. 2125, 2147; Pub. L. 107–56, title VIII, §811(c), Oct. 26, 2001, 115 Stat. 381.)\n",
      "\n",
      "Historical and Revision Notes\n",
      "\n",
      "Based on title 18, U.S.C., 1940 ed., §116 (Mar. 4, 1909, ch. 321, §60, 35 Stat. 1099).\n",
      "\n",
      "This section was extended to include radio and radio stations. Minor changes were made in phraseology.\n",
      "\n",
      "Editorial Notes\n",
      "\n",
      "Amendments\n",
      "\n",
      "2001—Pub. L. 107–56, in first par., struck out \"or attempts willfully or maliciously to injure or destroy\" after \"Whoever willfully or maliciously injures or destroys\" and inserted \"or attempts or conspires to do such an act,\" before \"shall be fined\".\n",
      "\n",
      "1994—Pub. L. 103–322, in first par., inserted \"or attempts willfully or maliciously to injure or destroy\" after \"willfully or maliciously injures or destroys\" and substituted \"fined under this title\" for \"fined not more than $10,000\".\n",
      "\n",
      "1961—Pub. L. 87–306 extended the provisions of the section to means of communication used or intended to be used for military or civil defense functions of the United States, made the provisions inapplicable to lawful strike activities, which do not injure any line or system used for such functions, and increased the punishment by fine from $1,000 to $10,000 and by imprisonment from 3 to 10 years.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the embeddings files\n",
    "gemini_sections_df = pd.read_parquet(r\"New_Embeddings_2025\\sections\\embeddings_gemini_text-005.parquet\")\n",
    "gemini_pages_df = pd.read_parquet(r\"New_Embeddings_2025\\pages\\embeddings_gemini_text-005_pages_semchunk.parquet\")\n",
    "gemini_chapters_df = pd.read_parquet(r\"New_Embeddings_2025\\chapters\\embeddings_gemini_text-005_chapters_semchunk.parquet\")\n",
    "\n",
    "voyage_sections_df = pd.read_parquet(r\"New_Embeddings_2025\\sections\\embeddings_voyage.parquet\")\n",
    "voyage_pages_df = pd.read_parquet(r\"New_Embeddings_2025\\pages\\embeddings_voyage_per_pages_semchunked.parquet\")\n",
    "voyage_chapters_df = pd.read_parquet(r\"New_Embeddings_2025\\chapters\\embeddings_voyage_per_chapter_semchunked.parquet\")\n",
    "\n",
    "def get_processed_content_by_index(index, source, model):\n",
    "    \"\"\"\n",
    "    Retrieve the Processed_Content or chunk based on the specified index and source.\n",
    "\n",
    "    Args:\n",
    "        index (int): The row index to retrieve.\n",
    "        source (str): The source dataset (\"gemini_text\", \"gemini_pages\", or \"voyage\").\n",
    "\n",
    "    Returns:\n",
    "        str: The corresponding Processed_Content or chunk.\n",
    "    \"\"\"\n",
    "    if model== \"gemini\":\n",
    "        if source == \"sections\":\n",
    "            return gemini_sections_df.loc[index, \"Processed_Content\"] if index in gemini_sections_df.index else None\n",
    "        elif source == \"pages\":\n",
    "            return gemini_pages_df.loc[index, \"chunk\"] if index in gemini_pages_df.index else None\n",
    "        elif source == \"chapters\":\n",
    "            return gemini_chapters_df.loc[index, \"chunk\"] if index in gemini_chapters_df.index else None\n",
    "        else:\n",
    "            return \"Invalid source specified.\"\n",
    "    if model == \"voyage\":\n",
    "        if source == \"sections\":\n",
    "            return voyage_sections_df.loc[index, \"Processed_Content\"] if index in voyage_sections_df.index else None\n",
    "        elif source == \"pages\":\n",
    "            return voyage_pages_df.loc[index, \"chunk\"] if index in voyage_pages_df.index else None\n",
    "        elif source == \"chapters\":\n",
    "            return voyage_chapters_df.loc[index, \"chunk\"] if index in voyage_chapters_df.index else None\n",
    "        else:\n",
    "            return \"Invalid source specified.\"\n",
    "\n",
    "# Example Usage:\n",
    "index_to_fetch = 637  # Change this index based on your needs\n",
    "print(get_processed_content_by_index(index_to_fetch, source=\"sections\", model= \"gemini\"))  # For Gemini Text\n",
    "print(\"-----------------------------------------------\")\n",
    "print(get_processed_content_by_index(index_to_fetch, source=\"pages\", model= \"gemini\"))  # For Gemini Pages\n",
    "print(\"-----------------------------------------------\")\n",
    "print(get_processed_content_by_index(index_to_fetch, source=\"chapters\", model= \"gemini\"))  # For Voyage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New_Embeddings_2025\n",
      "New_Embeddings_2025\\chapters\\embeddings_gemini_text-005_chapters_semchunk.parquet\n",
      "\n",
      "Columns in embeddings_gemini_text-005_chapters_semchunk.parquet: ['chunk', 'Embedding']\n",
      "Loaded embeddings_gemini_text-005_chapters_semchunk.parquet (gemini - chapters)\n",
      "New_Embeddings_2025\n",
      "New_Embeddings_2025\\chapters\\embeddings_voyage_per_chapter_semchunked.parquet\n",
      "\n",
      "Columns in embeddings_voyage_per_chapter_semchunked.parquet: ['chunk', 'Embedding']\n",
      "Loaded embeddings_voyage_per_chapter_semchunked.parquet (voyager - chapters)\n",
      "New_Embeddings_2025\n",
      "New_Embeddings_2025\\pages\\embeddings_gemini_text-005_pages_semchunk.parquet\n",
      "\n",
      "Columns in embeddings_gemini_text-005_pages_semchunk.parquet: ['chunk', 'Embedding']\n",
      "Loaded embeddings_gemini_text-005_pages_semchunk.parquet (gemini - pages)\n",
      "New_Embeddings_2025\n",
      "New_Embeddings_2025\\pages\\embeddings_voyage_per_pages_semchunked.parquet\n",
      "\n",
      "Columns in embeddings_voyage_per_pages_semchunked.parquet: ['chunk', 'Embedding']\n",
      "Loaded embeddings_voyage_per_pages_semchunked.parquet (voyager - pages)\n",
      "New_Embeddings_2025\n",
      "New_Embeddings_2025\\sections\\embeddings_gemini_text-005.parquet\n",
      "\n",
      "Columns in embeddings_gemini_text-005.parquet: ['Section', 'Url', 'Content', 'Metadata', 'Processed_Content', 'Processed_Section', 'Processed_Metadata', 'Embedding']\n",
      "Loaded embeddings_gemini_text-005.parquet (gemini - sections)\n",
      "New_Embeddings_2025\n",
      "New_Embeddings_2025\\sections\\embeddings_voyage.parquet\n",
      "\n",
      "Columns in embeddings_voyage.parquet: ['Section', 'Url', 'Content', 'Metadata', 'Processed_Content', 'Processed_Section', 'Processed_Metadata', 'Embedding']\n",
      "Loaded embeddings_voyage.parquet (voyager - sections)\n",
      "QUERY IS: shall be fined under this title or imprisoned not more than twenty years, or both.\n",
      "(b) Whoever is convicted of a violation of subsection (a) involving a motor vehicle that, at the\n",
      "time the violation occurred, carried high-level radioactive waste (as that term is defined in section\n",
      "2(12) of the Nuclear Waste Policy Act of 1982 (42 U.S.C. 10101(12))) or spent nuclear fuel (as that\n",
      "term is defined in section 2(23) of the Nuclear Waste Policy Act of 1982 (42 U.S.C. 10101(23))),\n",
      "shall be fined under this title and imprisoned for any term of years not less than 30, or for life.\n",
      "(Added July 14, 1956, ch. 595, §1, 70 Stat. 540; amended Pub. L. 103–322, title XXXIII,\n",
      "§330016(1)(L), Sept. 13, 1994, 108 Stat. 2147; Pub. L. 104–88, title IV, §402(a), Dec. 29, 1995, 109\n",
      "Stat. 955; Pub. L. 109–177, title IV, §406(c)(1), Mar. 9, 2006, 120 Stat. 245.)\n",
      "EDITORIAL NOTES\n",
      "AMENDMENTS\n",
      "2006—Subsec. (a). Pub. L. 109–177 inserted \"or conspires\" before \"to do any of the aforesaid acts\" in\n",
      "fourth par.\n",
      "1995—Pub. L. 104–88 designated existing provisions as subsec. (a) and added subsec. (b).\n",
      "1994—Pub. L. 103–322 substituted \"fined under this title\" for \"fined not more than $10,000\".\n",
      "STATUTORY NOTES AND RELATED SUBSIDIARIES\n",
      "EFFECTIVE DATE OF 1995 AMENDMENT\n",
      "Amendment by Pub. L. 104–88 effective Jan. 1, 1996, see section 2 of Pub. L. 104–88, set out as an\n",
      "Effective Date note under section 1301 of Title 49, Transportation.\n",
      "§34. Penalty when death results\n",
      "Whoever is convicted of any crime prohibited by this chapter, which has resulted in the death of\n",
      "any person, shall be subject also to the death penalty or to imprisonment for life.\n",
      "(Added July 14, 1956, ch. 595, §1, 70 Stat. 540; amended Pub. L. 103–322, title VI, §60003(a)(1),\n",
      "Sept. 13, 1994, 108 Stat. 1968.)\n",
      "EDITORIAL NOTES\n",
      "AMENDMENTS\n",
      "1994—Pub. L. 103–322 substituted \"imprisonment for life.\" for \"imprisonment for life, if the jury shall in\n",
      "its discretion so direct, or, in the case of a plea of guilty, or a plea of not guilty where the defendant has\n",
      "waived a trial by jury, if the court in its discretion shall so order.\"\n",
      "§35. Imparting or conveying false information\n",
      "(a) Whoever imparts or conveys or causes to be imparted or conveyed false information, knowing\n",
      "the information to be false, concerning an attempt or alleged attempt being made or to be made, to do\n",
      "any act which would be a crime prohibited by this chapter or chapter 97 or chapter 111 of this title\n",
      "shall be subject to a civil penalty of not more than $1,000 which shall be recoverable in a civil action\n",
      "brought in the name of the United States.\n",
      "(b) Whoever willfully and maliciously, or with reckless disregard for the safety of human life,\n",
      "imparts or conveys or causes to be imparted or conveyed false information, knowing the information\n",
      "to be false, concerning an attempt or alleged attempt being made or to be made, to do any act which\n",
      "would be a crime prohibited by this chapter or chapter 97 or chapter 111 of this title—shall be fined\n",
      "under this title, or imprisoned not more than five years, or both.\n",
      "(Added July 14, 1956, ch. 595, §1, 70 Stat. 540; amended Pub. L. 87–338, Oct. 3, 1961, 75 Stat. 751;\n",
      "[Release Point 118-78]\n",
      "\n",
      "DOC AFTER PROCESSING IS QUERY IS: shall be fined under this title or imprisoned not more than twenty years, or both.\n",
      "(b) Whoever is convicted of a violation of subsection (a) involving a motor vehicle that, at the\n",
      "time the violation occurred, carried high-level radioactive waste (as that term is defined in section\n",
      "2(12) of the Nuclear Waste Policy Act of 1982 (42 U.S.C. 10101(12))) or spent nuclear fuel (as that\n",
      "term is defined in section 2(23) of the Nuclear Waste Policy Act of 1982 (42 U.S.C. 10101(23))),\n",
      "shall be fined under this title and imprisoned for any term of years not less than 30, or for life.\n",
      "(Added July 14, 1956, ch. 595, §1, 70 Stat. 540; amended Pub. L. 103–322, title XXXIII,\n",
      "§330016(1)(L), Sept. 13, 1994, 108 Stat. 2147; Pub. L. 104–88, title IV, §402(a), Dec. 29, 1995, 109\n",
      "Stat. 955; Pub. L. 109–177, title IV, §406(c)(1), Mar. 9, 2006, 120 Stat. 245.)\n",
      "EDITORIAL NOTES\n",
      "AMENDMENTS\n",
      "2006—Subsec. (a). Pub. L. 109–177 inserted \"or conspires\" before \"to do any of the aforesaid acts\" in\n",
      "fourth par.\n",
      "1995—Pub. L. 104–88 designated existing provisions as subsec. (a) and added subsec. (b).\n",
      "1994—Pub. L. 103–322 substituted \"fined under this title\" for \"fined not more than $10,000\".\n",
      "STATUTORY NOTES AND RELATED SUBSIDIARIES\n",
      "EFFECTIVE DATE OF 1995 AMENDMENT\n",
      "Amendment by Pub. L. 104–88 effective Jan. 1, 1996, see section 2 of Pub. L. 104–88, set out as an\n",
      "Effective Date note under section 1301 of Title 49, Transportation.\n",
      "§34. Penalty when death results\n",
      "Whoever is convicted of any crime prohibited by this chapter, which has resulted in the death of\n",
      "any person, shall be subject also to the death penalty or to imprisonment for life.\n",
      "(Added July 14, 1956, ch. 595, §1, 70 Stat. 540; amended Pub. L. 103–322, title VI, §60003(a)(1),\n",
      "Sept. 13, 1994, 108 Stat. 1968.)\n",
      "EDITORIAL NOTES\n",
      "AMENDMENTS\n",
      "1994—Pub. L. 103–322 substituted \"imprisonment for life.\" for \"imprisonment for life, if the jury shall in\n",
      "its discretion so direct, or, in the case of a plea of guilty, or a plea of not guilty where the defendant has\n",
      "waived a trial by jury, if the court in its discretion shall so order.\"\n",
      "§35. Imparting or conveying false information\n",
      "(a) Whoever imparts or conveys or causes to be imparted or conveyed false information, knowing\n",
      "the information to be false, concerning an attempt or alleged attempt being made or to be made, to do\n",
      "any act which would be a crime prohibited by this chapter or chapter 97 or chapter 111 of this title\n",
      "shall be subject to a civil penalty of not more than $1,000 which shall be recoverable in a civil action\n",
      "brought in the name of the United States.\n",
      "(b) Whoever willfully and maliciously, or with reckless disregard for the safety of human life,\n",
      "imparts or conveys or causes to be imparted or conveyed false information, knowing the information\n",
      "to be false, concerning an attempt or alleged attempt being made or to be made, to do any act which\n",
      "would be a crime prohibited by this chapter or chapter 97 or chapter 111 of this title—shall be fined\n",
      "under this title, or imprisoned not more than five years, or both.\n",
      "(Added July 14, 1956, ch. 595, §1, 70 Stat. 540; amended Pub. L. 87–338, Oct. 3, 1961, 75 Stat. 751;\n",
      "[Release Point 118-78]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings: 100%|██████████| 1/1 [00:00<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of gemini_section_weights: torch.Size([1647])\n",
      "Shape of voyager_section_weights: torch.Size([1647])\n",
      "Shape of gemini_chapter_weights: torch.Size([809])\n",
      "Shape of voyager_chapter_weights: torch.Size([809])\n",
      "Shape of gemini_page_weights: torch.Size([2176])\n",
      "Shape of voyager_page_weights: torch.Size([2176])\n",
      "\n",
      "--- Sections ---\n",
      "Gemini Top Indices: tensor([ 755, 1445,  101,  757, 1088]) GEMINI TOP VALUES: tensor([0.0007, 0.0007, 0.0007, 0.0007, 0.0007])\n",
      "Voyager Top Indices: tensor([ 755,  757,  927, 1445,  102]) VOYAGER TOP VALUES: tensor([0.0009, 0.0009, 0.0008, 0.0008, 0.0008])\n",
      "\n",
      "--- Chapters ---\n",
      "Gemini Top Indices: tensor([384, 769,  51, 768, 535]) GEMINI TOP VALUES: tensor([0.0015, 0.0014, 0.0014, 0.0014, 0.0014])\n",
      "Voyager Top Indices: tensor([384, 769, 536, 535,  59]) VOYAGER TOP VALUES: tensor([0.0019, 0.0017, 0.0017, 0.0017, 0.0016])\n",
      "\n",
      "--- Pages ---\n",
      "Gemini Top Indices: tensor([  63, 1304,  485, 1144, 1143]) GEMINI TOP VALUES: tensor([0.0006, 0.0005, 0.0005, 0.0005, 0.0005])\n",
      "Voyager Top Indices: tensor([  63,  485, 1301, 1143,  452]) VOYAGER TOP VALUES: tensor([0.0008, 0.0006, 0.0006, 0.0006, 0.0006])\n"
     ]
    }
   ],
   "source": [
    "index_to_fetch= 63\n",
    "test_multi_level_attention(base_path, get_processed_content_by_index(index_to_fetch, source=\"pages\", model= \"voyage\"), forward_fn=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gemini_sections_gemini_top_values': {tensor(0.0007, grad_fn=<UnbindBackward0>): None, tensor(0.0006, grad_fn=<UnbindBackward0>): None, tensor(0.0006, grad_fn=<UnbindBackward0>): None, tensor(0.0006, grad_fn=<UnbindBackward0>): None, tensor(0.0006, grad_fn=<UnbindBackward0>): None}, 'gemini_sections_gemini_top_indices': {tensor(228): None, tensor(786): None, tensor(223): None, tensor(1150): None, tensor(684): None}, 'gemini_chapters_gemini_top_values': {tensor(0.0013, grad_fn=<UnbindBackward0>): None, tensor(0.0013, grad_fn=<UnbindBackward0>): None, tensor(0.0013, grad_fn=<UnbindBackward0>): None, tensor(0.0013, grad_fn=<UnbindBackward0>): None, tensor(0.0013, grad_fn=<UnbindBackward0>): None}, 'gemini_chapters_gemini_top_indices': {tensor(246): None, tensor(502): None, tensor(405): None, tensor(242): None, tensor(565): None}, 'gemini_pages_gemini_top_values': {tensor(0.0005, grad_fn=<UnbindBackward0>): None, tensor(0.0005, grad_fn=<UnbindBackward0>): None, tensor(0.0005, grad_fn=<UnbindBackward0>): None, tensor(0.0005, grad_fn=<UnbindBackward0>): None, tensor(0.0005, grad_fn=<UnbindBackward0>): None}, 'gemini_pages_gemini_top_indices': {tensor(1442): None, tensor(1722): None, tensor(636): None, tensor(1294): None, tensor(1360): None}}\n",
      "{'voyage_sections_voyager_top_values': {tensor(0.0007, grad_fn=<UnbindBackward0>): None, tensor(0.0007, grad_fn=<UnbindBackward0>): None, tensor(0.0006, grad_fn=<UnbindBackward0>): None, tensor(0.0006, grad_fn=<UnbindBackward0>): None, tensor(0.0006, grad_fn=<UnbindBackward0>): None}, 'voyage_sections_voyager_top_indices': {tensor(1615): None, tensor(1569): None, tensor(240): None, tensor(609): None, tensor(1542): None}, 'voyage_chapters_voyager_top_values': {tensor(0.0013, grad_fn=<UnbindBackward0>): None, tensor(0.0013, grad_fn=<UnbindBackward0>): None, tensor(0.0013, grad_fn=<UnbindBackward0>): None, tensor(0.0013, grad_fn=<UnbindBackward0>): None, tensor(0.0013, grad_fn=<UnbindBackward0>): None}, 'voyage_chapters_voyager_top_indices': {tensor(318): None, tensor(489): None, tensor(484): None, tensor(195): None, tensor(804): None}, 'voyage_pages_voyager_top_values': {tensor(0.0005, grad_fn=<UnbindBackward0>): None, tensor(0.0005, grad_fn=<UnbindBackward0>): None, tensor(0.0005, grad_fn=<UnbindBackward0>): None, tensor(0.0005, grad_fn=<UnbindBackward0>): None, tensor(0.0005, grad_fn=<UnbindBackward0>): None}, 'voyage_pages_voyager_top_indices': {tensor(354): None, tensor(2160): None, tensor(987): None, tensor(1696): None, tensor(995): None}}\n"
     ]
    }
   ],
   "source": [
    "# def check_text_in_indices(indices, source, model, input_text):\n",
    "#     \"\"\"\n",
    "#     Check if the input_text is present in any of the retrieved texts from given indices.\n",
    "#     \"\"\"\n",
    "#     results = {}\n",
    "#     for index in indices:\n",
    "#         content = get_processed_content_by_index(index, source, model)\n",
    "#         if content:\n",
    "#             results[index] = str(input_text).lower() in str(content).lower()\n",
    "#         else:\n",
    "#             results[index] = None  # Indicates that no content was found for the index\n",
    "#     return results\n",
    "\n",
    "\n",
    "# def check_text_in_indices_automated(top_indices_dict, input_text, model):\n",
    "#     \"\"\"\n",
    "#     Automated function to check if the input_text is present in any of the retrieved texts\n",
    "#     from given indices specified in the top_indices_dict for the specified model.\n",
    "#     \"\"\"\n",
    "#     results = {}\n",
    "    \n",
    "#     for source, indices_dict in top_indices_dict.items():\n",
    "#         for key, indices in indices_dict.items():\n",
    "#             # Check for the right model and source combination\n",
    "#             if model == \"gemini\" and \"gemini\" in key:\n",
    "#                 matches = check_text_in_indices(indices, source, \"gemini\", input_text)\n",
    "#                 results[f\"{model}_{source}_{key}\"] = matches\n",
    "#             elif model == \"voyage\" and \"voyager\" in key:\n",
    "#                 matches = check_text_in_indices(indices, source, \"voyage\", input_text)\n",
    "#                 results[f\"{model}_{source}_{key}\"] = matches\n",
    "\n",
    "#     return results\n",
    "\n",
    "# input_text=\"\"\"Whoever, being an officer, employee or agent of the United States or of any department or agency thereof, having received public money which he is not authorized to retain as salary, pay, or emolument, fails to render his accounts for the same as provided by law is guilty of embezzlement, and shall be fined under this title or in a sum equal to the amount of the money embezzled, whichever is greater, or imprisoned not more than ten years, or both; but if the amount embezzled does not exceed $1,000, he shall be fined under this title or imprisoned not more than one year, or both.\n",
    "# \"\"\"\n",
    "\n",
    "# # Call the automated function for \"gemini\" model\n",
    "# automated_matches = check_text_in_indices_automated(output['top_indices'], input_text, \"gemini\")\n",
    "# print(automated_matches)\n",
    "\n",
    "# # Call the automated function for \"voyage\" model\n",
    "# automated_matches_voyage = check_text_in_indices_automated(output['top_indices'], input_text, \"voyage\")\n",
    "# print(automated_matches_voyage)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
